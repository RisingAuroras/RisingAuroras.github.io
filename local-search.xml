<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%A3%9E%E6%A1%A8AI%20Studio%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%A3%9E%E6%A1%A8AI%20Studio%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="飞桨AI-Studio-强化学习入门"><a href="#飞桨AI-Studio-强化学习入门" class="headerlink" title="飞桨AI Studio 强化学习入门"></a>飞桨AI Studio 强化学习入门</h3><p>每个人都是过去经验的总和，你过去的经验造就了现在的你。—— 心理学</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203312034277.png" alt="image-20220331203447118"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203312037245.png" alt="image-20220331203723158"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203312041035.png" alt="image-20220331204101936"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203312044146.png" alt="image-20220331204456075"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203312046007.png" alt="image-20220331204629938"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203312048002.png" alt="image-20220331204814892"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203312052371.png" alt="image-20220331205223270"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203312054163.png" alt="image-20220331205446081"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203312059699.png" alt="image-20220331205951631"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203312109589.png" alt="image-20220331210916520"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204012105836.png" alt="image-20220401210550745"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204012109893.png" alt="image-20220401210904804"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204012112654.png" alt="image-20220401211249509"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204012116198.png" alt="image-20220401211602094"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204012120998.png" alt="image-20220401212044906"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204012122937.png" alt="image-20220401212211840"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204012123066.png" alt="image-20220401212323986"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204012123924.png" alt="image-20220401212352802"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204012128604.png" alt="image-20220401212836501"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021649408.png" alt="image-20220402164909264"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021650764.png" alt="image-20220402165054624"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021654657.png" alt="image-20220402165429563"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021656091.png" alt="image-20220402165646003"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021927377.png" alt="image-20220402192759264"></p><h4 id="基于表格型方法求解RL"><a href="#基于表格型方法求解RL" class="headerlink" title="基于表格型方法求解RL"></a>基于表格型方法求解RL</h4><h5 id="MDP，Q表格"><a href="#MDP，Q表格" class="headerlink" title="MDP，Q表格"></a>MDP，Q表格</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021933439.png" alt="image-20220402193346349"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021934508.png" alt="image-20220402193417425"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021941837.png" alt="image-20220402194145758"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021942054.png" alt="image-20220402194250961"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021943635.png" alt="image-20220402194317557"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021947928.png" alt="image-20220402194733833"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021949383.png" alt="image-20220402194912281"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021951893.png" alt="image-20220402195110828"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021958826.png" alt="image-20220402195832741"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204022000516.png" alt="image-20220402200009448"></p><h5 id="强化概念，TD更新，Sarsa引入"><a href="#强化概念，TD更新，Sarsa引入" class="headerlink" title="强化概念，TD更新，Sarsa引入"></a>强化概念，TD更新，Sarsa引入</h5><p>强化概念就是用下一个状态的价值更新当前状态的价值（在重复训练之后，下一个状态的价值就会不断强化影响下一个状态的价值）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204022003562.png" alt="image-20220402200313427"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204022005787.png" alt="image-20220402200503709"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204022010761.png" alt="image-20220402201054657"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204022018740.png" alt="image-20220402201825665"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204022019358.png" alt="image-20220402201925291"></p><h5 id="Sarsa算法的介绍与代码解析"><a href="#Sarsa算法的介绍与代码解析" class="headerlink" title="Sarsa算法的介绍与代码解析"></a>Sarsa算法的介绍与代码解析</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204022022423.png" alt="image-20220402202210328"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204032144691.png" alt="image-20220403214438361"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204032145187.png" alt="image-20220403214552786"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204032150276.png" alt="image-20220403215001937"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204032154657.png" alt="image-20220403215429349"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204032155921.png" alt="image-20220403215513632"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204032158919.png" alt="image-20220403215827630"></p><p><strong>Off-Policy 和 On-Policy区别是什么</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041623407.png" alt="image-20220404162323326"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041629856.png" alt="image-20220404162940738"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041631480.png" alt="image-20220404163114346"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041632262.png" alt="image-20220404163239147"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041633174.png" alt="image-20220404163324056"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041634294.png" alt="image-20220404163450189"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041635309.png" alt="image-20220404163515186"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041638517.png" alt="image-20220404163809431"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041639088.png" alt="image-20220404163917995"></p><h4 id="基于神经网络求解RL"><a href="#基于神经网络求解RL" class="headerlink" title="基于神经网络求解RL"></a>基于神经网络求解RL</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041658943.png" alt="image-20220404165852847"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041710023.png" alt="image-20220404171020847"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041709017.png" alt="image-20220404170942946"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041711744.png" alt="image-20220404171118637"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041713958.png" alt="image-20220404171336822"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042003966.png" alt="image-20220404200358881"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042006031.png" alt="image-20220404200559909"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042008553.png" alt="image-20220404200817433"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042008737.png" alt="image-20220404200841586"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042015617.png" alt="image-20220404201534478"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042016611.png" alt="image-20220404201634484"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042020354.png" alt="image-20220404202050251"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042026309.png" alt="image-20220404202640196"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042027641.png" alt="image-20220404202756528"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042028335.png" alt="image-20220404202846210"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042029463.png" alt="image-20220404202937359"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042033671.png" alt="image-20220404203323549"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042034948.png" alt="image-20220404203443863"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042036689.png" alt="image-20220404203609503"></p><p>留白</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204042048575.png" alt="image-20220404204803469"></p><h4 id="随机策略和策略梯度"><a href="#随机策略和策略梯度" class="headerlink" title="随机策略和策略梯度"></a>随机策略和策略梯度</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052155312.png" alt="image-20220405215211917"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052153032.png" alt="image-20220405215318922"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052157294.png" alt="image-20220405215704174"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052158131.png" alt="image-20220405215856012"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052159782.png" alt="image-20220405215943681"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052200667.png" alt="image-20220405220044556"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052203403.png" alt="image-20220405220343309"></p><p>智能体的选择是我们可以优化的，环境的转移是我们不能够控制的</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052206839.png" alt="image-20220405220640738"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052209043.png" alt="image-20220405220949953"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052211864.png" alt="image-20220405221123772"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052212407.png" alt="image-20220405221210300"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052216720.png" alt="image-20220405221602644"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052217067.png" alt="image-20220405221734970"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052219878.png" alt="image-20220405221953766"></p><h5 id="PlicyGradeint-算法"><a href="#PlicyGradeint-算法" class="headerlink" title="PlicyGradeint 算法"></a>PlicyGradeint 算法</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061400573.png" alt="image-20220406140011420"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h3 id="李宏毅机器学习"><a href="#李宏毅机器学习" class="headerlink" title="李宏毅机器学习"></a>李宏毅机器学习</h3><ul><li><p>机器学习就是让机器找一个函数</p></li><li><p>机器学习的任务</p><ul><li><p>regression</p></li><li><p>classification</p></li><li><p>structured learning(create something with structure)</p></li></ul><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203061452915.png" alt="image-20220306145251677"></p></li></ul><h4 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h4><ul><li><p>步骤</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203311643124.png" alt="image-20220331164303958"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204081712642.png" alt="1111"></p><p>正则化(预防overfitting)</p></li></ul><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204012042301.png" alt="image-20220401204206160"></p><p>$\lambda$越大，$w_i$会越小，曲线会比较平滑</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204012043625.png" alt="image-20220401204326297"></p><p>机器学习攻略：为了更好的效果</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021041423.png" alt="image-20220402103511942"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021041890.png" alt="image-20220402104128799"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021043356.png" alt="image-20220402104343277"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021044933.png" alt="image-20220402104448844"></p><p>在training data上，显然层数越高，拟合效果越好（过拟合），而不是减少，如果减少的话，就考虑 是不是优化问题</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021047932.png" alt="image-20220402104706838"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021053260.png" alt="image-20220402105358174"></p><p><strong>克服overfitting</strong></p><p>增加数据</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021105067.png" alt="image-20220402110513933"></p><p>降低函数弹性</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021109910.png" alt="image-20220402110925818"></p><p>but</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021111408.png" alt="image-20220402111138326"></p><p>模型复杂度</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021113142.png" alt="image-20220402111325067"></p><p>交叉验证</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021620702.png" alt="image-20220402162014614"></p><p>防止validation set随机</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021623989.png" alt="image-20220402162347896"></p><p>overfitting 可以用数据来克服，mismatch不可以</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204021626811.png" alt="image-20220402162622753"></p><h4 id="类神经网络训练不起来怎么办"><a href="#类神经网络训练不起来怎么办" class="headerlink" title="类神经网络训练不起来怎么办"></a>类神经网络训练不起来怎么办</h4><h5 id="Optimization-Fails"><a href="#Optimization-Fails" class="headerlink" title="Optimization Fails"></a>Optimization Fails</h5><p>找到问题是局部最优点，还是鞍点</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204031625267.png" alt="image-20220403162522168"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204031632800.png" alt="image-20220403163219723"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204031633571.png" alt="image-20220403163326469"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204031640446.png" alt="image-20220403164037360"></p><h5 id="Batch-and-Momentum"><a href="#Batch-and-Momentum" class="headerlink" title="Batch and Momentum"></a>Batch and Momentum</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204031932472.png" alt="image-20220403193243260"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204031943545.png" alt="image-20220403194334460"></p><p>不同批次Batch可能LOSS曲线略有差异，所以可能跳过局部最优点处</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204031945670.png" alt="image-20220403194506593"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204031953461.png" alt="image-20220403195340375"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204031953435.png" alt="image-20220403195359341"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204031958510.png" alt="image-20220403195846407"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204031959212.png" alt="image-20220403195935126"></p><p>Momentum</p><p>引入<code>惯性</code></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204032011709.png" alt="image-20220403201142602"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204032013791.png" alt="image-20220403201331695"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204032016167.png" alt="111"></p><h5 id="自动调整学习率"><a href="#自动调整学习率" class="headerlink" title="自动调整学习率"></a>自动调整学习率</h5><p>可能是学习率过大，导致无法收敛到最优点</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204040938868.png" alt="image-20220404093821780"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204040941233.png" alt="image-20220404094150166"></p><p>学习率过大容易跳过最优点，学习率过小，容易造成收敛速度过慢</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204040945072.png" alt="image-20220404094543945"></p><p><strong>特制化learning rate</strong></p><p>在梯度大的时候，learning rate 调小一点，梯度小的时候 learning rate调大一点</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204040959005.png" alt="image-20220404095922888"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041009739.png" alt="image-20220404100915650"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041015237.png" alt="image-20220404101529162"></p><p><strong>RMSProp</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041022787.png" alt="image-20220404102226713"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041025707.png" alt="image-20220404102507632"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041025114.png" alt="image-20220404102531970"></p><p><strong>Root Mean Square使用效果</strong></p><p>到纵向时gradient很小处，会上下摆动（但是会修正回来）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041030967.png" alt="image-20220404103015838"></p><p>用Learning Rate Scheduling 来改善</p><p>learning rage decay</p><p>$\eta^t$是一个随着时间越来越小的函数（很显然，随着时间增加，越来越趋于收敛）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041034514.png" alt="image-20220404103421404"></p><p>warm up</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041036285.png" alt="image-20220404103646206"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041040122.png" alt="image-20220404104052025"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041049753.png" alt="image-20220404104914676"></p><h5 id="损失函数的影响"><a href="#损失函数的影响" class="headerlink" title="损失函数的影响"></a>损失函数的影响</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041050931.png" alt="image-20220404105000780"></p><p>分类问题</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041545917.png" alt="image-20220404154500835"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041547800.png" alt="image-20220404154726715"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041547228.png" alt="image-20220404154742137"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041551314.png" alt="image-20220404155116229"></p><p>softmax 归一求概率</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041555713.png" alt="image-20220404155512630"></p><p>二分类往往直接带入Sigmod，实际上sigmod 和 softmax 是同一回事</p><p>交叉熵</p><p>pytorch中，cross-entropy内嵌了softmax</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041610002.png" alt="image-20220404161019918"></p><p>Cross-entropy更适合分类问题，通过修改Loss function来减小optimize 的难度</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204041618414.png" alt="image-20220404161835274"></p><h5 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h5><p>不同维度的变量范围不一样，造成Error Surface复杂程度不一样，会对optimization造成影响</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082037230.png" alt="image-20220408203736076"></p><p>解决方法：Feature Normalization</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082043204.png" alt="image-20220408204326088"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082045029.png" alt="image-20220408204555936"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082046084.png" alt="image-20220408204656984"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082133869.png" alt="image-20220408213318798"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082134609.png" alt="image-20220408213454530"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082135666.png" alt="image-20220408213517589"></p><p>加入$z_1$发生改变，那么会影响$\mu$和$\sigma$，所以也会影响$a_1,a_2,a_3$</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082136141.png" alt="image-20220408213659041"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082142203.png" alt="image-20220408214246094"></p><p>加上$\beta$和$\gamma$的意思是：我们进行Normalization后，mean为0，variances为1，可能会带来某些影响，所以我们加入$\beta 和\gamma$，如果需要的话，让网络自己去<code>train</code>（$\gamma$初值为1的向量，$\beta$初值是为0的向量</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082147697.png" alt="image-20220408214702612"></p><p>在testing时，batch normalization的运作方式</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082153188.png" alt="image-20220408215347114"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082200276.png" alt="image-20220408220047189"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082204306.png" alt="image-20220408220412179"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204082204596.png" alt="image-20220408220428532"></p><h4 id="浅谈机器学习原理（一）"><a href="#浅谈机器学习原理（一）" class="headerlink" title="浅谈机器学习原理（一）"></a>浅谈机器学习原理（一）</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204051219741.png" alt="image-20220405121902635"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204051616705.png" alt="image-20220405161658596"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204051619120.png" alt="image-20220405161945037"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204051620198.png" alt="image-20220405162003118"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204051620956.png" alt="image-20220405162036876"></p><h4 id="NewOptimizers-for-Deep-Learning"><a href="#NewOptimizers-for-Deep-Learning" class="headerlink" title="NewOptimizers for Deep Learning"></a>NewOptimizers for Deep Learning</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204121026920.png" alt="image-20220412102600815"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204121056443.png" alt="image-20220412105619364"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204121058377.png" alt="image-20220412105805271"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204121100693.png" alt="image-20220412110024601"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204121103462.png" alt="image-20220412110341362"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204121106129.png" alt="image-20220412110641031"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204121108647.png" alt="image-20220412110821570"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204121109169.png" alt="image-20220412110942086"></p><h4 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141450666.png" alt="image-20220414145039603"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141450850.png" alt="image-20220414145014703"></p><p>如果fully connected(全连接) $weight$有$100<em>100</em>3*1000$个，随着参数越多，我们可以增加模型的弹性和能力，但是也增加了overfitting的风险</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141451442.png" alt="image-20220414145105368"></p><p>对影像辨识问题的观察</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141502278.png" alt="image-20220414150228164"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141521114.png" alt="image-20220414152153035"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141529325.png" alt="image-20220414152941215"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141529355.png" alt="image-20220414152958274"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141530958.png" alt="image-20220414153026880"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141534067.png" alt="image-20220414153443993"></p><p>另一种解释</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141604277.png" alt="image-20220414160426186"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141604881.png" alt="image-20220414160439809"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141604440.png" alt="image-20220414160446363"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141604196.png" alt="image-20220414160454116"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141605883.png" alt="image-20220414160526794"></p><p>只要网络比较深，哪怕filter大小为$3*3$，也不用怕检测不到大的pattern</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141605751.png" alt="image-20220414160554667"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141607621.png" alt="image-20220414160706549"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141607148.png" alt="image-20220414160730023"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141925023.png" alt="image-20220414192506930"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141925327.png" alt="image-20220414192524238"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141925854.png" alt="image-20220414192535769"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141926122.png" alt="image-20220414192628008"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141926678.png" alt="image-20220414192635531"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141941938.png" alt="image-20220414194148836"></p><p>数据增强——Spatial Transformer</p><h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202205181534331.png" alt="image-20220518153419173"></p><h4 id="Self-attention-自注意力机制"><a href="#Self-attention-自注意力机制" class="headerlink" title="Self-attention(自注意力机制)"></a>Self-attention(自注意力机制)</h4><h4 id="回归评价指标MSE、RMSE、MAE、R-Squared"><a href="#回归评价指标MSE、RMSE、MAE、R-Squared" class="headerlink" title="回归评价指标MSE、RMSE、MAE、R-Squared"></a>回归评价指标MSE、RMSE、MAE、R-Squared</h4><h5 id="均方误差（MSE）"><a href="#均方误差（MSE）" class="headerlink" title="均方误差（MSE）"></a>均方误差（MSE）</h5><p>MSE （Mean Squared Error）叫做均方误差。看公式</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052051800.png" alt="image-20220405205148729"></p><h5 id="均方根误差（RMSE）"><a href="#均方根误差（RMSE）" class="headerlink" title="均方根误差（RMSE）"></a>均方根误差（RMSE）</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052052277.png" alt="image-20220405205210215"></p><p>这不就是MSE开个根号么。有意义么？其实实质是一样的。只不过用于数据更好的描述。<br> 例如：要做房价预测，每平方是万元（真贵），我们预测结果也是万元。那么差值的平方单位应该是 千万级别的。那我们不太好描述自己做的模型效果。怎么说呢？我们的模型误差是 多少千万？。。。。。。于是干脆就开个根号就好了。我们误差的结果就跟我们数据是一个级别的可，在描述模型的时候就说，我们模型的误差是多少万元</p><h5 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052053205.png" alt="image-20220405205308141"></p><h5 id="R-Squared"><a href="#R-Squared" class="headerlink" title="R Squared"></a>R Squared</h5><p>上面的几种衡量标准针对不同的模型会有不同的值。比如说预测房价 那么误差单位就是万元。数子可能是3，4，5之类的。那么预测身高就可能是0.1，0.6之类的。没有什么可读性，到底多少才算好呢？不知道，那要根据模型的应用场景来。<br> 看看分类算法的衡量标准就是正确率，而正确率又在0～1之间，最高百分之百。最低0。很直观，而且不同模型一样的。那么线性回归有没有这样的衡量标准呢？答案是有的。<br> 那就是R Squared也就R方</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052053045.png" alt="image-20220405205328002"></p><p>光看这些东西很懵逼，其中分子是Residual Sum of Squares 分母是 Total Sum of Squares<br>那就看公式吧</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052053579.png" alt="image-20220405205338528"></p><p>上面<strong>分子</strong>就是我们训练出的模型预测的所有误差。<br> 下面<strong>分母</strong>就是不管什么我们猜的结果就是y的平均数。（瞎猜的误差）</p><p>那结果就来了。<br> 如果结果是0，就说明我们的模型跟瞎猜差不多。<br> 如果结果是1。就说明我们模型无错误。<br> 如果结果是0-1之间的数，就是我们模型的好坏程度。<br> 如果结果是负数。说明我们的模型还不如瞎猜。（其实导致这种情况说明我们的数据其实没有啥线性关系）</p><p>化简上面的公式<br> 分子分母同时除以m</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052054398.png" alt="image-20220405205419338"></p><p>那么分子就变成了我们的均方误差MSE，下面分母就变成了方差。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204052054527.png" alt="image-20220405205437460"></p><h4 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206241111638.png" alt="image-20220624111108506"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206241121362.png" alt="image-20220624112102241"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206241123108.png" alt="image-20220624112347984"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206271655584.png" alt="image-20220627165527452"></p><h4 id="未听懂部分"><a href="#未听懂部分" class="headerlink" title="未听懂部分"></a>未听懂部分</h4><p><strong>P16分类神奇宝贝 p17 逻辑回归</strong>  </p><p><strong>self-attention</strong></p><p>​</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%96%87%E7%89%88%20%C2%B7%E6%9D%8E%E6%B2%90%C2%B7/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%96%AF%E5%9D%A6%E7%A6%8F21%E7%A7%8B%E5%AD%A3%EF%BC%9A%E5%AE%9E%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%96%87%E7%89%88%20%C2%B7%E6%9D%8E%E6%B2%90%C2%B7/</url>
    
    <content type="html"><![CDATA[<h3 id="斯坦福21秋季：实用机器学习中文版-·李沐·"><a href="#斯坦福21秋季：实用机器学习中文版-·李沐·" class="headerlink" title="斯坦福21秋季：实用机器学习中文版 ·李沐·"></a>斯坦福21秋季：实用机器学习中文版 <code>·李沐·</code></h3><h4 id="机器学习流程"><a href="#机器学习流程" class="headerlink" title="机器学习流程"></a>机器学习流程</h4><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201111027259.png" alt="image-20220111102659117"></p><h4 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h4><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201111039618.png" alt="image-20220111103943491"></p><h4 id="Roles"><a href="#Roles" class="headerlink" title="Roles"></a>Roles</h4><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201111201090.png" alt="image-20220111120123927"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201111447876.png" alt="image-20220111144728803"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201111450740.png" alt="image-20220111145024685"></p><h4 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h4><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201111527222.png" alt="image-20220111152751141"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201111545177.png" alt="image-20220111154549110"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h3 id="数学建模学习笔记"><a href="#数学建模学习笔记" class="headerlink" title="数学建模学习笔记"></a>数学建模学习笔记</h3><h4 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h4><p><strong>数学模型</strong></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110251708374.png" alt="image-20211025170849201"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110251748670.png" alt="image-20211025174803358"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110251808330.png" alt="image-20211025180821228"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110252029785.png" alt="image-20211025202930703"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110252037014.png" alt="image-20211025203750932"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110252044351.png" alt="image-20211025204417304"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110252102629.png" alt="image-20211025210211583"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110252106526.png" alt="image-20211025210623453"></p><h4 id="线性规划模型"><a href="#线性规划模型" class="headerlink" title="线性规划模型"></a>线性规划模型</h4><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110260932126.png" alt="image-20211026093211043"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110260936208.png" alt="image-20211026093644113"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110260941207.png" alt="image-20211026094101111"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110260947930.png" alt="image-20211026094757853"></p><p><strong>matlab 只能求解最小值，如果求最大值需要在目标函数前面加一个负号</strong></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110260955493.png" alt="image-20211026095537431"></p><h4 id="整数规划"><a href="#整数规划" class="headerlink" title="整数规划"></a>整数规划</h4><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110282104683.png" alt="image-20211028210401625"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110282105570.png" alt="image-20211028210518502"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202110292020011.png" alt="image-20211029202014880"></p><h4 id="非线性规划模型（NP）"><a href="#非线性规划模型（NP）" class="headerlink" title="非线性规划模型（NP）"></a>非线性规划模型（NP）</h4><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202111011112095.png" alt="image-20211101111224948"></p><h4 id="层次分析法"><a href="#层次分析法" class="headerlink" title="层次分析法"></a>层次分析法</h4><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202111022021960.png" alt="image-20211102202143851"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202111022028847.png" alt="image-20211102202839679"></p><h4 id="灰色系统的应用"><a href="#灰色系统的应用" class="headerlink" title="灰色系统的应用"></a>灰色系统的应用</h4><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202111051132097.png" alt="image-20211105113233936"></p><h4 id="一些技巧"><a href="#一些技巧" class="headerlink" title="一些技巧"></a>一些技巧</h4><ol><li>松弛变量的加入</li><li></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h3 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h3><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202202141556523.png" alt="image-20220214155632398"></p><p>混淆矩阵</p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202202141558891.png" alt="image-20220214155800792"></p><p>准确率 &#x3D; (TP + TN)&#x2F;总数</p><p>精确率 &#x3D;TP&#x2F;挑出的总数</p><p>召回率&#x3D;(TP+FN)&#x2F;挑选目标总数</p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202202141605867.png" alt="image-20220214160522793"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202202141815275.png" alt="image-20220214181504137"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202202141829591.png" alt="image-20220214182954528"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E3%80%90%E7%8E%8B%E6%A0%91%E6%A3%AE%E3%80%91%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0(DRL)/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E3%80%90%E7%8E%8B%E6%A0%91%E6%A3%AE%E3%80%91%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0(DRL)/</url>
    
    <content type="html"><![CDATA[<h1 id="【王树森】深度强化学习-DRL"><a href="#【王树森】深度强化学习-DRL" class="headerlink" title="【王树森】深度强化学习(DRL)"></a>【王树森】深度强化学习(DRL)</h1><p><code>注：以下内容中，大写的为随机变量，小写的为观测值</code></p><h4 id="强化学习基础"><a href="#强化学习基础" class="headerlink" title="强化学习基础"></a>强化学习基础</h4><h5 id="Terminologies"><a href="#Terminologies" class="headerlink" title="Terminologies"></a>Terminologies</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061440370.png" alt="image-20220406144008102"></p><p>这里$\pi$ 是一个离散的概率密度，在当前状态$s$的情况下，做出$a$动作的概率</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061443064.png" alt="image-20220406144318818"></p><p>强化学习的目标就是获得的奖励尽量要高 </p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061446304.png" alt="image-20220406144638115"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061449417.png" alt="image-20220406144901171"></p><h5 id="agent与enviroment交互"><a href="#agent与enviroment交互" class="headerlink" title="agent与enviroment交互"></a>agent与enviroment交互</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061451466.png" alt="image-20220406145146387"></p><h5 id="强化学习的随机性"><a href="#强化学习的随机性" class="headerlink" title="强化学习的随机性"></a>强化学习的随机性</h5><p>动作的随机性 and 状态转移的随机性</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061452087.png" alt="image-20220406145259007"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061455706.png" alt="image-20220406145512607"></p><h5 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061457597.png" alt="image-20220406145740342"></p><h5 id="Reward-and-Return-回报"><a href="#Reward-and-Return-回报" class="headerlink" title="Reward and Return(回报)"></a>Reward and Return(回报)</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061504086.png" alt="image-20220406150406051"></p><p>引入$\gamma$作为”折扣”</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061503836.png" alt="image-20220406150311759"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061509593.png" alt="image-20220406150939506"></p><h5 id="Value-Function"><a href="#Value-Function" class="headerlink" title="Value  Function"></a>Value  Function</h5><ul><li>行动价值函数（Action-Value Function）</li><li>状态价值函数（State-Value Function)</li></ul><p>$U_t$依赖于未来$S_t,S_{t+1},S_{t+2}… and\ A_t,A_{t+1}…$，是一个随机变量</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061515088.png" alt="image-20220406151536993"></p><p>行动价值函数$Q_{\pi}$ 和策略$\pi$有关，它是对随机变量$U_t$求条件期望得到的一个数；最优行动价值函数$Q^*$是所有$\pi$中，让$Q$最大的那个$\pi$</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061528881.png" alt="image-20220406152828798"></p><p>状态价值函数可以对某一局面进行打分</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061537117.png" alt="image-20220406153726031"></p><p>它对$Q_{\pi}$中的A求期望消掉A</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061537980.png" alt="image-20220406153751889"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061538922.png" alt="image-20220406153856824"></p><p><strong>Summarize</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061547898.png" alt="image-20220406154744815"></p><h5 id="基于策略或者基于价值"><a href="#基于策略或者基于价值" class="headerlink" title="基于策略或者基于价值"></a>基于策略或者基于价值</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061555625.png" alt="image-20220406155554540"></p><h5 id="检验平台-Gym"><a href="#检验平台-Gym" class="headerlink" title="检验平台-Gym"></a>检验平台-Gym</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061558106.png" alt="11111"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061601909.png" alt="image-20220406160114826"></p><p>render()渲染，展示环境</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061604261.png" alt="image-20220406160425164"></p><h5 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061611675.png" alt="image-20220406161126581"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061613111.png" alt="image-20220406161330030"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204061615709.png" alt="image-20220406161532616"></p><h4 id="价值学习-Value-Based"><a href="#价值学习-Value-Based" class="headerlink" title="价值学习(Value-Based)"></a>价值学习(Value-Based)</h4><p>上节课我们学习了行动价值函数（Action-Value)</p><p>$U_t$是一个随机变量，依赖于将来的行动Action和状态S，我们$U_t$求期望，消除未来的影响，使得$Q_{\pi}(s_t,a_t)$依赖于$s_t,a_t,\pi$。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072135905.png" alt="image-20220407213539771"></p><p>进一步，我们可以求$\pi$最大化，求最优状态价值函数$Q^*$ ，$Q^*(s_t,a_t)$意味着在$s_t$状态加，做出行动$a_t$所得到的价值分数</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072138627.png" alt="image-20220407213829512"></p><h5 id="DQN（Deep-Q-Network-DQN-）"><a href="#DQN（Deep-Q-Network-DQN-）" class="headerlink" title="DQN（Deep Q-Network (DQN)）"></a>DQN（Deep Q-Network (DQN)）</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072147454.png" alt="image-20220407214756368"></p><p>$Q(s,a;w)$ ：神经网络的参数是$w$,输入是$s$，输出是做出动作$a$的价值分数</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072149598.png" alt="image-20220407214945521"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072153911.png" alt="image-20220407215347810"></p><p>做出动作$up$的价值分数最高，所以选择$up$，然后状态转移函数$p(|)$会人random一个新的状态</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072154744.png" alt="image-20220407215402636"></p><p>根据输入$s_t$，选择价值分数最大的动作$a_t$</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072155160.png" alt="image-20220407215543076"></p><p>怎么训练DQN？</p><ul><li>TD Learning（不完成旅程也能更新参数）<ol><li>Sarsa</li><li>Q-learning</li><li>Multi-Step TD Target</li></ol></li></ul><h6 id="Temporal-Difference-TD-Learning"><a href="#Temporal-Difference-TD-Learning" class="headerlink" title="Temporal Difference (TD) Learning"></a>Temporal Difference (TD) Learning</h6><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072208187.png"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072209925.png"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072210230.png" alt="image-20220407221012136"></p><p>时序差分算法的目标就是让$TD\ Error$尽可能的小，趋近于0</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072211509.png" alt="image-20220407221107415"></p><p>$\gamma$ 是介于0 和 1之间的折扣率</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072214535.png" alt="image-20220407221400441"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072217249.png" alt="image-20220407221731179"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072231418.png" alt="image-20220407223107328"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072231950.png" alt="image-20220407223132861"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072240201.png" alt="image-20220407224025114"></p><p><strong>Summary</strong> </p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072242362.png" alt="image-20220407224254280"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204072244460.png" alt="image-20220407224426374"></p><h4 id="策略学习-Policy-Based"><a href="#策略学习-Policy-Based" class="headerlink" title="策略学习(Policy-Based)"></a>策略学习(Policy-Based)</h4><p>策略函数Policy Function</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101453990.png" alt="image-20220410145317863"></p><p>由于输入的状态$s$是多种多样的，所以我们可以用一个函数来近似</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101457771.png" alt="image-20220410145724667"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101458960.png" alt="image-20220410145800853"></p><h5 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101503296.png" alt="image-20220410150313207"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101503763.png" alt="image-20220410150345678"></p><p>这里我的理解是：选择最优的策略动作，才能最大化状态价值函数$V(s;\theta)$，所以现在我们的目的变为了最大化$V(s;\theta)$，我们用梯度上升算法来更新$\theta$</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101506710.png" alt="image-20220410150647616"></p><p>Policy Gradient 的推导</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101527592.png" alt="image-20220410152738513"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101527968.png" alt="image-20220410152747888"></p><p>这里我们加设$Q_{\pi}$函数与$\theta$无关，但实际上是有关的，所以这里推导是不严谨了，但是方便理解</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101529012.png" alt="image-20220410152941922"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101530616.png" alt="image-20220410153002529"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101530072.png" alt="image-20220410153056975"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101531113.png" alt="image-20220410153115024"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101531290.png" alt="image-20220410153123226"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101532799.png" alt="image-20220410153258705"></p><p>因为神经网络是一个很复杂的函数，我们无法对此进行积分，所以我们用蒙特卡洛方法进行近似</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101539665.png" alt="image-20220410153923601"></p><p>因为$g(\widehat{a},\theta)$是策略梯度的无偏估计，所以我们用它来近似策略梯度</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101540360.png" alt="image-20220410154013275"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101546838.png" alt="image-20220410154603754"></p><p>Summary For 策略梯度算法(<strong>梯度上升</strong>更新$\theta$)</p><p>​<img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101557043.png" alt="image-20220410155718966"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101601527.png" alt="image-20220410160158450"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101648891.png" alt="image-20220410164828795"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101650025.png" alt="image-20220410165048955"></p><p><strong>Summary</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204101701420.png" alt="image-20220410170124341"></p><h5 id="Actor-Critic-Methods"><a href="#Actor-Critic-Methods" class="headerlink" title="Actor-Critic Methods"></a>Actor-Critic Methods</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102018130.png" alt="image-20220410201847055"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102025303.png" alt="image-20220410202502258"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102040446.png" alt="image-20220410204004353"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102047454.png" alt="image-20220410204731343"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102047364.png" alt="image-20220410204755256"></p><p>让运动员的平均分越来越高，并且让裁判的打分越来越精准</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102048710.png" alt="image-20220410204857526"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102056258.png" alt="image-20220410205636174"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102057128.png" alt="image-20220410205712046"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102124579.png" alt="image-20220410212401511"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102124357.png" alt="image-20220410212416267"></p><p>图解</p><p>运动员根据$state$做出$Action$</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102131739.png" alt="image-20220410213118659"></p><p>裁判会根据做出的动作$a$和$state$进行打分，并将分数反馈给运动员</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102130791.png" alt="image-20220410213043711"></p><p>运动员会根据分数来调整自己的动作（迎合裁判）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102130287.png" alt="image-20220410213053209"></p><p>裁判也会提高自己的水平（根据Reward $r$)，以此让运动员做出更好的动作</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102134680.png" alt="image-20220410213428597"></p><p>Summary of AC算法</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102145173.png" alt="image-20220410214556084"></p><p>第九步，$q_t$和$\delta_t$ 都对，不过$\delta_t$ 是叫做带baseline的策略梯度算法</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102149297.png" alt="image-20220410214913193"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102150618.png" alt="image-20220410215003521"></p><p><strong>Summary</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102213099.png" alt="image-20220410221300023"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102213674.png" alt="image-20220410221317599"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204102214718.png" alt="image-20220410221422626"></p><h4 id="实例分析：AlphaGo的基本原理"><a href="#实例分析：AlphaGo的基本原理" class="headerlink" title="实例分析：AlphaGo的基本原理"></a>实例分析：AlphaGo的基本原理</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110910961.png" alt="image-20220411090519196"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110910641.png" alt="image-20220411090526718"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110905686.png" alt="image-20220411090535634"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110910074.png" alt="image-20220411091057001"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110911385.png" alt="image-20220411091111303"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110914715.png" alt="image-20220411091404630"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110915696.png" alt="image-20220411091509588"></p><p><strong>1. Behavior Cloning</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110946896.png" alt="image-20220411094646849"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110947478.png" alt="image-20220411094750375"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110948467.png" alt="image-20220411094801389"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110948210.png" alt="image-20220411094827138"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110950341.png" alt="image-20220411095019226"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110950702.png" alt="image-20220411095043614"></p><p><strong>2. 训练策略网络</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110954897.png" alt="image-20220411095412839"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110957052.png" alt="image-20220411095731972"></p><p>这里没有折扣，如果赢了，我们认为之前下的每一步棋都是好棋，如果输了，认为每一步棋都是臭棋（没有办法区分一步棋是好是坏）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204110958093.png" alt="image-20220411095852009"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111047876.png" alt="image-20220411104755790"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111049572.png" alt="image-20220411104956481"></p><p>直接用策略网络还是不够好，所以采用蒙特卡洛树搜索+策略网络的方法</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111051857.png" alt="image-20220411105137788"></p><p>用蒙特卡洛树搜索需要训练一个价值网络，这个价值网络是对$状态价值函数v$的近似，而不是对行动价值Q的近似</p><p><strong>3. 训练价值网络</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111052729.png" alt="image-20220411105257686"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111056835.png" alt="image-20220411105616742"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111057212.png" alt="image-20220411105725110"></p><p>这并不是之前讲的AC算法，这里需要先训练策略网络，然后再根据策略网络训练价值网络。</p><p>如下第一步<code>Play a game to the end</code>中用到了策略网络进行博弈</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111105871.png" alt="image-20220411110516785"></p><p><strong>4. 蒙特卡洛树搜索</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111509724.png" alt="image-20220411150934676"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111515854.png" alt="image-20220411151543765"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111523655.png" alt="image-20220411152349569"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111531810.png" alt="image-20220411153158719"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111538298.png" alt="image-20220411153836217"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111538021.png" alt="image-20220411153846943"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111539707.png" alt="image-20220411153901618"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111544390.png" alt="image-20220411154432315"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111544718.png" alt="image-20220411154454624"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111550625.png" alt="image-20220411155032532"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111551447.png" alt="image-20220411155107351"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111551842.png" alt="image-20220411155120749"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111552197.png" alt="image-20220411155233107"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111553925.png" alt="image-20220411155331825"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111553611.png" alt="image-20220411155348539"></p><p>一开始$Q(a)$为零，所以$score$的分数主要取决于$\pi(|)$，之后随着搜索次数的增加，$N(a)$增大，第二项变小，$score$的分数主要取决于$Q(a)$</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111556340.png" alt="image-20220411155644266"></p><p>经过许多次搜索迭代之后</p><p>$Q(a)$值和$\pi$ 越大，访问次数$N(a)$值就会越大</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111559855.png" alt="image-20220411155941786"></p><p><strong>Summary Of MCTS</strong>$\P$</p><p><img src="C:/Users/auroras/AppData/Roaming/Typora/typora-user-images/image-20220411160655312.png" alt="image-20220411160655312"></p><p><strong>Summary of AlphaGo</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111611258.png" alt="image-20220411161137181"></p><p><strong>AlphaGo Zero v.s. AlphaGo</strong></p><p>旧版MCTS 是模仿人类玩家，而新版MCTS是模仿自己搜索</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111614682.png" alt="image-20220411161459613"></p><p>仿真环境behavior可能是无用的，实际环境下behavior是有用的（不然代价太大）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111616131.png" alt="image-20220411161649054"></p><p>新版在train时就用了MCTS，用策略网络预测P，用MCTS预测n,我们应该让p接近n才行，因为搜索得到的结果是比较靠谱的，我们用梯度下降来更新策略网络以此修正</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111621203.png" alt="image-20220411162137126"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111623334.png" alt="image-20220411162331260"></p><h3 id="Monte-Carlo-Algorithms"><a href="#Monte-Carlo-Algorithms" class="headerlink" title="Monte Carlo Algorithms"></a>Monte Carlo Algorithms</h3><p>蒙特卡罗方法（Monte Carlo method），也称 统计模拟方法<br>蒙特卡洛方法的理论基础是大数定律。大数定律是描述相当多次数重复试验的结果的定律，在大数定理的保证下:</p><p>利用事件发生的 频率 作为事件发生的 概率 的近似值。</p><p>所以只要设计一个随机试验，使一个事件的概率与某未知数有关，然后通过重复试验，以频率近似值表示概率，即可求得该未知数的近似值。</p><p>样本数量越多，其平均就越趋近于真实值。</p><p>此种方法可以求解微分方程，求多重积分，求特征值等。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111933637.png" alt="image-20220411193336575"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111933678.png" alt="image-20220411193347633"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111933102.png" alt="image-20220411193359051"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111934316.png" alt="image-20220411193409271"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111934780.png" alt="image-20220411193426734"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204111935877.png" alt="image-20220411193512800"></p><h1 id="TD算法"><a href="#TD算法" class="headerlink" title="TD算法"></a>TD算法</h1><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204131957348.png" alt="image-20220413195749233"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204131958428.png" alt="image-20220413195759449"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204131959953.png" alt="image-20220413195910881"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204131959761.png" alt="image-20220413195952647"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132000348.png" alt="image-20220413200004272"></p><p>用蒙特卡洛算法近似期望</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132000178.png" alt="image-20220413200019118"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132000973.png" alt="image-20220413200053912"></p><p><img src="C:/Users/auroras/AppData/Roaming/Typora/typora-user-images/image-20220413200114955.png" alt="image-20220413200114955"></p><p>$Q_\pi$是纯估计，蒙特卡洛近似的期望有部分真实值，我们的目标是让$Q_\pi$去接近$y_t$</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132003746.png" alt="image-20220413200319666"></p><h4 id="Sarsa算法"><a href="#Sarsa算法" class="headerlink" title="Sarsa算法"></a>Sarsa算法</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132003196.png" alt="image-20220413200354148"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132007959.png" alt="image-20220413200735863"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132008168.png" alt="image-20220413200805069"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132009013.png" alt="image-20220413200900933"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132009413.png" alt="image-20220413200945348"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132010175.png" alt="image-20220413201008127"></p><p>如果$state$和$action$很复杂，那么表格将不在适用，我们用神经网络近似动作价值函数$Q_\pi$</p><p>动作价值函数函数$Q$</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132016388.png" alt="image-20220413201620277"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132017618.png" alt="image-20220413201712555"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132018822.png" alt="image-20220413201831738"></p><p>$Summary$</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132023276.png" alt="image-20220413202301198"></p><h4 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h4><p>Sarsa 对应$Q_{\pi}$，Q-learning 对应$Q^*$ </p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132035331.png" alt="image-20220413203538265"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132040279.png" alt="image-20220413204004211"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132051369.png" alt="image-20220413205131281"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132053943.png" alt="image-20220413205322853"></p><p>​<img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132057828.png" alt="image-20220413205753754"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132058545.png" alt="image-20220413205805454"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132059474.png" alt="image-20220413205914409"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132059966.png" alt="image-20220413205927895"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132103351.png" alt="image-20220413210303310"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132103766.png" alt="image-20220413210316673"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132103478.png" alt="image-20220413210328403"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132104945.png" alt="image-20220413210430890"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132208516.png" alt="image-20220413220828396"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132208720.png" alt="image-20220413220838657"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132209000.png" alt="image-20220413220933917"></p><p>$Summary$</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204132210823.png" alt="image-20220413221053749"></p><h4 id="Multi-Step-TD-Target"><a href="#Multi-Step-TD-Target" class="headerlink" title="Multi-Step TD Target"></a>Multi-Step TD Target</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141035070.png" alt="image-20220414103549951"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141036964.png" alt="image-20220414103603117"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141036799.png" alt="image-20220414103624729"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141036397.png" alt="image-20220414103653331"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141038046.png" alt="image-20220414103809981"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141038729.png" alt="image-20220414103827649"></p><p>​<img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141047668.png" alt="image-20220414104705587"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141047738.png" alt="image-20220414104718659"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141048443.png" alt="image-20220414104838373"></p><h1 id="价值函数学习高级技巧"><a href="#价值函数学习高级技巧" class="headerlink" title="价值函数学习高级技巧"></a>价值函数学习高级技巧</h1><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141128823.png" alt="image-20220414112803774"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141128432.png" alt="image-20220414112840325"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141129986.png" alt="image-20220414112905907"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141129139.png" alt="image-20220414112959058"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141131683.png" alt="image-20220414113156609"></p><p>但是，它会有两个主要的缺点</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141136687.png" alt="image-20220414113607616"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141136267.png" alt="image-20220414113656197"></p><h4 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141200450.png" alt="image-20220414120029346"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141200511.png" alt="image-20220414120042422"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141201646.png" alt="image-20220414120127571"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141201647.png" alt="image-20220414120135580"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141201655.png" alt="image-20220414120145571"></p><p><strong>优先经验回放</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141202533.png" alt="image-20220414120151981"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141202249.png" alt="222"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141202689.png" alt="image-20220414120251578"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141203282.png" alt="image-20220414120327196"></p><p>抽样概率不同，会出现偏差，为了消除偏差我们动态调整学习率</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141203165.png" alt="image-20220414120342097"></p><p>抽样概率越大，学习率应该相应较小；抽样概率越小，学习率应该相应较大</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141206443.png" alt="image-20220414120617358"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141206034.png" alt="image-20220414120628963"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204141206497.png" alt="image-20220414120649396"></p><h4 id="高估问题-amp-解决方法"><a href="#高估问题-amp-解决方法" class="headerlink" title="高估问题&amp;解决方法"></a>高估问题&amp;解决方法</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204171356813.png" alt="image-20220417135621674"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204171358135.png" alt="image-20220417135809073"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202204171359699.png" alt="image-20220417135900628"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291652326.png" alt="image-20220629165215185"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291654402.png" alt="image-20220629165416308"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291658697.png" alt="image-20220629165850605"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291700706.png" alt="image-20220629170047620"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291702302.png" alt="image-20220629170255199"></p><p>循环往复，高估现象会加剧</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291705688.png" alt="image-20220629170516587"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291709878.png" alt="image-20220629170907781"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291710965.png" alt="image-20220629171023878"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291715280.png" alt="image-20220629171538169"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291742418.png" alt="image-20220629174206330"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291806045.png" alt="image-20220629180649955"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291807773.png" alt="image-20220629180705703"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291807407.png" alt="image-20220629180717343"></p><p>Target Network 无法解决高估问题，只能缓解，因为$W^-$ 与 $W$有关</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291808431.png" alt="image-20220629180854342"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291814686.png" alt="image-20220629181450599"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291815175.png" alt="image-20220629181500094"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291815841.png" alt="image-20220629181507760"></p><p>Double DQN中，$a^*$是在原DQN网络中选出的，而$y_t$是在Target Network中得出的，所以并不是$max\ Q$的问题。但是DQN只是更好的缓解了高估问题，并没有根除。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291819393.png" alt="image-20220629181910298"></p><p><strong>Summary</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291821331.png" alt="image-20220629182134235"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206291823836.png" alt="image-20220629182313742"></p><h4 id="Dueling-Network"><a href="#Dueling-Network" class="headerlink" title="Dueling Network"></a>Dueling Network</h4><p><strong>Advantage Function(优势函数)</strong></p><p>动作小$a$越好，$A^*$的值越大</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206300949982.png" alt="image-20220630094914862"></p><p><strong>两个基本定理</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206300951326.png" alt="image-20220630095159232"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206300952689.png" alt="image-20220630095213612"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206300954085.png" alt="image-20220630095419011"><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206300954744.png" alt="image-20220630095437647"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206300955185.png" alt="image-20220630095520096"></p><p><strong>Dueling Network</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301008832.png" alt="image-20220630100835734"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301008813.png" alt="image-20220630100849692"></p><p>$A^*$和$V^*$共享卷积层</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301045689.png" alt="image-20220630104509568"></p><p>Dueling Network 比DQN结构要好，所以它的表现更好</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301045946.png" alt="image-20220630104532830"></p><p>用Q-Learning算法来训练Dueling Network，Dueling Network只是网络结构与DQN不同，训练方法是一样的 </p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301048405.png" alt="image-20220630104826321"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301107566.png" alt="image-20220630110757488"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301108599.png" alt="image-20220630110814526"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301108107.png" alt="image-20220630110843035"></p><p>在实验中，发现mean的效果哦会更好</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301109678.png" alt="image-20220630110912593"></p><p>在训练时，把V和A看做一个整体，直接训练Q</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301110857.png" alt="image-20220630111008777"></p><h1 id="策略学习"><a href="#策略学习" class="headerlink" title="策略学习"></a>策略学习</h1><h4 id="策略梯度中的Baseline"><a href="#策略梯度中的Baseline" class="headerlink" title="策略梯度中的Baseline"></a>策略梯度中的Baseline</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301912513.png" alt="image-20220630191247411"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301913437.png" alt="image-20220630191331356"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301913533.png" alt="image-20220630191350457"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301914627.png" alt="image-20220630191408550"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301914590.png" alt="image-20220630191419503"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301914905.png" alt="image-20220630191452817"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301915334.png" alt="image-20220630191512246"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301915915.png" alt="image-20220630191524827"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301917548.png" alt="image-20220630191703470"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301917056.png" alt="image-20220630191713965"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301917046.png" alt="image-20220630191740955"></p><p>算法中，用蒙特卡洛近似如下公式，虽然$b$不影响如下公式，但是会影响蒙特卡洛近似，如果$b$选择好，近似于$Q_\pi$的话，那么会使得蒙特卡洛的方差降低，算法会收敛更快。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301917095.png" alt="image-20220630191756023"></p><p>用<strong>蒙特卡洛方法</strong>近似期望</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301927935.png" alt="image-20220630192736852"></p><p>$g(a_t)$是对策略梯度的蒙特卡洛近似</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301928979.png" alt="image-20220630192829881"></p><p>$\beta$是学习率，$g(a_t)$是随机梯度</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301934311.png" alt="image-20220630193425221"></p><p>$b$不会影响$g(a_t)$的方差，但是会影响$g(a_t)$的数值，如果$b$的选取恰当，那么会降低$g(a_t)$的方差</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301937138.png" alt="image-20220630193753051"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301943711.png" alt="image-20220630194305623"></p><p>$v_\pi$是$Q_\pi$的期望，所以是比较接近$Q_\pi$的</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202206301943411.png" alt="image-20220630194321330"></p><h4 id="Reinforce-With-Baseline"><a href="#Reinforce-With-Baseline" class="headerlink" title="Reinforce With Baseline"></a>Reinforce With Baseline</h4><p>目标：用Reinforce算法训练策略网络，同时训练价值网络作为Baseline起辅助作用</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207080956544.png" alt="image-20220708095652404"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207080957347.png" alt="image-20220708095731245"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207080958680.png" alt="image-20220708095807575"><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081007461.png" alt="image-20220708100732349"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081008608.png" alt="image-20220708100809527"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081008924.png" alt="image-20220708100821855"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081008511.png" alt="image-20220708100833420"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081008694.png" alt="image-20220708100847600"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081009299.png" alt="image-20220708100909176"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081009665.png" alt="image-20220708100924556"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081010097.png" alt="image-20220708101042979"><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081050899.png" alt="image-20220708105012798"><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081050001.png" alt="image-20220708105039908"><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081050736.png" alt="image-20220708105057629"><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081051077.png" alt="image-20220708105106955"></p><h4 id="A2C-方法"><a href="#A2C-方法" class="headerlink" title="A2C 方法"></a>A2C 方法</h4><p>与AC算法不同的是，AC算法中Critic用的是动作价值函数Q，而A2C方法中用的是状态价值函数V，比Q好训练（Q依赖于S和A，V只依赖于S）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081057700.png" alt="image-20220708105701606"></p><p>也是用到了两个神经网络，结构和上个算法相似</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081059481.png" alt="image-20220708105940359"></p><p>训练方法</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081102089.png" alt="image-20220708110207000"></p><p>数学推导</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081103914.png" alt="image-20220708110346846"></p><h4 id="Reinforce-与-A2C的区别"><a href="#Reinforce-与-A2C的区别" class="headerlink" title="Reinforce 与 A2C的区别"></a>Reinforce 与 A2C的区别</h4><p>神经网络结构一样</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081125371.png" alt="image-20220708112557260"></p><p>区别1：</p><p>价值网络v的用途不一样。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081128931.png" alt="image-20220708112812811"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081128097.png" alt="image-20220708112857997"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081129913.png" alt="image-20220708112920797"></p><p>A2C用的是$y_t$，而Reinforce用的是真实奖励$u_t$</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081133308.png"></p><p><strong>A2C versus Reinforce</strong></p><p>Reinforce 是A2C的一种特例</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081136971.png" alt="image-20220708113614882"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207081137601.png" alt="image-20220708113750488"></p><h1 id="离散控制与连续控制"><a href="#离散控制与连续控制" class="headerlink" title="离散控制与连续控制"></a>离散控制与连续控制</h1><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207150939129.png" alt="image-20220715093949982"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207150940862.png" alt="image-20220715094040757"></p><p>对连续控制的处理1——离散化，适用于自由度比较小的问题</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207150951537.png" alt="image-20220715095118470"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207150953177.png" alt="image-20220715095302122"></p><h4 id="确定策略梯度"><a href="#确定策略梯度" class="headerlink" title="确定策略梯度"></a>确定策略梯度</h4><p>Deterministic Policy Gradient (DPG)</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207150956392.png" alt="image-20220715095607290"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151005493.png" alt="image-20220715100504414"><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151005971.png" alt="image-20220715100516884"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151007662.png" alt="image-20220715100702568"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151010400.png" alt="image-20220715101038285"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151011786.png" alt="image-20220715101129682"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151032442.png" alt="image-20220715103226340"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151033402.png" alt="image-20220715103310295"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151034895.png" alt="image-20220715103445811"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151037751.png" alt="image-20220715103712641"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151038627.png" alt="image-20220715103830560"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151039842.png" alt="image-20220715103907781"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151043305.png" alt="image-20220715104321212"></p><h4 id="随机策略做连续控制"><a href="#随机策略做连续控制" class="headerlink" title="随机策略做连续控制"></a>随机策略做连续控制</h4><h1 id="多智能体强化学习"><a href="#多智能体强化学习" class="headerlink" title="多智能体强化学习"></a>多智能体强化学习</h1><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151905228.png" alt="image-20220715190538158"></p><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151919835.png" alt="image-20220715191928740"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151920369.png" alt="image-20220715192041303"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151922503.png" alt="image-20220715192239403"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207151925848.png" alt="image-20220715192502775"></p><p>Summary</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221116063.png" alt="image-20220722111636957"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221117887.png" alt="image-20220722111756809"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221118368.png" alt="image-20220722111838279"></p><h4 id="Multi-Agent-Reinforcement-Learning（多智能体强化学习）的三种架构"><a href="#Multi-Agent-Reinforcement-Learning（多智能体强化学习）的三种架构" class="headerlink" title="Multi-Agent Reinforcement Learning（多智能体强化学习）的三种架构"></a>Multi-Agent Reinforcement Learning（多智能体强化学习）的三种架构</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221128399.png" alt="image-20220722112802289"></p><p>Summary</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221152013.png" alt="image-20220722115240930"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221153390.png" alt="image-20220722115335299"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221154655.png" alt="image-20220722115407574"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221157760.png" alt="image-20220722115735656"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow%E5%AE%9E%E8%B7%B5%E3%80%8B%20%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8ETensorFlow%E5%AE%9E%E8%B7%B5%E3%80%8B%20%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h3 id="《深度学习与TensorFlow实践》-读书笔记"><a href="#《深度学习与TensorFlow实践》-读书笔记" class="headerlink" title="《深度学习与TensorFlow实践》 读书笔记"></a>《深度学习与TensorFlow实践》 读书笔记</h3><h4 id="导论"><a href="#导论" class="headerlink" title="导论"></a>导论</h4><p>人工智能分支：之间并不是泾渭分明，而是沟壑纵横</p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201252205304.png" alt="image-20220125220545167"></p><p>深度学习 属于可统计不可推理</p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201252215035.png" alt="image-20220125221545914"></p><p>深度学习与传统机器学习算法的比较：高度依赖于数据</p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201252208481.png" alt="image-20220125220834391"></p><ul><li>赫伯特$\cdot$西蒙（1975年图灵奖，1978年诺贝尔经济学奖） 对<code>学习</code>下的定义：”如果一个系统，能够通过执行某个过程，&#x3D;&#x3D;就此改进了它的性能&#x3D;&#x3D;，那么这个过程就是学习“。</li><li>深度学习高度依赖于数据</li><li></li></ul><h4 id="神经网络学习"><a href="#神经网络学习" class="headerlink" title="神经网络学习"></a>神经网络学习</h4><ul><li><p>人工神经网络概念：</p><p>人工神经网络是一种由具有自适应性的简单单元构成的广泛并行互联的网络，它的组织结构能够模拟生物神经系统对真实世界所做出的交互反应</p></li><li></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E3%80%8A%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%8B/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E3%80%8A%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="《吴恩达机器学习笔记》"><a href="#《吴恩达机器学习笔记》" class="headerlink" title="《吴恩达机器学习笔记》"></a>《吴恩达机器学习笔记》</h3><h4 id="第一节"><a href="#第一节" class="headerlink" title="第一节"></a>第一节</h4><ul><li><p><code>关于不知道如何编写无人驾驶直升机的算法程序，让机器自己学习去解决。</code></p></li><li><p>机器学习的定义</p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110212126661.png" alt="image-20211021144834853" style="zoom:50%;" /><p>经验E，性能度量P，任务T</p><p>在跳棋程序自我学习时，E是数百万次的下棋训练，P是程序赢的概率，T是进行下棋</p></li><li><p>主要的两类学习算法</p><ul><li>监督学习（supervised learning)：告诉你正确答案，让你设计算法预测<ul><li>分类问题（classification problem）- - 预测离散值的输出</li><li>回归问题（regression problem）- - 预测连续值的输出</li></ul></li><li>无监督学习(unsupervised learning)：<ul><li>聚类算法（clustering algorithm）例如百度谷歌新闻，用此来聚集相关主题的新闻</li></ul></li></ul></li></ul><h4 id="第二节"><a href="#第二节" class="headerlink" title="第二节"></a>第二节</h4><ul><li><p>一些有用的符号</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110212126841.png" alt="image-20211021153612645"></p></li><li><p>监督学习流程</p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110212126609.png" alt="image-20211021154904515" style="zoom:50%;" /><p>h为假设函数</p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110212126313.png" alt="image-20211021155317778" style="zoom:50%;" /><p><img src="C:\Users\auroras\AppData\Roaming\Typora\typora-user-images\image-20211021161710694.png" alt="image-20211021161710694"></p><p>J为代价函数</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110212126992.png" alt="image-20211021164021139"></p></li><li><p>梯度下降算法</p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110222047355.png" alt="image-20211022204750310" style="zoom: 67%;" /><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110222043758.png" alt="image-20211022204326685"></p><p>$\alpha$  代表学习率（learning rate) 控制梯度下降的幅度</p><p>$\theta_0 \ and \  \theta_1$ 同时更新 </p><p>depending on the initial condition, gradient descent may end up at different local optima.（根据初始条件，梯度下降可能会以不同的局部最优值结束。  ）</p></li><li><p>线性回归算法（用直线模型拟合数据）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110222137924.png" alt="image-20211022213709860"></p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110222207916.png" alt="image-20211022220749841" style="zoom:50%;" /><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110222208332.png" alt="image-20211022220818285"></p></li></ul><h4 id="第三节"><a href="#第三节" class="headerlink" title="第三节"></a>第三节</h4><p>线代一些基础知识</p><h4 id="第四节"><a href="#第四节" class="headerlink" title="第四节"></a>第四节</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110242216715.png" alt="image-20211024221644629"></p><ul><li><p>多元梯度下降法</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110251105335.png" alt="image-20211025110520219"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110251111998.png" alt="image-20211025111136914"></p><ul><li><p>多元梯度下降法中的一些技巧</p><ol><li><p>特征缩放</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201121525377.png" alt="image-20220112152517233"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201121526877.png" alt="image-20220112152626821"></p></li><li><p>学习率</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110251204834.png" alt="image-20211025120448791"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110251538536.png" alt="image-20211025153830470"></p></li></ol></li></ul></li><li><p>特征与多项式回归</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202110260859420.png" alt="image-20211026085858309"></p><p>这时，特征缩放会显得特别重要</p><ul><li><p>正规方程</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202112151112928.png" alt="image-20211215111243725"></p></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202112151455095.png" alt="image-20211215145502988"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202112151500714.png" alt="image-20211215150056625"></p><p>$X\cdot \theta&#x3D;y$</p><p>$X^T\cdot X\cdot \theta&#x3D;X^T\cdot y$</p><p>$\theta&#x3D;(X^TX)^{-1}X^Ty$</p><h4 id="第五节"><a href="#第五节" class="headerlink" title="第五节"></a>第五节</h4><p>代价函数</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202112232055234.png" alt="image-20211223205510965"></p><p> 向量化优化</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202112232113846.png" alt="image-20211223211316758"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202112252205741.png" alt="image-20211225220543633"></p><h4 id="第六节"><a href="#第六节" class="headerlink" title="第六节"></a>第六节</h4><p>logistic 回归算法（分类算法）</p><p>​Logistic 函数 $g(x)&#x3D;\dfrac{1}{1+e^{-z}}$</p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201132027690.png" alt="image-20220113202734597" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201132029980.png" alt="image-20220113202931914" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201132037202.png" alt="image-20220113203704132" style="zoom:50%;" /><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201140942750.png" alt="image-20220114094249611"></p><p>决策边界(decision boundary)是决策函数的属性,不是训练集的属性，我们使用训练集来拟合参数$\theta$ </p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201141006268.png" alt="image-20220114100638181" style="zoom:50%;" /><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201141020605.png" alt="image-20220114102020541"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201141031949.png" alt="image-20220114103133853"></p><p>逻辑回归的代价函数无法使用梯度下降算法来收敛到全局最优（因为很容易使其收敛到局部最优）</p><p>因此需要将代价函数变形，使其可以使用梯度下降算法求解（使其变为凸函数，可以进行凸优化）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201141048681.png" alt="image-20220114104800605"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201141050539.png" alt="image-20220114105027482"></p><p>我们可以将分段函数写为一个函数</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201141516788.png" alt="image-20220114151642747"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201141519128.png" alt="image-20220114151909068"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201141527133.png" alt="image-20220114152758071"></p><p>虽然最后逻辑回归中梯度下降求解的形式和线性规划中一致，但是$h_\theta$ 函数并不一样。不过特征缩放也可适用于逻辑回归</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201141532922.png" alt="image-20220114153203847"></p><p>一些比梯度下降算法更高级的优化算法（收敛更快）：</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201261719680.png" alt="image-20220126171929560"></p><p>高级算法代码（可以看做加强版的梯度下降）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201281033531.png" alt="image-20220128103313394"></p><p><strong>代价函数伪代码：</strong></p><p>返回代价函数值和梯度值</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201281040997.png" alt="image-20220128104015933"></p><p>多类别分类问题 中一对多方法：</p><p>将多分类转化为若干个二分类问题，然后找概率y值最高的一个输出作为结果</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201281103240.png" alt="image-20220128110346167"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201281104387.png" alt="image-20220128110434324"></p><h4 id="第七节"><a href="#第七节" class="headerlink" title="第七节"></a>第七节</h4><p><strong>欠拟合（underfitting）high bias</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201281110193.png" alt="image-20220128111059161"></p><p><strong>过度拟合问题（Overfitting)：</strong></p><p>过多变量并且较少数据时往往出现（高阶多项式）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201281121445.png" alt="image-20220128112117409"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201281111307.png" alt="image-20220128111111280"></p><p>解决方法：</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201281137445.png" alt="image-20220128113718389"></p><p><strong>正则化(Regularization):</strong></p><p>加入惩罚，使参数尽量小，简化假设模型（不减少特征）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201281820210.png" alt="image-20220128182015169"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201281822976.png" alt="image-20220128182224920"></p><p>例如房屋预测模型，加入正则项，目的是使得$\theta$尽可能的小 </p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201281830847.png" alt="image-20220128183043805"></p><p>正则化参数$\lambda$ 作用有两个，一个是作为正则项拟合数据，另一个引入惩罚是为了使$\theta$ 尽可能的小，避免过拟合。</p><p>但是如果$\lambda$ 值太大，那么引入的惩罚过大，导致所有$\theta$都接近为0,会出现欠拟合</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201282039648.png" alt="image-20220128203938583"></p><p><strong>线性回归的正则化</strong></p><p>梯度下降</p><p>引入$\lambda$进行梯度下降，$1-\alpha\dfrac{\lambda}{m}$   $&lt; 1$ ，（m是一个比较大的数），所以$\theta$会不断缩小  </p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201282050851.png" alt="image-20220128205003746"></p><p>正规化</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201282128553.png" alt="image-20220128212825481"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201282131000.png" alt="image-20220128213142933"></p><p>Logistic 回归正则化（逻辑回归）：</p><p>公式形式与线性回归一致，但是逻辑回归引入了逻辑函数</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201290945558.png" alt="image-20220129094530459"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201291007682.png" alt="image-20220129100731596"></p><h4 id="第八节-神经网络"><a href="#第八节-神经网络" class="headerlink" title="第八节 神经网络"></a>第八节 神经网络</h4><p>非线性假设</p><p>神经网络：</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201292100471.png" alt="image-20220129210041315"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201292133066.png" alt="image-20220129213315983"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201300956548.png" alt="image-20220130095625313"></p><p>前向传播（向量化计算h(x)）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201301054412.png" alt="image-20220130105444175"></p><p>神经网络训练自己的特征</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201301119357.png" alt="image-20220130111926174"></p><p>逻辑与</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201301958627.png" alt="image-20220130195847421"></p><p>或运算</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201302000983.png" alt="image-20220130200022919"></p><p>非运算</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201302007357.png" alt="image-20220130200757305"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202201311012831.png" alt="image-20220131101241701"></p><h4 id="第九节"><a href="#第九节" class="headerlink" title="第九节"></a>第九节</h4><p><strong>代价函数</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081000288.png" alt="image-20220208100001146"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081005419.png" alt="image-20220208100537324"></p><p><strong>反向传播算法</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081134056.png" alt="image-20220208113426931"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081142806.png" alt="image-20220208114239689"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081436638.png" alt="image-20220208143617504"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081447236.png" alt="image-20220208144719135"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081449422.png" alt="image-20220208144958299"></p><p>李宏毅解释的反向传播</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203091705990.png" alt="image-20220309170545896"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203091647565.png" alt="image-20220309164655425"></p><p>显然对于$\partial z &#x2F;\partial w$(z&#x3D;x1$\cdot$w1+x2$\cdot$w2+b ) ，$w_i$的偏微分就是前面的”输入” $x_i$，这就是一个Forward pass 的过程</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203091657276.png" alt="image-20220309165707170"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203092009636.png" alt="image-20220309200910519"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203092017515.png" alt="image-20220309201704425"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203092024861.png" alt="image-20220309202412758"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203092105772.png" alt="image-20220309210528647"></p><p>Backward Pass，就类似于反向建立Neural Network，计算$\partial C&#x2F;\partial z$ </p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203092122572.png" alt="image-20220309212208461"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203092132282.png" alt="image-20220309213234182"></p><p><strong>梯度检测</strong></p><p>为了防止数值上的问题，一般$\theta$ 不会取太小，常取1e-4</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081556103.png" alt="image-20220208155653992"><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081604882.png" alt="image-20220208160443758"></p><p>估算偏导数</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081557771.png" alt="image-20220208155723681"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081627602.png" alt="image-20220208162733480"></p><p><strong>随机初始化</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081630380.png" alt="image-20220208163039289"></p><p>零初始化对神经网络是没有意义的</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081636223.png" alt="image-20220208163655133"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202081640134.png" alt="image-20220208164049051"></p><p><strong>如何训练神经网络</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202102145609.png" alt="image-20220210214521479"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202102148080.png" alt="image-20220210214851974"></p><h4 id="第十节-决定下一步做什么"><a href="#第十节-决定下一步做什么" class="headerlink" title="第十节 决定下一步做什么"></a>第十节 决定下一步做什么</h4><p>无所谓的尝试可能浪费很多时间</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202111000808.png" alt="image-20220211100013709"></p><p>机器学习诊断法</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202111012259.png" alt="image-20220211101233178"></p><p><strong>评估假设</strong></p><p>前<code>70%</code>作为训练集，后<code>30%</code>作为测试集</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202111018926.png" alt="image-20220211101845822"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202111025656.png" alt="image-20220211102545572"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202111035176.png" alt="image-20220211103505072"></p><p><strong>模型的选择、训练、验证、测试</strong></p><p>对于测试集拟合的产生的$\theta$新的数据可能拟合效果不好</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202121950911.png" alt="image-20220212195032766"></p><p>划分数据集合为 训练集（train 教科书），交叉验证集（cv 课后 作业），测试集（test 期末考试）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202121955712.png" alt="image-20220212195530598"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202121957723.png" alt="image-20220212195720648"></p><p>用训练集拟合得到$\theta$ ，然后用交叉验证集来计算 J error（泛化误差），选择最小的一个作为d</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202122012505.png" alt="image-20220212201256417"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202122122700.png" alt="image-20220212212227608"></p><p><strong>偏差（Bias)与方差(Variance)</strong></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202122128783.png" alt="image-20220212212825677"></p><p>正则化与偏差(欠拟合）、方差（过拟合）</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202140948300.png" alt="image-20220214094837168"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202140949661.png" alt="image-20220214094906575"></p><p>用J（包含正则化项）来求theta，然后为了比较lameda对theta的影响，用Jtrain和Jcv绘制曲线（不包含正则化项）。其实训练时用的是J，而Jtrain和Jcv只是用来画线说明问题</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202140959553.png" alt="image-20220214095925447"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202141011222.png" alt="image-20220214101149116"></p><p>学习曲线绘制</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202141045317.png" alt="image-20220214104559231"></p><p>高偏差</p><p>高偏差下，训练集大小对于 降低 J error 没大作用</p><p><img src="C:/Users/auroras/AppData/Roaming/Typora/typora-user-images/image-20220214105627219.png" alt="image-20220214105627219"></p><p>高方差下，增加训练集，对降低J error 是有效的</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202141104203.png" alt="image-20220214110410108"></p><p>根据学习曲线改进学习方法（尽量不做徒劳工作</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202141120393.png" alt="image-20220214112049277"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202202141126650.png" alt="image-20220214112620550"></p><h4 id="第十一节"><a href="#第十一节" class="headerlink" title="第十一节"></a>第十一节</h4><p>误差分析</p><p>不对称性分类的误差评估</p><p>混淆矩阵 &amp; 准确率、精确率、召回率</p><p>高的准确率和召回率可以评价一个算法是不是足够好</p><p>通过F值来判断算法的好坏</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203042051627.png" alt="image-20220304205153520"></p><h4 id="第十二节-SVM"><a href="#第十二节-SVM" class="headerlink" title="第十二节 SVM"></a>第十二节 SVM</h4><p>SVM是监督学习算法</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203051425964.png" alt="image-20220305142524731"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202203051929668.png" alt="image-20220305192952595"></p><h4 id="补充的知识点"><a href="#补充的知识点" class="headerlink" title="补充的知识点"></a>补充的知识点</h4><h5 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h5><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202112312022755.png" alt="image-20211231202240287"></p><h4 id="相关术语"><a href="#相关术语" class="headerlink" title="相关术语"></a>相关术语</h4><p>线性回归  Linear regression<br>单变量线性回归 Linear regression with one variable</p><p>代价函数  Cost Function<br>平方误差代价函数 Squared error cost function<br>建模误差  Modeling error<br>等高线　　contour plot 、contour figure<br>梯度下降  Gradient descent<br>批处理梯度下降   Batch gradient descent</p><p>学习效率 　Learning rate<br>同步更新 simultaneous update<br>非同步更新 non-simultaneous update</p><p>局部最优  local optimum<br>全局最优  global optimum<br>全局最小值   global minimum<br>局部最小值   local minimum</p><p>微分项    derivative term<br>微积分    calculus<br>导数　　　 derivatives<br>偏导数    partial derivatives<br>负导数    nagative derivative<br>负斜率    nagative slope</p><p>收敛      converge<br>发散      diverge<br>陡峭      steep<br>碗型      bow-shaped function<br>凸函数    convex function</p><p>线性代数   linear algebra<br>迭代算法   iterative algorithm<br>正规方程组   normal equations methods<br>梯度下降的泛化   a generalization of the gradient descent algorithm<br>越过最低点    overshoot the minimum</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E3%80%8A%E5%88%A9%E7%94%A8Python%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%C2%B7%E7%AC%AC2%E7%89%88%E3%80%8B/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E3%80%8A%E5%88%A9%E7%94%A8Python%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%C2%B7%E7%AC%AC2%E7%89%88%E3%80%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="《利用Python进行数据分析·第2版》"><a href="#《利用Python进行数据分析·第2版》" class="headerlink" title="《利用Python进行数据分析·第2版》"></a>《利用Python进行数据分析·第2版》</h1><h2 id="前置-requirements-txt"><a href="#前置-requirements-txt" class="headerlink" title="前置 requirements.txt"></a>前置 requirements.txt</h2><p>python项目如何在另一个环境上重新构建项目所需要的运行环境依赖包？</p><p>使用的时候边记载是个很麻烦的事情，总会出现遗漏的包的问题，这个时候手动安装也很麻烦，不能确定代码报错的需要安装的包是什么版本。这些问题，requirements.txt都可以解决！</p><p>生成requirements.txt，有两种方式：</p><p>第一种 适用于 单虚拟环境的情况： ：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">pip <span class="hljs-keyword">freeze</span> &gt; requirements.txt<br></code></pre></td></tr></table></figure><p>为什么只适用于单虚拟环境？因为这种方式，会将环境中的依赖包全都加入，如果使用的全局环境，则下载的所有包都会在里面，不管是不时当前项目依赖的，如下图</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207242106058.png" alt="image-20220724210619993"></p><p>当然这种情况并不是我们想要的，当我们使用的是全局环境时，可以使用第二种方法。</p><p>第二种 (推荐) 使用 pipreqs ，github地址为： <a href="https://github.com/bndr/pipreqs">https://github.com/bndr/pipreqs</a></p><p># 安装</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> pipreqs<br></code></pre></td></tr></table></figure><p># 在当前目录生成</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs brainfuck"><span class="hljs-comment">pipreqs</span> <span class="hljs-string">.</span> <span class="hljs-literal">--</span><span class="hljs-comment">encoding=utf8</span> <span class="hljs-literal">--</span><span class="hljs-comment">force</span><br></code></pre></td></tr></table></figure><p>注意 –encoding&#x3D;utf8 为使用utf8编码，不然可能会报UnicodeDecodeError: ‘gbk’ codec can’t decode byte 0xae in position 406: illegal multibyte sequence 的错误。</p><p>–force 强制执行，当 生成目录下的requirements.txt存在时覆盖。</p><p>当当当，可以看见我依赖的只有这些啦</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207242107497.png" alt="image-20220724210714441"></p><p>使用requirements.txt安装依赖的方式：</p><p>pip install -r requirements.txt</p><h2 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h2><p> <strong>重要的Python库</strong></p><p>NumPy</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207141842308.png" alt="image-20220714184200197"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207141844506.png" alt="image-20220714184405458"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207141846643.png" alt="image-20220714184616600"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207141850242.png" alt="image-20220714185004185"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207141851874.png" alt="image-20220714185100829"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207141852340.png" alt="image-20220714185216285"></p><p>引入惯例</p><p>Python社区已经广泛采取了一些常用模块的命名惯例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">import</span> statsmodels <span class="hljs-keyword">as</span> sm<br></code></pre></td></tr></table></figure><h2 id="第九章-绘图和可视化"><a href="#第九章-绘图和可视化" class="headerlink" title="第九章 绘图和可视化"></a>第九章 绘图和可视化</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221559781.png" alt="image-20220722155938732"></p><p>学习本章代码案例的最简单方法是在Jupyter notebook进行交互式绘图。在Jupyter notebook中执行下面的语句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib notebook<br></code></pre></td></tr></table></figure><h3 id="9-1-matplotlib-API入门"><a href="#9-1-matplotlib-API入门" class="headerlink" title="9.1 matplotlib API入门"></a>9.1 matplotlib API入门</h3><p>matplotlib的通常引入约定是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">11</span>]: <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br></code></pre></td></tr></table></figure><p>在Jupyter中运行%matplotlib notebook（或在IPython中运行%matplotlib），就可以创建一个简单的图形。如果一切设置正确，会看到图9-1：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">12</span>]: <span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>In [<span class="hljs-number">13</span>]: data = np.arange(<span class="hljs-number">10</span>)<br><br>In [<span class="hljs-number">14</span>]: data<br>Out[<span class="hljs-number">14</span>]: array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>])<br><br>In [<span class="hljs-number">15</span>]: plt.plot(data)<br></code></pre></td></tr></table></figure><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221603162.png" alt="image-20220722160327118" style="zoom:50%;" /><p>图9-1 简单的线图</p><p>虽然seaborn这样的库和pandas的内置绘图函数能够处理许多普通的绘图任务，但如果需要自定义一些高级功能的话就必须学习matplotlib API。</p><blockquote><p>笔记：虽然本书没有详细地讨论matplotlib的各种功能，但足以将你引入门。matplotlib的示例库和文档是学习高级特性的最好资源。</p></blockquote><p><strong>Figure和Subplot</strong></p><p>matplotlib的图像都位于Figure对象中。你可以用plt.figure创建一个新的Figure：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">16</span>]: fig = plt.figure()<br></code></pre></td></tr></table></figure><p>如果用的是IPython，这时会弹出一个空窗口，但在Jupyter中，必须再输入更多命令才能看到。plt.figure有一些选项，特别是figsize，它用于确保当图片保存到磁盘时具有一定的大小和纵横比。</p><p>不能通过空Figure绘图。必须用add_subplot创建一个或多个subplot才行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">17</span>]: ax1 = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>这条代码的意思是：图像应该是2×2的（即最多4张图），且当前选中的是4个subplot中的第一个（编号从1开始）。如果再把后面两个subplot也创建出来，最终得到的图像如图9-2所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">18</span>]: ax2 = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><br>In [<span class="hljs-number">19</span>]: ax3 = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221611789.png" alt="image-20220722161101728"></p><blockquote><p>提示：使用Jupyter notebook有一点不同，即每个小窗重新执行后，图形会被重置。因此，对于复杂的图形，，你必须将所有的绘图命令存在一个小窗里。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">fig = plt.figure()<br>ax1 = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>ax2 = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>ax3 = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p>如果这时执行一条绘图命令（如plt.plot([1.5, 3.5, -2, 1.6])），matplotlib就会在最后一个用过的subplot（如果没有则创建一个）上进行绘制，隐藏创建figure和subplot的过程。因此，如果我们执行下列命令，你就会得到如图9-3所示的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">20</span>]: plt.plot(np.random.randn(<span class="hljs-number">50</span>).cumsum(), <span class="hljs-string">&#x27;k--&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221614012.png" alt="image-20220722161420954"></p><p>“k–”是一个线型选项，用于告诉matplotlib绘制黑色虚线图。上面那些由fig.add_subplot所返回的对象是AxesSubplot对象，直接调用它们的实例方法就可以在其它空着的格子里面画图了，如图9-4所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">21</span>]: ax1.hist(np.random.randn(<span class="hljs-number">100</span>), bins=<span class="hljs-number">20</span>, color=<span class="hljs-string">&#x27;k&#x27;</span>, alpha=<span class="hljs-number">0.3</span>)<br><br>In [<span class="hljs-number">22</span>]: ax2.scatter(np.arange(<span class="hljs-number">30</span>), np.arange(<span class="hljs-number">30</span>) + <span class="hljs-number">3</span> * np.random.randn(<span class="hljs-number">30</span>))<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221615217.png" alt="image-20220722161540156"></p><p>你可以在matplotlib的文档中找到各种图表类型。</p><p>创建包含subplot网格的figure是一个非常常见的任务，matplotlib有一个更为方便的方法plt.subplots，它可以创建一个新的Figure，并返回一个含有已创建的subplot对象的NumPy数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">24</span>]: fig, axes = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><br>In [<span class="hljs-number">25</span>]: axes<br>Out[<span class="hljs-number">25</span>]: <br>array([[&lt;matplotlib.axes._subplots.AxesSubplot <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7fb626374048</span>&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7fb62625db00</span>&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7fb6262f6c88</span>&gt;],<br>       [&lt;matplotlib.axes._subplots.AxesSubplot <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7fb6261a36a0</span>&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7fb626181860</span>&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7fb6260fd4e0</span>&gt;]], dtype<br>=<span class="hljs-built_in">object</span>)<br></code></pre></td></tr></table></figure><p>这是非常实用的，因为可以轻松地对axes数组进行索引，就好像是一个二维数组一样，例如axes[0,1]。你还可以通过sharex和sharey指定subplot应该具有相同的X轴或Y轴。在比较相同范围的数据时，这也是非常实用的，否则，matplotlib会自动缩放各图表的界限。有关该方法的更多信息，请参见表9-1。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221638492.png" alt="image-20220722163849434"></p><p><strong>调整subplot周围的间距</strong></p><p>默认情况下，matplotlib会在subplot外围留下一定的边距，并在subplot之间留下一定的间距。间距跟图像的高度和宽度有关，因此，如果你调整了图像大小（不管是编程还是手工），间距也会自动调整。利用Figure的subplots_adjust方法可以轻而易举地修改间距，此外，它也是个顶级函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">subplots_adjust(left=<span class="hljs-literal">None</span>, bottom=<span class="hljs-literal">None</span>, right=<span class="hljs-literal">None</span>, top=<span class="hljs-literal">None</span>,<br>                wspace=<span class="hljs-literal">None</span>, hspace=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><p>wspace和hspace用于控制宽度和高度的百分比，可以用作subplot之间的间距。下面是一个简单的例子，其中我将间距收缩到了0（如图9-5所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">fig, axes = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, sharex=<span class="hljs-literal">True</span>, sharey=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>        axes[i, j].hist(np.random.randn(<span class="hljs-number">500</span>), bins=<span class="hljs-number">50</span>, color=<span class="hljs-string">&#x27;k&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br>plt.subplots_adjust(wspace=<span class="hljs-number">0</span>, hspace=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221640911.png" alt="image-20220722164012848"></p><p>不难看出，其中的轴标签重叠了。matplotlib不会检查标签是否重叠，所以对于这种情况，你只能自己设定刻度位置和刻度标签。后面几节将会详细介绍该内容。</p><p><strong>颜色、标记和线型</strong></p><p>matplotlib的plot函数接受一组X和Y坐标，还可以接受一个表示颜色和线型的字符串缩写。例如，要根据x和y绘制绿色虚线，你可以执行如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">ax.plot(x, y, <span class="hljs-string">&#x27;g--&#x27;</span>)<br></code></pre></td></tr></table></figure><p>这种在一个字符串中指定颜色和线型的方式非常方便。在实际中，如果你是用代码绘图，你可能不想通过处理字符串来获得想要的格式。通过下面这种更为明确的方式也能得到同样的效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">ax.plot(x, y, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, color=<span class="hljs-string">&#x27;g&#x27;</span>)<br></code></pre></td></tr></table></figure><p>常用的颜色可以使用颜色缩写，你也可以指定颜色码（例如，’#CECECE’）。你可以通过查看plot的文档字符串查看所有线型的合集（在IPython和Jupyter中使用plot?）。</p><p>线图可以使用标记强调数据点。因为matplotlib可以创建连续线图，在点之间进行插值，因此有时可能不太容易看出真实数据点的位置。标记也可以放到格式字符串中，但标记类型和线型必须放在颜色后面（见图9-6）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">30</span>]: <span class="hljs-keyword">from</span> numpy.random <span class="hljs-keyword">import</span> randn<br><br>In [<span class="hljs-number">31</span>]: plt.plot(randn(<span class="hljs-number">30</span>).cumsum(), <span class="hljs-string">&#x27;ko--&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221643184.png" alt="image-20220722164303120"></p><p>还可以将其写成更为明确的形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plot(randn(<span class="hljs-number">30</span>).cumsum(), color=<span class="hljs-string">&#x27;k&#x27;</span>, linestyle=<span class="hljs-string">&#x27;dashed&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br></code></pre></td></tr></table></figure><p>在线型图中，非实际数据点默认是按线性方式插值的。可以通过drawstyle选项修改（见图9-7）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">33</span>]: data = np.random.randn(<span class="hljs-number">30</span>).cumsum()<br><br>In [<span class="hljs-number">34</span>]: plt.plot(data, <span class="hljs-string">&#x27;k--&#x27;</span>, label=<span class="hljs-string">&#x27;Default&#x27;</span>)<br>Out[<span class="hljs-number">34</span>]: [&lt;matplotlib.lines.Line2D at <span class="hljs-number">0x7fb624d86160</span>&gt;]<br><br>In [<span class="hljs-number">35</span>]: plt.plot(data, <span class="hljs-string">&#x27;k-&#x27;</span>, drawstyle=<span class="hljs-string">&#x27;steps-post&#x27;</span>, label=<span class="hljs-string">&#x27;steps-post&#x27;</span>)<br>Out[<span class="hljs-number">35</span>]: [&lt;matplotlib.lines.Line2D at <span class="hljs-number">0x7fb624d869e8</span>&gt;]<br><br>In [<span class="hljs-number">36</span>]: plt.legend(loc=<span class="hljs-string">&#x27;best&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221644920.png" alt="image-20220722164452858"></p><p>你可能注意到运行上面代码时有输出&lt;matplotlib.lines.Line2D at …&gt;。matplotlib会返回引用了新添加的子组件的对象。大多数时候，你可以放心地忽略这些输出。这里，因为我们传递了label参数到plot，我们可以创建一个plot图例，指明每条使用plt.legend的线。</p><blockquote><p>笔记：你必须调用plt.legend（或使用ax.legend，如果引用了轴的话）来创建图例，无论你绘图时是否传递label标签选项。</p></blockquote><p><strong>刻度、标签和图例</strong></p><p>对于大多数的图表装饰项，其主要实现方式有二：使用过程型的pyplot接口（例如，matplotlib.pyplot）以及更为面向对象的原生matplotlib API。</p><p>pyplot接口的设计目的就是交互式使用，含有诸如xlim、xticks和xticklabels之类的方法。它们分别控制图表的范围、刻度位置、刻度标签等。其使用方式有以下两种：</p><ul><li>调用时不带参数，则返回当前的参数值（例如，plt.xlim()返回当前的X轴绘图范围）。</li><li>调用时带参数，则设置参数值（例如，plt.xlim([0,10])会将X轴的范围设置为0到10）。</li></ul><p>所有这些方法都是对当前或最近创建的AxesSubplot起作用的。它们各自对应subplot对象上的两个方法，以xlim为例，就是ax.get_xlim和ax.set_xlim。我更喜欢使用subplot的实例方法（因为我喜欢明确的事情，而且在处理多个subplot时这样也更清楚一些）。当然你完全可以选择自己觉得方便的那个。</p><p><strong>设置标题、轴标签、刻度以及刻度标签</strong></p><p>为了说明自定义轴，我将创建一个简单的图像并绘制一段随机漫步（如图9-8所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">37</span>]: fig = plt.figure()<br><br>In [<span class="hljs-number">38</span>]: ax = fig.add_subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>In [<span class="hljs-number">39</span>]: ax.plot(np.random.randn(<span class="hljs-number">1000</span>).cumsum())<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221647044.png" alt="image-20220722164714972"></p><p>要改变x轴刻度，最简单的办法是使用set_xticks和set_xticklabels。前者告诉matplotlib要将刻度放在数据范围中的哪些位置，默认情况下，这些位置也就是刻度标签。但我们可以通过set_xticklabels将任何其他的值用作标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">40</span>]: ticks = ax.set_xticks([<span class="hljs-number">0</span>, <span class="hljs-number">250</span>, <span class="hljs-number">500</span>, <span class="hljs-number">750</span>, <span class="hljs-number">1000</span>])<br><br>In [<span class="hljs-number">41</span>]: labels = ax.set_xticklabels([<span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>, <span class="hljs-string">&#x27;three&#x27;</span>, <span class="hljs-string">&#x27;four&#x27;</span>, <span class="hljs-string">&#x27;five&#x27;</span>],<br>   ....:                             rotation=<span class="hljs-number">30</span>, fontsize=<span class="hljs-string">&#x27;small&#x27;</span>)<br></code></pre></td></tr></table></figure><p>rotation选项设定x刻度标签倾斜30度。最后，再用set_xlabel为X轴设置一个名称，并用set_title设置一个标题（见图9-9的结果）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">42</span>]: ax.set_title(<span class="hljs-string">&#x27;My first matplotlib plot&#x27;</span>)<br>Out[<span class="hljs-number">42</span>]: &lt;matplotlib.text.Text at <span class="hljs-number">0x7fb624d055f8</span>&gt;<br><br>In [<span class="hljs-number">43</span>]: ax.set_xlabel(<span class="hljs-string">&#x27;Stages&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221653445.png" alt="image-20220722165330377"></p><p>Y轴的修改方式与此类似，只需将上述代码中的x替换为y即可。轴的类有集合方法，可以批量设定绘图选项。前面的例子，也可以写为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">props = &#123;<br>    <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;My first matplotlib plot&#x27;</span>,<br>    <span class="hljs-string">&#x27;xlabel&#x27;</span>: <span class="hljs-string">&#x27;Stages&#x27;</span><br>&#125;<br>ax.<span class="hljs-built_in">set</span>(**props)<br></code></pre></td></tr></table></figure><p><strong>添加图例</strong></p><p>图例（legend）是另一种用于标识图表元素的重要工具。添加图例的方式有多种。最简单的是在添加subplot的时候传入label参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">44</span>]: <span class="hljs-keyword">from</span> numpy.random <span class="hljs-keyword">import</span> randn<br><br>In [<span class="hljs-number">45</span>]: fig = plt.figure(); ax = fig.add_subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>In [<span class="hljs-number">46</span>]: ax.plot(randn(<span class="hljs-number">1000</span>).cumsum(), <span class="hljs-string">&#x27;k&#x27;</span>, label=<span class="hljs-string">&#x27;one&#x27;</span>)<br>Out[<span class="hljs-number">46</span>]: [&lt;matplotlib.lines.Line2D at <span class="hljs-number">0x7fb624bdf860</span>&gt;]<br><br>In [<span class="hljs-number">47</span>]: ax.plot(randn(<span class="hljs-number">1000</span>).cumsum(), <span class="hljs-string">&#x27;k--&#x27;</span>, label=<span class="hljs-string">&#x27;two&#x27;</span>)<br>Out[<span class="hljs-number">47</span>]: [&lt;matplotlib.lines.Line2D at <span class="hljs-number">0x7fb624be90f0</span>&gt;]<br><br>In [<span class="hljs-number">48</span>]: ax.plot(randn(<span class="hljs-number">1000</span>).cumsum(), <span class="hljs-string">&#x27;k.&#x27;</span>, label=<span class="hljs-string">&#x27;three&#x27;</span>)<br>Out[<span class="hljs-number">48</span>]: [&lt;matplotlib.lines.Line2D at <span class="hljs-number">0x7fb624be9160</span>&gt;]<br></code></pre></td></tr></table></figure><p>在此之后，你可以调用ax.legend()或plt.legend()来自动创建图例（结果见图9-10）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">49</span>]: ax.legend(loc=<span class="hljs-string">&#x27;best&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221659843.png" alt="image-20220722165900772"></p><p>legend方法有几个其它的loc位置参数选项。请查看文档字符串（使用ax.legend?）。</p><p>loc告诉matplotlib要将图例放在哪。如果你不是吹毛求疵的话，”best”是不错的选择，因为它会选择最不碍事的位置。要从图例中去除一个或多个元素，不传入label或传入label&#x3D;’<em>nolegend</em>‘即可。（中文第一版这里把best错写成了beat）</p><p><strong>注解以及在Subplot上绘图</strong></p><p>除标准的绘图类型，你可能还希望绘制一些子集的注解，可能是文本、箭头或其他图形等。注解和文字可以通过text、arrow和annotate函数进行添加。text可以将文本绘制在图表的指定坐标(x,y)，还可以加上一些自定义格式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">ax.text(x, y, <span class="hljs-string">&#x27;Hello world!&#x27;</span>,<br>        family=<span class="hljs-string">&#x27;monospace&#x27;</span>, fontsize=<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p>注解中可以既含有文本也含有箭头。例如，我们根据最近的标准普尔500指数价格（来自Yahoo!Finance）绘制一张曲线图，并标出2008年到2009年金融危机期间的一些重要日期。你可以在Jupyter notebook的一个小窗中试验这段代码（图9-11是结果）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime<br><br>fig = plt.figure()<br>ax = fig.add_subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>data = pd.read_csv(<span class="hljs-string">&#x27;examples/spx.csv&#x27;</span>, index_col=<span class="hljs-number">0</span>, parse_dates=<span class="hljs-literal">True</span>)<br>spx = data[<span class="hljs-string">&#x27;SPX&#x27;</span>]<br><br>spx.plot(ax=ax, style=<span class="hljs-string">&#x27;k-&#x27;</span>)<br><br>crisis_data = [<br>    (datetime(<span class="hljs-number">2007</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>), <span class="hljs-string">&#x27;Peak of bull market&#x27;</span>),<br>    (datetime(<span class="hljs-number">2008</span>, <span class="hljs-number">3</span>, <span class="hljs-number">12</span>), <span class="hljs-string">&#x27;Bear Stearns Fails&#x27;</span>),<br>    (datetime(<span class="hljs-number">2008</span>, <span class="hljs-number">9</span>, <span class="hljs-number">15</span>), <span class="hljs-string">&#x27;Lehman Bankruptcy&#x27;</span>)<br>]<br><br><span class="hljs-keyword">for</span> date, label <span class="hljs-keyword">in</span> crisis_data:<br>    ax.annotate(label, xy=(date, spx.asof(date) + <span class="hljs-number">75</span>),<br>                xytext=(date, spx.asof(date) + <span class="hljs-number">225</span>),<br>                arrowprops=<span class="hljs-built_in">dict</span>(facecolor=<span class="hljs-string">&#x27;black&#x27;</span>, headwidth=<span class="hljs-number">4</span>, width=<span class="hljs-number">2</span>,<br>                                headlength=<span class="hljs-number">4</span>),<br>                horizontalalignment=<span class="hljs-string">&#x27;left&#x27;</span>, verticalalignment=<span class="hljs-string">&#x27;top&#x27;</span>)<br><br><span class="hljs-comment"># Zoom in on 2007-2010</span><br>ax.set_xlim([<span class="hljs-string">&#x27;1/1/2007&#x27;</span>, <span class="hljs-string">&#x27;1/1/2011&#x27;</span>])<br>ax.set_ylim([<span class="hljs-number">600</span>, <span class="hljs-number">1800</span>])<br><br>ax.set_title(<span class="hljs-string">&#x27;Important dates in the 2008-2009 financial crisis&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231014940.png" alt="image-20220723101428755"></p><p>这张图中有几个重要的点要强调：ax.annotate方法可以在指定的x和y坐标轴绘制标签。我们使用set_xlim和set_ylim人工设定起始和结束边界，而不使用matplotlib的默认方法。最后，用ax.set_title添加图标标题。</p><p>更多有关注解的示例，请访问matplotlib的在线示例库。</p><p>图形的绘制要麻烦一些。matplotlib有一些表示常见图形的对象。这些对象被称为块（patch）。其中有些（如Rectangle和Circle），可以在matplotlib.pyplot中找到，但完整集合位于matplotlib.patches。</p><p>要在图表中添加一个图形，你需要创建一个块对象shp，然后通过ax.add_patch(shp)将其添加到subplot中（如图9-12所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">fig = plt.figure()<br>ax = fig.add_subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>rect = plt.Rectangle((<span class="hljs-number">0.2</span>, <span class="hljs-number">0.75</span>), <span class="hljs-number">0.4</span>, <span class="hljs-number">0.15</span>, color=<span class="hljs-string">&#x27;k&#x27;</span>, alpha=<span class="hljs-number">0.3</span>)<br>circ = plt.Circle((<span class="hljs-number">0.7</span>, <span class="hljs-number">0.2</span>), <span class="hljs-number">0.15</span>, color=<span class="hljs-string">&#x27;b&#x27;</span>, alpha=<span class="hljs-number">0.3</span>)<br>pgon = plt.Polygon([[<span class="hljs-number">0.15</span>, <span class="hljs-number">0.15</span>], [<span class="hljs-number">0.35</span>, <span class="hljs-number">0.4</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.6</span>]],<br>                   color=<span class="hljs-string">&#x27;g&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br><br>ax.add_patch(rect)<br>ax.add_patch(circ)<br>ax.add_patch(pgon)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231019981.png" alt="image-20220723101900879"></p><p>如果查看许多常见图表对象的具体实现代码，你就会发现它们其实就是由块patch组装而成的。</p><p><strong>将图表保存到文件</strong></p><p>利用plt.savefig可以将当前图表保存到文件。该方法相当于Figure对象的实例方法savefig。例如，要将图表保存为SVG文件，你只需输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.savefig(<span class="hljs-string">&#x27;figpath.svg&#x27;</span>)<br></code></pre></td></tr></table></figure><p>文件类型是通过文件扩展名推断出来的。因此，如果你使用的是.pdf，就会得到一个PDF文件。我在发布图片时最常用到两个重要的选项是dpi（控制“每英寸点数”分辨率）和bbox_inches（可以剪除当前图表周围的空白部分）。要得到一张带有最小白边且分辨率为400DPI的PNG图片，你可以：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.savefig(<span class="hljs-string">&#x27;figpath.png&#x27;</span>, dpi=<span class="hljs-number">400</span>, bbox_inches=<span class="hljs-string">&#x27;tight&#x27;</span>)<br></code></pre></td></tr></table></figure><p>savefig并非一定要写入磁盘，也可以写入任何文件型的对象，比如BytesIO：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO<br>buffer = BytesIO()<br>plt.savefig(buffer)<br>plot_data = buffer.getvalue()<br></code></pre></td></tr></table></figure><p>表9-2列出了savefig的其它选项。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231025861.png" alt="image-20220723102514760"></p><p><strong>matplotlib配置</strong></p><p>matplotlib自带一些配色方案，以及为生成出版质量的图片而设定的默认配置信息。幸运的是，几乎所有默认行为都能通过一组全局参数进行自定义，它们可以管理图像大小、subplot边距、配色方案、字体大小、网格类型等。一种Python编程方式配置系统的方法是使用rc方法。例如，要将全局的图像默认大小设置为10×10，你可以执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.rc(<span class="hljs-string">&#x27;figure&#x27;</span>, figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><p>rc的第一个参数是希望自定义的对象，如’figure’、’axes’、’xtick’、’ytick’、’grid’、’legend’等。其后可以跟上一系列的关键字参数。一个简单的办法是将这些选项写成一个字典：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">font_options = &#123;<span class="hljs-string">&#x27;family&#x27;</span> : <span class="hljs-string">&#x27;monospace&#x27;</span>,<br>                <span class="hljs-string">&#x27;weight&#x27;</span> : <span class="hljs-string">&#x27;bold&#x27;</span>,<br>                <span class="hljs-string">&#x27;size&#x27;</span>   : <span class="hljs-string">&#x27;small&#x27;</span>&#125;<br>plt.rc(<span class="hljs-string">&#x27;font&#x27;</span>, **font_options)<br></code></pre></td></tr></table></figure><p>要了解全部的自定义选项，请查阅matplotlib的配置文件matplotlibrc（位于matplotlib&#x2F;mpl-data目录中）。如果对该文件进行了自定义，并将其放在你自己的.matplotlibrc目录中，则每次使用matplotlib时就会加载该文件。</p><p>下一节，我们会看到，seaborn包有若干内置的绘图主题或类型，它们使用了matplotlib的内部配置。</p><h3 id="9-2-使用pandas和seaborn绘图"><a href="#9-2-使用pandas和seaborn绘图" class="headerlink" title="9.2 使用pandas和seaborn绘图"></a>9.2 使用pandas和seaborn绘图</h3><p>matplotlib实际上是一种比较低级的工具。要绘制一张图表，你组装一些基本组件就行：数据展示（即图表类型：线型图、柱状图、盒形图、散布图、等值线图等）、图例、标题、刻度标签以及其他注解型信息。</p><p>在pandas中，我们有多列数据，还有行和列标签。pandas自身就有内置的方法，用于简化从DataFrame和Series绘制图形。另一个库seaborn（<a href="https://seaborn.pydata.org/%EF%BC%89%EF%BC%8C%E7%94%B1Michael">https://seaborn.pydata.org/），由Michael</a> Waskom创建的静态图形库。Seaborn简化了许多常见可视类型的创建。</p><blockquote><p>提示：引入seaborn会修改matplotlib默认的颜色方案和绘图类型，以提高可读性和美观度。即使你不使用seaborn API，你可能也会引入seaborn，作为提高美观度和绘制常见matplotlib图形的简化方法。</p></blockquote><p><strong>线型图</strong></p><p>Series和DataFrame都有一个用于生成各类图表的plot方法。默认情况下，它们所生成的是线型图（如图9-13所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">60</span>]: s = pd.Series(np.random.randn(<span class="hljs-number">10</span>).cumsum(), index=np.arange(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>))<br><br>In [<span class="hljs-number">61</span>]: s.plot()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231031311.png" alt="image-20220723103150223"></p><p>该Series对象的索引会被传给matplotlib，并用以绘制X轴。可以通过use_index&#x3D;False禁用该功能。X轴的刻度和界限可以通过xticks和xlim选项进行调节，Y轴就用yticks和ylim。plot参数的完整列表请参见表9-3。我只会讲解其中几个，剩下的就留给读者自己去研究了。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231038457.png" alt="image-20220723103804329"></p><p>pandas的大部分绘图方法都有一个可选的ax参数，它可以是一个matplotlib的subplot对象。这使你能够在网格布局中更为灵活地处理subplot的位置。</p><p>DataFrame的plot方法会在一个subplot中为各列绘制一条线，并自动创建图例（如图9-14所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">62</span>]: df = pd.DataFrame(np.random.randn(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>).cumsum(<span class="hljs-number">0</span>),<br>   ....:                   columns=[<span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;D&#x27;</span>],<br>   ....:                   index=np.arange(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>))<br><br>In [<span class="hljs-number">63</span>]: df.plot()<br></code></pre></td></tr></table></figure><p>plot属性包含一批不同绘图类型的方法。例如，df.plot()等价于df.plot.line()。后面会学习这些方法。</p><blockquote><p>笔记：plot的其他关键字参数会被传给相应的matplotlib绘图函数，所以要更深入地自定义图表，就必须学习更多有关matplotlib API的知识。</p></blockquote><p>DataFrame还有一些用于对列进行灵活处理的选项，例如，是要将所有列都绘制到一个subplot中还是创建各自的subplot。详细信息请参见表9-4。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231040165.png" alt="image-20220723104032051"></p><blockquote><p>注意： 有关时间序列的绘图，请见第11章。</p></blockquote><p><strong>柱状图</strong></p><p>plot.bar()和plot.barh()分别绘制水平和垂直的柱状图。这时，Series和DataFrame的索引将会被用作X（bar）或Y（barh）刻度（如图9-15所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">64</span>]: fig, axes = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br><br>In [<span class="hljs-number">65</span>]: data = pd.Series(np.random.rand(<span class="hljs-number">16</span>), index=<span class="hljs-built_in">list</span>(<span class="hljs-string">&#x27;abcdefghijklmnop&#x27;</span>))<br><br>In [<span class="hljs-number">66</span>]: data.plot.bar(ax=axes[<span class="hljs-number">0</span>], color=<span class="hljs-string">&#x27;k&#x27;</span>, alpha=<span class="hljs-number">0.7</span>)<br>Out[<span class="hljs-number">66</span>]: &lt;matplotlib.axes._subplots.AxesSubplot at <span class="hljs-number">0x7fb62493d470</span>&gt;<br><br>In [<span class="hljs-number">67</span>]: data.plot.barh(ax=axes[<span class="hljs-number">1</span>], color=<span class="hljs-string">&#x27;k&#x27;</span>, alpha=<span class="hljs-number">0.7</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231042592.png" alt="image-20220723104214531"></p><p>color&#x3D;’k’和alpha&#x3D;0.7设定了图形的颜色为黑色，并使用部分的填充透明度。对于DataFrame，柱状图会将每一行的值分为一组，并排显示，如图9-16所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">69</span>]: df = pd.DataFrame(np.random.rand(<span class="hljs-number">6</span>, <span class="hljs-number">4</span>),<br>   ....:                   index=[<span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>, <span class="hljs-string">&#x27;three&#x27;</span>, <span class="hljs-string">&#x27;four&#x27;</span>, <span class="hljs-string">&#x27;five&#x27;</span>, <span class="hljs-string">&#x27;six&#x27;</span>],<br>   ....:                   columns=pd.Index([<span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;D&#x27;</span>], name=<span class="hljs-string">&#x27;Genus&#x27;</span>))<br><br>In [<span class="hljs-number">70</span>]: df<br>Out[<span class="hljs-number">70</span>]: <br>Genus         A         B         C         D<br>one    <span class="hljs-number">0.370670</span>  <span class="hljs-number">0.602792</span>  <span class="hljs-number">0.229159</span>  <span class="hljs-number">0.486744</span><br>two    <span class="hljs-number">0.420082</span>  <span class="hljs-number">0.571653</span>  <span class="hljs-number">0.049024</span>  <span class="hljs-number">0.880592</span><br>three  <span class="hljs-number">0.814568</span>  <span class="hljs-number">0.277160</span>  <span class="hljs-number">0.880316</span>  <span class="hljs-number">0.431326</span><br>four   <span class="hljs-number">0.374020</span>  <span class="hljs-number">0.899420</span>  <span class="hljs-number">0.460304</span>  <span class="hljs-number">0.100843</span><br>five   <span class="hljs-number">0.433270</span>  <span class="hljs-number">0.125107</span>  <span class="hljs-number">0.494675</span>  <span class="hljs-number">0.961825</span><br>six    <span class="hljs-number">0.601648</span>  <span class="hljs-number">0.478576</span>  <span class="hljs-number">0.205690</span>  <span class="hljs-number">0.560547</span><br><br>In [<span class="hljs-number">71</span>]: df.plot.bar()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231042295.png" alt="image-20220723104238218"></p><p>注意，DataFrame各列的名称”Genus”被用作了图例的标题。</p><p>设置stacked&#x3D;True即可为DataFrame生成堆积柱状图，这样每行的值就会被堆积在一起（如图9-17所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">73</span>]: df.plot.barh(stacked=<span class="hljs-literal">True</span>, alpha=<span class="hljs-number">0.5</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231043935.png" alt="image-20220723104355868"></p><blockquote><p>笔记：柱状图有一个非常不错的用法：利用value_counts图形化显示Series中各值的出现频率，比如s.value_counts().plot.bar()。</p></blockquote><p>再以本书前面用过的那个有关小费的数据集为例，假设我们想要做一张堆积柱状图以展示每天各种聚会规模的数据点的百分比。我用read_csv将数据加载进来，然后根据日期和聚会规模创建一张交叉表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">75</span>]: tips = pd.read_csv(<span class="hljs-string">&#x27;examples/tips.csv&#x27;</span>)<br><br>In [<span class="hljs-number">76</span>]: party_counts = pd.crosstab(tips[<span class="hljs-string">&#x27;day&#x27;</span>], tips[<span class="hljs-string">&#x27;size&#x27;</span>])<br><br>In [<span class="hljs-number">77</span>]: party_counts<br>Out[<span class="hljs-number">77</span>]: <br>size  <span class="hljs-number">1</span>   <span class="hljs-number">2</span>   <span class="hljs-number">3</span>   <span class="hljs-number">4</span>  <span class="hljs-number">5</span>  <span class="hljs-number">6</span><br>day                      <br>Fri   <span class="hljs-number">1</span>  <span class="hljs-number">16</span>   <span class="hljs-number">1</span>   <span class="hljs-number">1</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br>Sat   <span class="hljs-number">2</span>  <span class="hljs-number">53</span>  <span class="hljs-number">18</span>  <span class="hljs-number">13</span>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span><br>Sun   <span class="hljs-number">0</span>  <span class="hljs-number">39</span>  <span class="hljs-number">15</span>  <span class="hljs-number">18</span>  <span class="hljs-number">3</span>  <span class="hljs-number">1</span><br>Thur  <span class="hljs-number">1</span>  <span class="hljs-number">48</span>   <span class="hljs-number">4</span>   <span class="hljs-number">5</span>  <span class="hljs-number">1</span>  <span class="hljs-number">3</span><br><br><span class="hljs-comment"># Not many 1- and 6-person parties</span><br>In [<span class="hljs-number">78</span>]: party_counts = party_counts.loc[:, <span class="hljs-number">2</span>:<span class="hljs-number">5</span>]<br></code></pre></td></tr></table></figure><p>然后进行规格化，使得各行的和为1，并生成图表（如图9-18所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Normalize to sum to 1</span><br>In [<span class="hljs-number">79</span>]: party_pcts = party_counts.div(party_counts.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>), axis=<span class="hljs-number">0</span>)<br><br>In [<span class="hljs-number">80</span>]: party_pcts<br>Out[<span class="hljs-number">80</span>]: <br>size         <span class="hljs-number">2</span>         <span class="hljs-number">3</span>         <span class="hljs-number">4</span>         <span class="hljs-number">5</span><br>day                                         <br>Fri   <span class="hljs-number">0.888889</span>  <span class="hljs-number">0.055556</span>  <span class="hljs-number">0.055556</span>  <span class="hljs-number">0.000000</span><br>Sat   <span class="hljs-number">0.623529</span>  <span class="hljs-number">0.211765</span>  <span class="hljs-number">0.152941</span>  <span class="hljs-number">0.011765</span><br>Sun   <span class="hljs-number">0.520000</span>  <span class="hljs-number">0.200000</span>  <span class="hljs-number">0.240000</span>  <span class="hljs-number">0.040000</span><br>Thur  <span class="hljs-number">0.827586</span>  <span class="hljs-number">0.068966</span>  <span class="hljs-number">0.086207</span>  <span class="hljs-number">0.017241</span><br><br>In [<span class="hljs-number">81</span>]: party_pcts.plot.bar()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231049952.png" alt="image-20220723104954886"></p><p>于是，通过该数据集就可以看出，聚会规模在周末会变大。</p><p>对于在绘制一个图形之前，需要进行合计的数据，使用seaborn可以减少工作量。用seaborn来看每天的小费比例（图9-19是结果）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">83</span>]: <span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br>In [<span class="hljs-number">84</span>]: tips[<span class="hljs-string">&#x27;tip_pct&#x27;</span>] = tips[<span class="hljs-string">&#x27;tip&#x27;</span>] / (tips[<span class="hljs-string">&#x27;total_bill&#x27;</span>] - tips[<span class="hljs-string">&#x27;tip&#x27;</span>])<br><br>In [<span class="hljs-number">85</span>]: tips.head()<br>Out[<span class="hljs-number">85</span>]: <br>   total_bill   tip smoker  day    time  size   tip_pct<br><span class="hljs-number">0</span>       <span class="hljs-number">16.99</span>  <span class="hljs-number">1.01</span>     No  Sun  Dinner     <span class="hljs-number">2</span>  <span class="hljs-number">0.063204</span><br><span class="hljs-number">1</span>       <span class="hljs-number">10.34</span>  <span class="hljs-number">1.66</span>     No  Sun  Dinner     <span class="hljs-number">3</span>  <span class="hljs-number">0.191244</span><br><span class="hljs-number">2</span>       <span class="hljs-number">21.01</span>  <span class="hljs-number">3.50</span>     No  Sun  Dinner     <span class="hljs-number">3</span>  <span class="hljs-number">0.199886</span><br><span class="hljs-number">3</span>       <span class="hljs-number">23.68</span>  <span class="hljs-number">3.31</span>     No  Sun  Dinner     <span class="hljs-number">2</span>  <span class="hljs-number">0.162494</span><br><span class="hljs-number">4</span>       <span class="hljs-number">24.59</span>  <span class="hljs-number">3.61</span>     No  Sun  Dinner     <span class="hljs-number">4</span>  <span class="hljs-number">0.172069</span><br><br>In [<span class="hljs-number">86</span>]: sns.barplot(x=<span class="hljs-string">&#x27;tip_pct&#x27;</span>, y=<span class="hljs-string">&#x27;day&#x27;</span>, data=tips, orient=<span class="hljs-string">&#x27;h&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231051930.png" alt="image-20220723105133868"></p><p>seaborn的绘制函数使用data参数，它可能是pandas的DataFrame。其它的参数是关于列的名字。因为一天的每个值有多次观察，柱状图的值是tip_pct的平均值。绘制在柱状图上的黑线代表95%置信区间（可以通过可选参数配置）。</p><p>seaborn.barplot有颜色选项，使我们能够通过一个额外的值设置（见图9-20）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">88</span>]: sns.barplot(x=<span class="hljs-string">&#x27;tip_pct&#x27;</span>, y=<span class="hljs-string">&#x27;day&#x27;</span>, hue=<span class="hljs-string">&#x27;time&#x27;</span>, data=tips, orient=<span class="hljs-string">&#x27;h&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231054307.png" alt="image-20220723105400242"></p><p>注意，seaborn已经自动修改了图形的美观度：默认调色板，图形背景和网格线的颜色。你可以用seaborn.set在不同的图形外观之间切换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">90</span>]: sns.<span class="hljs-built_in">set</span>(style=<span class="hljs-string">&quot;whitegrid&quot;</span>)<br></code></pre></td></tr></table></figure><p><strong>直方图和密度图</strong></p><p>直方图（histogram）是一种可以对值频率进行离散化显示的柱状图。数据点被拆分到离散的、间隔均匀的面元中，绘制的是各面元中数据点的数量。再以前面那个小费数据为例，通过在Series使用plot.hist方法，我们可以生成一张“小费占消费总额百分比”的直方图（如图9-21所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">92</span>]: tips[<span class="hljs-string">&#x27;tip_pct&#x27;</span>].plot.hist(bins=<span class="hljs-number">50</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241238989.png" alt="image-20220724123753852"></p><p>与此相关的一种图表类型是密度图，它是通过计算“可能会产生观测数据的连续概率分布的估计”而产生的。一般的过程是将该分布近似为一组核（即诸如正态分布之类的较为简单的分布）。因此，密度图也被称作KDE（Kernel Density Estimate，核密度估计）图。使用plot.kde和标准混合正态分布估计即可生成一张密度图（见图9-22）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">94</span>]: tips[<span class="hljs-string">&#x27;tip_pct&#x27;</span>].plot.density()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241240396.png" alt="image-20220724124030315"></p><p>seaborn的distplot方法绘制直方图和密度图更加简单，还可以同时画出直方图和连续密度估计图。作为例子，考虑一个双峰分布，由两个不同的标准正态分布组成（见图9-23）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">96</span>]: comp1 = np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=<span class="hljs-number">200</span>)<br><br>In [<span class="hljs-number">97</span>]: comp2 = np.random.normal(<span class="hljs-number">10</span>, <span class="hljs-number">2</span>, size=<span class="hljs-number">200</span>)<br><br>In [<span class="hljs-number">98</span>]: values = pd.Series(np.concatenate([comp1, comp2]))<br><br>In [<span class="hljs-number">99</span>]: sns.distplot(values, bins=<span class="hljs-number">100</span>, color=<span class="hljs-string">&#x27;k&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241241980.png" alt="image-20220724124111907"></p><p><strong>散布图或点图</strong></p><p>点图或散布图是观察两个一维数据序列之间的关系的有效手段。在下面这个例子中，我加载了来自statsmodels项目的macrodata数据集，选择了几个变量，然后计算对数差：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">100</span>]: macro = pd.read_csv(<span class="hljs-string">&#x27;examples/macrodata.csv&#x27;</span>)<br><br>In [<span class="hljs-number">101</span>]: data = macro[[<span class="hljs-string">&#x27;cpi&#x27;</span>, <span class="hljs-string">&#x27;m1&#x27;</span>, <span class="hljs-string">&#x27;tbilrate&#x27;</span>, <span class="hljs-string">&#x27;unemp&#x27;</span>]]<br><br>In [<span class="hljs-number">102</span>]: trans_data = np.log(data).diff().dropna()<br><br>In [<span class="hljs-number">103</span>]: trans_data[-<span class="hljs-number">5</span>:]<br>Out[<span class="hljs-number">103</span>]: <br>          cpi        m1  tbilrate     unemp<br><span class="hljs-number">198</span> -<span class="hljs-number">0.007904</span>  <span class="hljs-number">0.045361</span> -<span class="hljs-number">0.396881</span>  <span class="hljs-number">0.105361</span><br><span class="hljs-number">199</span> -<span class="hljs-number">0.021979</span>  <span class="hljs-number">0.066753</span> -<span class="hljs-number">2.277267</span>  <span class="hljs-number">0.139762</span><br><span class="hljs-number">200</span>  <span class="hljs-number">0.002340</span>  <span class="hljs-number">0.010286</span>  <span class="hljs-number">0.606136</span>  <span class="hljs-number">0.160343</span><br><span class="hljs-number">201</span>  <span class="hljs-number">0.008419</span>  <span class="hljs-number">0.037461</span> -<span class="hljs-number">0.200671</span>  <span class="hljs-number">0.127339</span><br><span class="hljs-number">202</span>  <span class="hljs-number">0.008894</span>  <span class="hljs-number">0.012202</span> -<span class="hljs-number">0.405465</span>  <span class="hljs-number">0.042560</span><br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241241474.png" alt="image-20220724124144388"></p><p>在探索式数据分析工作中，同时观察一组变量的散布图是很有意义的，这也被称为散布图矩阵（scatter plot matrix）。纯手工创建这样的图表很费工夫，所以seaborn提供了一个便捷的pairplot函数，它支持在对角线上放置每个变量的直方图或密度估计（见图9-25）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">107</span>]: sns.pairplot(trans_data, diag_kind=<span class="hljs-string">&#x27;kde&#x27;</span>, plot_kws=&#123;<span class="hljs-string">&#x27;alpha&#x27;</span>: <span class="hljs-number">0.2</span>&#125;)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241242670.png" alt="image-20220724124210541"></p><p>你可能注意到了plot_kws参数。它可以让我们传递配置选项到非对角线元素上的图形使用。对于更详细的配置选项，可以查阅seaborn.pairplot文档字符串。</p><p><strong>分面网格（facet grid）和类型数据</strong></p><p>要是数据集有额外的分组维度呢？有多个分类变量的数据可视化的一种方法是使用小面网格。seaborn有一个有用的内置函数factorplot，可以简化制作多种分面图（见图9-26）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">108</span>]: sns.factorplot(x=<span class="hljs-string">&#x27;day&#x27;</span>, y=<span class="hljs-string">&#x27;tip_pct&#x27;</span>, hue=<span class="hljs-string">&#x27;time&#x27;</span>, col=<span class="hljs-string">&#x27;smoker&#x27;</span>,<br>  .....:                kind=<span class="hljs-string">&#x27;bar&#x27;</span>, data=tips[tips.tip_pct &lt; <span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241242631.png" alt="image-20220724124253548"></p><p>除了在分面中用不同的颜色按时间分组，我们还可以通过给每个时间值添加一行来扩展分面网格：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">109</span>]: sns.factorplot(x=<span class="hljs-string">&#x27;day&#x27;</span>, y=<span class="hljs-string">&#x27;tip_pct&#x27;</span>, row=<span class="hljs-string">&#x27;time&#x27;</span>,<br>   .....:                col=<span class="hljs-string">&#x27;smoker&#x27;</span>,<br>   .....:                kind=<span class="hljs-string">&#x27;bar&#x27;</span>, data=tips[tips.tip_pct &lt; <span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241243830.png" alt="image-20220724124318731"></p><p>factorplot支持其它的绘图类型，你可能会用到。例如，盒图（它可以显示中位数，四分位数，和异常值）就是一个有用的可视化类型（见图9-28）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">110</span>]: sns.factorplot(x=<span class="hljs-string">&#x27;tip_pct&#x27;</span>, y=<span class="hljs-string">&#x27;day&#x27;</span>, kind=<span class="hljs-string">&#x27;box&#x27;</span>,<br>   .....:                data=tips[tips.tip_pct &lt; <span class="hljs-number">0.5</span>])<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241243112.png" alt="image-20220724124359020"></p><p>使用更通用的seaborn.FacetGrid类，你可以创建自己的分面网格。请查阅seaborn的文档（<a href="https://seaborn.pydata.org/%EF%BC%89%E3%80%82">https://seaborn.pydata.org/）。</a></p><h3 id="9-3-其它的Python可视化工具"><a href="#9-3-其它的Python可视化工具" class="headerlink" title="9.3 其它的Python可视化工具"></a>9.3 其它的Python可视化工具</h3><p>与其它开源库类似，Python创建图形的方式非常多（根本罗列不完）。自从2010年，许多开发工作都集中在创建交互式图形以便在Web上发布。利用工具如Boken（<a href="https://bokeh.pydata.org/en/latest/%EF%BC%89%E5%92%8CPlotly%EF%BC%88https://github.com/plotly/plotly.py%EF%BC%89%EF%BC%8C%E7%8E%B0%E5%9C%A8%E5%8F%AF%E4%BB%A5%E5%88%9B%E5%BB%BA%E5%8A%A8%E6%80%81%E4%BA%A4%E4%BA%92%E5%9B%BE%E5%BD%A2%EF%BC%8C%E7%94%A8%E4%BA%8E%E7%BD%91%E9%A1%B5%E6%B5%8F%E8%A7%88%E5%99%A8%E3%80%82">https://bokeh.pydata.org/en/latest/）和Plotly（https://github.com/plotly/plotly.py），现在可以创建动态交互图形，用于网页浏览器。</a></p><p>对于创建用于打印或网页的静态图形，我建议默认使用matplotlib和附加的库，比如pandas和seaborn。对于其它数据可视化要求，学习其它的可用工具可能是有用的。我鼓励你探索绘图的生态系统，因为它将持续发展。</p><h3 id="9-4-总结"><a href="#9-4-总结" class="headerlink" title="9.4 总结"></a>9.4 总结</h3><p>本章的目的是熟悉一些基本的数据可视化操作，使用pandas，matplotlib，和seaborn。如果视觉显示数据分析的结果对你的工作很重要，我鼓励你寻求更多的资源来了解更高效的数据可视化。这是一个活跃的研究领域，你可以通过在线和纸质的形式学习许多优秀的资源。</p><p>下一章，我们将重点放在pandas的数据聚合和分组操作上。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/YOLO%E7%B3%BB%E5%88%97%E5%AD%A6%E4%B9%A0/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/YOLO%E7%B3%BB%E5%88%97%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h3 id="YOLO系列学习"><a href="#YOLO系列学习" class="headerlink" title="YOLO系列学习"></a>YOLO系列学习</h3><p>霹雳大佬学习三部曲：<br>1.网络先查找资料，先看下网上介绍。<br>2.读原论文，读原论文，读原论文。（不要认为你听别人的讲解你自己感觉懂了就是懂了）<br>3.读代码，标星多的，先看readme，先看网络搭建，数据预处理，损失函数的计算，数据预处理，损失函数很重要。（不要把别人代码运行起来就觉得自己懂了）</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PyTorch%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PyTorch%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="PyTorch官方教程"><a href="#PyTorch官方教程" class="headerlink" title="PyTorch官方教程"></a>PyTorch官方教程</h3><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><strong>Introduction</strong></h4><p> <strong>What is PyTorch?</strong></p><p>PyTorch is a Python-based scientific computing package serving two broad purposes:</p><ul><li>A replacement for NumPy to use the power of GPUs and other accelerators.</li><li>An automatic differentiation library that is useful to implement neural networks.</li></ul><p><strong>Goal of this tutorial:</strong></p><ul><li>Understand PyTorch’s Tensor library and neural networks at a high level.</li><li>Train a small neural network to classify images</li></ul><h4 id="TENSORS"><a href="#TENSORS" class="headerlink" title="TENSORS"></a>TENSORS</h4><p>Tensors 是一种特殊的数据结构，与数组和矩阵非常相似。 在PyTorch中，我们使用Tensors 来编码模型的输入和输出，以及模型的参数。  </p><p>Tensors 与NumPy的ndarrays类似，除了Tensors 可以在gpu或其他专用硬件上运行以加速计算。 如果你熟悉ndarrays，那么你对Tensors API就很熟悉了。 如果没有，请遵循这个快速的API演练。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br></code></pre></td></tr></table></figure><p> <strong>Tensor Initialization</strong></p><p>Tensor 可以用各种方式初始化。 看看下面的例子:  </p><p><code>Directly from data</code></p><p>Tensor 可以直接从数据中创建。 数据类型被自动推断出来。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br>x_data = torch.tensor(data)<br></code></pre></td></tr></table></figure><p><code>From a NumPy array</code></p><p>Tensor 可以从NumPy数组中创建(反之亦然——参见Bridge with NumPy)。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">np_array = np.array(data)<br>x_np = torch.from_numpy(np_array)<br></code></pre></td></tr></table></figure><p><code>From another tensor:</code></p><p>新Tensor 保留了参数Tensor 的属性(形状、数据类型)，除非显式地重写。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x_ones = torch.ones_like(x_data) <span class="hljs-comment"># retains the properties of x_data</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Ones Tensor: \n <span class="hljs-subst">&#123;x_ones&#125;</span> \n&quot;</span>)<br><br>x_rand = torch.rand_like(x_data, dtype=torch.<span class="hljs-built_in">float</span>) <span class="hljs-comment"># overrides the datatype of x_data</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Random Tensor: \n <span class="hljs-subst">&#123;x_rand&#125;</span> \n&quot;</span>)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs lua">Ones Tensor:<br> tensor(<span class="hljs-string">[[1, 1],</span><br><span class="hljs-string">        [1, 1]]</span>)<br><br>Random Tensor:<br> tensor(<span class="hljs-string">[[0.4621, 0.1440],</span><br><span class="hljs-string">        [0.6105, 0.6398]]</span>)<br></code></pre></td></tr></table></figure><p><code>With random or constant values:</code></p><p>形状是<strong>tensor dimensions</strong>的元组。 在下面的函数中，它决定了输出tensor的维数。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">shape = (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>,)<br>rand_tensor = torch.rand(shape)<br>ones_tensor = torch.ones(shape)<br>zeros_tensor = torch.zeros(shape)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Random Tensor: \n <span class="hljs-subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Ones Tensor: \n <span class="hljs-subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Zeros Tensor: \n <span class="hljs-subst">&#123;zeros_tensor&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs lua">Random Tensor:<br> tensor(<span class="hljs-string">[[0.9037, 0.2988, 0.8528],</span><br><span class="hljs-string">        [0.9466, 0.9646, 0.3117]]</span>)<br><br>Ones Tensor:<br> tensor(<span class="hljs-string">[[1., 1., 1.],</span><br><span class="hljs-string">        [1., 1., 1.]]</span>)<br><br>Zeros Tensor:<br> tensor(<span class="hljs-string">[[0., 0., 0.],</span><br><span class="hljs-string">        [0., 0., 0.]]</span>)<br></code></pre></td></tr></table></figure><p><strong>Tensor Attributes</strong></p><p>Tensor 属性描述了它们的形状、数据类型和存储它们的设备。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = torch.rand(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Shape of tensor: <span class="hljs-subst">&#123;tensor.shape&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Datatype of tensor: <span class="hljs-subst">&#123;tensor.dtype&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Device tensor is stored on: <span class="hljs-subst">&#123;tensor.device&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Shape</span> of tensor: torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br><span class="hljs-attribute">Datatype</span> of tensor: torch.float32<br><span class="hljs-attribute">Device</span> tensor is stored <span class="hljs-literal">on</span>: cpu<br></code></pre></td></tr></table></figure><p><strong>Tensor Operations</strong></p><p>超过100个Tensor 操作，包括转置，索引，切片，数学操作，线性代数，随机抽样，以及更多的综合描述在这里。  </p><p>它们都可以在GPU上运行(通常比在CPU上运行速度更快)。 如果你使用Colab，通过编辑&gt;笔记本设置分配一个GPU</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># We move our tensor to the GPU if available</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>  tensor = tensor.to(<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Device tensor is stored on: <span class="hljs-subst">&#123;tensor.device&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs delphi">Device tensor <span class="hljs-keyword">is</span> <span class="hljs-keyword">stored</span> <span class="hljs-keyword">on</span>: cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>尝试列表中的一些操作。 如果您熟悉NumPy API，您会发现使用Tensor API很容易。  </p><p><code>Standard numpy-like indexing and slicing:</code></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">tensor = torch<span class="hljs-selector-class">.ones</span>(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br>tensor<span class="hljs-selector-attr">[:,1]</span> = <span class="hljs-number">0</span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(tensor)</span></span><br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs inform7">tensor(<span class="hljs-comment">[<span class="hljs-comment">[1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1.]</span>]</span>)<br></code></pre></td></tr></table></figure><p>加入tensors你可以用torch。 将一系列tensors沿给定维数连接起来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(t1)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs inform7">tensor(<span class="hljs-comment">[<span class="hljs-comment">[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]</span>]</span>)<br></code></pre></td></tr></table></figure><p><code>Multiplying tensors</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># This computes the element-wise product</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;tensor.mul(tensor) \n <span class="hljs-subst">&#123;tensor.mul(tensor)&#125;</span> \n&quot;</span>)<br><span class="hljs-comment"># Alternative syntax:</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;tensor * tensor \n <span class="hljs-subst">&#123;tensor * tensor&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs inform7">tensor.mul(tensor)<br> tensor(<span class="hljs-comment">[<span class="hljs-comment">[1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1.]</span>]</span>)<br><br>tensor * tensor<br> tensor(<span class="hljs-comment">[<span class="hljs-comment">[1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1.]</span>]</span>)<br></code></pre></td></tr></table></figure><p>它计算两个tensors之间的矩阵乘法  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;tensor.matmul(tensor.T) \n <span class="hljs-subst">&#123;tensor.matmul(tensor.T)&#125;</span> \n&quot;</span>)<br><span class="hljs-comment"># Alternative syntax:</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;tensor @ tensor.T \n <span class="hljs-subst">&#123;tensor @ tensor.T&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>​Out:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">tensor<span class="hljs-selector-class">.matmul</span>(tensor.T)<br> <span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[[3., 3., 3., 3.]</span>,<br>        <span class="hljs-selector-attr">[3., 3., 3., 3.]</span>,<br>        <span class="hljs-selector-attr">[3., 3., 3., 3.]</span>,<br>        <span class="hljs-selector-attr">[3., 3., 3., 3.]</span>])<br><br>tensor @ tensor<span class="hljs-selector-class">.T</span><br> <span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[[3., 3., 3., 3.]</span>,<br>        <span class="hljs-selector-attr">[3., 3., 3., 3.]</span>,<br>        <span class="hljs-selector-attr">[3., 3., 3., 3.]</span>,<br>        <span class="hljs-selector-attr">[3., 3., 3., 3.]</span>])<br></code></pre></td></tr></table></figure><p>具有后缀的操作为就地操作。 例如:x.copy_(y)， x.t_()，将改变x。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(tensor, <span class="hljs-string">&quot;\n&quot;</span>)<br>tensor.add_(<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(tensor)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs inform7">tensor(<span class="hljs-comment">[<span class="hljs-comment">[1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 0., 1., 1.]</span>]</span>)<br><br>tensor(<span class="hljs-comment">[<span class="hljs-comment">[6., 5., 6., 6.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[6., 5., 6., 6.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[6., 5., 6., 6.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[6., 5., 6., 6.]</span>]</span>)<br></code></pre></td></tr></table></figure><p>&#x3D;&#x3D;NOTE:&#x3D;&#x3D;</p><p>就地操作可以节省一些内存，但在计算导数时可能会出现问题，因为会立即丢失历史记录。 因此，不鼓励使用它们</p><p><strong>Bridge with NumPy</strong></p><p>CPU上的Tensors 和NumPy数组可以共享它们的底层内存位置，改变其中一个就会改变另一个。  </p><p><strong>Tensor to NumPy array</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">t = torch.ones(<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br>n = t.numpy()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">t</span>: tensor([<span class="hljs-number">1</span>., <span class="hljs-number">1</span>., <span class="hljs-number">1</span>., <span class="hljs-number">1</span>., <span class="hljs-number">1</span>.])<br><span class="hljs-attribute">n</span>:<span class="hljs-meta"> [1. 1. 1. 1. 1.]</span><br></code></pre></td></tr></table></figure><p>tensor 的变化反映在NumPy数组中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">t.add_(<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">t</span>: tensor([<span class="hljs-number">2</span>., <span class="hljs-number">2</span>., <span class="hljs-number">2</span>., <span class="hljs-number">2</span>., <span class="hljs-number">2</span>.])<br><span class="hljs-attribute">n</span>:<span class="hljs-meta"> [2. 2. 2. 2. 2.]</span><br></code></pre></td></tr></table></figure><p><strong>NumPy array to Tensor</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">n = np.ones(<span class="hljs-number">5</span>)<br>t = torch.from_numpy(n)<br></code></pre></td></tr></table></figure><p>NumPy数组的变化反映在tensor中。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">np.add(n, <span class="hljs-number">1</span>, out=n)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">t</span>: tensor([<span class="hljs-number">2</span>., <span class="hljs-number">2</span>., <span class="hljs-number">2</span>., <span class="hljs-number">2</span>., <span class="hljs-number">2</span>.], dtype=torch.float64)<br><span class="hljs-attribute">n</span>:<span class="hljs-meta"> [2. 2. 2. 2. 2.]</span><br></code></pre></td></tr></table></figure><h4 id="A-GENTLE-INTRODUCTION-TO-TORCH-AUTOGRAD"><a href="#A-GENTLE-INTRODUCTION-TO-TORCH-AUTOGRAD" class="headerlink" title="A GENTLE INTRODUCTION TO TORCH.AUTOGRAD"></a>A GENTLE INTRODUCTION TO <code>TORCH.AUTOGRAD</code></h4><p>torch.autograd是PyTorch的automatic differentiation engine(自动微分引擎) ，为神经网络训练提供动力。 在本节中，您将从概念上理解autograd如何帮助神经网络训练。  </p><p><strong>Background</strong></p><p>神经网络(nns)是一组嵌套函数的集合，在某些输入数据上执行。 这些函数是由参数(由权重和偏差组成)定义的，在PyTorch中，这些参数存储在tensors中。  </p><p>Training a NN happens in two steps:</p><p><strong>Forward Propagation</strong>: In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.(在前向支撑中，神经网络对正确的输出进行最佳猜测。 它在每个函数中运行输入数据来进行猜测。)</p><p><strong>Backward Propagation</strong>: In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (<em>gradients</em>), and optimizing the parameters using gradient descent. For a more detailed walkthrough of backprop, check out this <a href="https://www.youtube.com/watch?v=tIeHLnjs5U8">video from 3Blue1Brown</a>.(在背撑模型中，神经网络根据其猜测的误差比例调整参数。 它通过从输出往回遍历，收集关于函数参数(梯度)的误差的导数，并使用梯度下降优化参数来做到这一点。)</p><p><strong>Usage in PyTorch</strong></p><p>让我们看一下单个训练步骤。 在这个例子中，我们从torchvision中加载了一个预先训练好的resnet18模型。 我们创建一个随机数据张量来表示一个具有3个channels，高度和宽度为64的图像，其对应的标签初始化为一些随机值。 在预先训练的模型中，标签的形状为(1,1000)。  </p><p>NOTE：本教程只在CPU上工作，不会在GPU上工作(即使张量移动到CUDA)。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch, torchvision<br>model = torchvision.models.resnet18(pretrained=<span class="hljs-literal">True</span>)<br>data = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>labels = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>)<br></code></pre></td></tr></table></figure><p>接下来，我们将输入数据在模型的每一层中运行，以做出预测。 这是forward pass。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">prediction = model(data) <span class="hljs-comment"># forward pass</span><br></code></pre></td></tr></table></figure><p>我们使用模型的预测和相应的标签来计算误差(loss)。 下一步是通过网络反向传播此错误。 当我们对error tensor调用<code>. Backward()</code>时，向后传播就开始了。  然后，Autograd在参数的<code>.grad</code>属性中计算并存储每个模型参数的梯度。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">loss = (prediction - labels).<span class="hljs-built_in">sum</span>()<br>loss.backward() <span class="hljs-comment"># backward pass</span><br></code></pre></td></tr></table></figure><p>接下来，我们加载一个优化器，在本例中，SGD的学习率(learning rate )为0.01，动力(<a href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d">momentum</a> )为0.9。 我们在优化器中注册模型的所有参数。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optim = torch.optim.SGD(model.parameters(), lr=<span class="hljs-number">1e-2</span>, momentum=<span class="hljs-number">0.9</span>)<br></code></pre></td></tr></table></figure><p>最后，我们调用<code>.step()</code>来启动梯度下降。 优化器根据存储在<code>.grad</code>中的梯度来调整每个参数。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optim.step() <span class="hljs-comment">#gradient descent</span><br></code></pre></td></tr></table></figure><p>现在，您已经具备了训练神经网络所需的一切条件。 下面几节详细介绍了<code>autograd</code>的工作方式——可以跳过它们。  </p><p><strong>Differentiation in Autograd</strong></p><p>让我们看看<code>autograd</code>如何收集梯度。 我们创建了两个tensors a和b，它们的<code>requires_grad=True</code>。 这向autograd发出信号，表示应该跟踪它们上的每个操作。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>a = torch.tensor([<span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>], requires_grad=<span class="hljs-literal">True</span>)<br>b = torch.tensor([<span class="hljs-number">6.</span>, <span class="hljs-number">4.</span>], requires_grad=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202203101650916.png" alt="image-20220310165013855"></p><p>让我们假设“a”和“b”是一个NN的参数，“Q”是误差。 在NN训练中，我们需要误差w.r.t.参数的梯度，即。  </p><p>$\dfrac{\partial Q}{\partial a} &#x3D; 9a^2$</p><p>$\dfrac{\partial Q}{\partial b} &#x3D; -2b$</p><p>当我们在Q上调用<code>.backward()</code>时，autograd计算这些梯度并将它们存储在各自tensors的<code>.grad</code>属性中。  </p><p>我们需要在<code>Q.backward()</code>中显式传递一个梯度参数，因为它是一个向量。 梯度是一个与Q形状相同的tensor ，它表示Q w.r.t本身的梯度，即。  </p><p>$\dfrac{dQ}{dQ} &#x3D; 1$</p><p>同样，我们也可以将Q聚合为标量并隐式地向后调用，如<code>Q.sum().backward()</code>。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">external_grad = torch.tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>Q.backward(gradient=external_grad)<br></code></pre></td></tr></table></figure><p>梯度现在存储在<code>a.grad</code>和<code>b.grad</code>中  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># check if collected gradients are correct</span><br><span class="hljs-built_in">print</span>(<span class="hljs-number">9</span>*a**<span class="hljs-number">2</span> == a.grad)<br><span class="hljs-built_in">print</span>(-<span class="hljs-number">2</span>*b == b.grad)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">tensor</span><span class="hljs-params">([True, True])</span></span><br><span class="hljs-function"><span class="hljs-title">tensor</span><span class="hljs-params">([True, True])</span></span><br></code></pre></td></tr></table></figure><p><strong>Optional Reading - Vector Calculus using <code>autograd</code></strong></p><p>数学上，如果你有一个向量值函数  $ \vec{y}&#x3D;f(\vec{x}) ,$则$ \vec{y}$的梯度关于$ \vec{x}$为雅克比矩阵$ J$</p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202203111501461.png" alt="image-20220311150056368"></p><p>一般来说，$torch.autograd$ 是计算矢量雅克比矩阵乘积的引擎，也就是说，给定任意的向量$\vec{v}$，计算乘积$J^{T}\cdot \vec{v}$</p><p>如果 $\vec{v}$是一个标量函数$l&#x3D;g(\vec{y})$梯度</p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202203111506597.png" alt="image-20220311150628557"></p><p>那么根据链式法则，矢量与雅可比矩阵的乘积将是$l$关于$\vec{x}$的梯度</p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202203111507309.png" alt="image-20220311150754266"></p><p>向量-雅克比矩阵乘积的特点就是我们在上面例子使用的；<code>external_grad</code> 代表$\vec{v}$.</p><p><strong>Computational Graph</strong></p><p>概念上，autograd在一个由Function对象组成的有向无环图(DAG)中保存数据(tensors)和所有执行的操作(以及产生的新tensors)的记录。在这个DAG中，叶是输入 tensors，根是输出 tensors。 通过从根到叶跟踪这个图，可以使用链式法则自动计算梯度。 </p><p> 在forward pass时，autograd会同时做两件事:  </p><ul><li>运行请求的操作来计算结果tensor，并且   </li><li>在DAG中保持操作的梯度函数。</li></ul><p>当在DAG根目录上调用.backward()时，向后传递开始。 autograd:  </p><ul><li>计算每个<code>.grad_fn</code>的梯度，  </li><li>将它们累加到各自张量的<code>.grad</code>属性中，并且  </li><li>利用链式法则，一直传播到leaf tensors。</li></ul><p>下面是我们示例中的DAG的可视化表示。 在图中，箭头指向forward pass的方向。 节点表示前向传递中每个操作的向后函数。 蓝色的叶节点代表 leaf tensors a和b  </p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202203111538322.png" alt="image-20220311153809271"></p><p><strong>NOTE</strong></p><p>在PyTorch中，DAGs是动态的,需要注意的重要一点是，图是从头创建的;每次<code>.backward()</code>调用之后，autograd开始填充一个新的图。 这正是允许你在模型中使用控制流语句的原因; 如果需要，您可以在每次迭代中更改形状、大小和操作。  </p><p><strong>Exclusion from the DAG</strong></p><p><code>torch.autograd</code> tracks operations on all tensors which have their <code>requires_grad</code> flag set to <code>True</code>. For tensors that don’t require gradients, setting this attribute to <code>False</code> excludes it from the gradient computation DAG.</p><p><code>torch.autograd</code> 跟踪所有require_grad标志设置为True的tensors 的操作。 对于不需要梯度的tensors ，将此属性设置为False将其排除在梯度计算DAG中。  </p><p>操作的输出tensor 将需要梯度，即使只有一个输入tensor 具有requires_grad&#x3D;True。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.rand(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>)<br>y = torch.rand(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>)<br>z = torch.rand((<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), requires_grad=<span class="hljs-literal">True</span>)<br><br>a = x + y<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Does `a` require gradients? : <span class="hljs-subst">&#123;a.requires_grad&#125;</span>&quot;</span>)<br>b = x + z<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Does `b` require gradients?: <span class="hljs-subst">&#123;b.requires_grad&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>在神经网络中，不计算梯度的参数通常称为<strong>frozen parameters</strong>.。 如果提前知道不需要这些参数的梯度，那么“冻结”模型的一部分是很有用的(这通过减少自动计算提供了一些性能好处)。  </p><p> 从DAG中排除很重要的另一个常见的用例是对预先训练的网络进行微调  </p><p> 在微调中，我们冻结了大部分模型，通常只修改分类器层来预测新标签。 让我们通过一个小示例来演示这一点。 和前面一样，我们加载一个预先训练的resnet18模型，并冻结所有参数。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn, optim<br><br>model = torchvision.models.resnet18(pretrained=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># Freeze all the parameters in the network</span><br><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():<br>    param.requires_grad = <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p>假设我们想要在一个有10个标签的新数据集上微调模型。 在resnet中，分类器是最后一个线性层模型。 我们可以简单地用一个新的线性层(默认情况下是解冻的)来替换它，它充当我们的分类器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.fc = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p>在模型中的所有参数，除了<code>model.fc</code>的参数冻结。 计算梯度的唯一参数是<code>model.fc</code>的权重和偏差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Optimize only the classifier</span><br>optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">1e-2</span>, momentum=<span class="hljs-number">0.9</span>)<br></code></pre></td></tr></table></figure><p>注意，尽管我们在优化器中注册了所有参数，但唯一计算梯度(因此在梯度下降中更新)的参数是分类器的权重和偏差。  </p><p>在<code>torch.no_grad()</code>中，作为上下文管理器也可以使用相同的排他功能。  </p><h4 id="NEURAL-NETWORKS"><a href="#NEURAL-NETWORKS" class="headerlink" title="NEURAL NETWORKS"></a>NEURAL NETWORKS</h4><p>Neural networks can be constructed using the package.</p><p>神经网络可以用 <code>torch.nn</code>包来构建</p><p>Now that you had a glimpse of <code>autograd</code>, <code>nn</code> depends on <code>autograd</code> to define models and differentiate them. An <code>nn.Module</code> contains layers, and a method <code>forward(input)</code> that returns the <code>output</code>.</p><p>现在您已经对<code>autograd</code>有了一些了解，nn依赖于<code>autograd</code>来定义模型并区分它们。 一个<code>nn.Module</code>包含层和一个返回输出的<code>forward(input)</code>方法。  </p><p>例如，看看这个分类数字图像的网络:  </p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202203111627134.png" alt="image-20220311162744074"></p><p>这是一个简单的前馈网络。它接受输入，一个接一个地通过几个层提供输入，最后给出输出。  </p><p>一个典型的神经网络训练过程如下:  </p><ul><li>定义具有一些可学习参数(或权值)的神经网络  </li><li>迭代输入数据集</li><li>通过网络处理输入</li><li>计算损失(输出离正确值有多远)  </li><li>将梯度传播回网络的参数中  </li><li>更新网络的权值，通常使用一个简单的更新规则:  <code>weight = weight - learning_rate * gradient</code></li></ul><h5 id="Define-the-network"><a href="#Define-the-network" class="headerlink" title="Define the network"></a><strong>Define the network</strong></h5><p>Let’s define this network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Net, self).__init__()<br>        <span class="hljs-comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span><br>        <span class="hljs-comment"># kernel</span><br>        self.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-comment"># an affine operation: y = Wx + b</span><br>        self.fc1 = nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>)  <span class="hljs-comment"># 5*5 from image dimension</span><br>        self.fc2 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        self.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># Max pooling over a (2, 2) window</span><br>        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br>        <span class="hljs-comment"># If the size is a square, you can specify with a single number</span><br>        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="hljs-number">2</span>)<br>        x = torch.flatten(x, <span class="hljs-number">1</span>) <span class="hljs-comment"># flatten all dimensions except the batch dimension</span><br>        x = F.relu(self.fc1(x))<br>        x = F.relu(self.fc2(x))<br>        x = self.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br>net = Net()<br><span class="hljs-built_in">print</span>(net)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">Net(<br>  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))<br>  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))<br>  (fc1): Linear(<span class="hljs-attribute">in_features</span>=400, <span class="hljs-attribute">out_features</span>=120, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>  (fc2): Linear(<span class="hljs-attribute">in_features</span>=120, <span class="hljs-attribute">out_features</span>=84, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>  (fc3): Linear(<span class="hljs-attribute">in_features</span>=84, <span class="hljs-attribute">out_features</span>=10, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>)<br></code></pre></td></tr></table></figure><p>您只需要定义<code>forward</code>函数，<code>backward</code>函数(计算梯度的地方)将使用<code>autograd</code>为您自动定义。 你可以在<code>forward</code>函数中使用任何<code>Tensor </code>运算。  </p><p>模型的可学习参数由<code>net.parameters()</code>返回。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">params = <span class="hljs-built_in">list</span>(net.parameters())<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(params))<br><span class="hljs-built_in">print</span>(params[<span class="hljs-number">0</span>].size())  <span class="hljs-comment"># conv1&#x27;s .weight</span><br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">10</span><br><span class="hljs-attribute">torch</span>.Size([<span class="hljs-number">6</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br></code></pre></td></tr></table></figure><p>让我们尝试一个随机的32x32输入。 注:此网(LeNet)的预期输入大小为32x32。 要在MNIST数据集上使用此网络，请将数据集上的图像大小调整为32x32。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">input</span> = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)<br>out = net(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(out)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tensor</span>([[-<span class="hljs-number">0</span>.<span class="hljs-number">0004</span>, -<span class="hljs-number">0</span>.<span class="hljs-number">0036</span>,  <span class="hljs-number">0</span>.<span class="hljs-number">0390</span>, -<span class="hljs-number">0</span>.<span class="hljs-number">0431</span>,  <span class="hljs-number">0</span>.<span class="hljs-number">0928</span>,  <span class="hljs-number">0</span>.<span class="hljs-number">1599</span>, -<span class="hljs-number">0</span>.<span class="hljs-number">0806</span>, -<span class="hljs-number">0</span>.<span class="hljs-number">0377</span>,<br>          <span class="hljs-attribute">0</span>.<span class="hljs-number">0627</span>, -<span class="hljs-number">0</span>.<span class="hljs-number">1197</span>]], grad_fn=&lt;AddmmBackward0&gt;)<br></code></pre></td></tr></table></figure><p>使用随机梯度将所有参数和后台的梯度缓冲区归零:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">net.zero_grad()<br>out.backward(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><p><strong>NOTE</strong></p><p><code>torch.nn</code> 只支持小批量( mini-batches). The entire <code>torch.nn</code> package 只支持小批量样本输入，而不支持单个样本输入。  </p><p>例如, <code>nn.Conv2d</code> will take in a 4D Tensor of <code>nSamples x nChannels x Height x Width</code>.</p><p>如果你只有一个样本，只需使用<code>input.unsqueeze(0)</code>添加一个假的批量尺寸。  </p><p>在继续之前，让我们回顾一下到目前为止看到的所有类。  </p><p><strong>Recap:</strong></p><ul><li><code>torch.Tensor</code> - 一个多维数组，支持像<code>backward()</code>这样的自适应操作。 也保持了tensor的梯度w.r.t。 </li><li><code>nn.Module</code> - 模块-神经网络模块。 封装参数的方便方式，带有将它们移动到GPU、导出、加载等的帮助程序。  </li><li><code>nn.Parameter</code> - tensor的一种，当作为一个属性分配给一个模块时，它会自动注册为一个参数。 </li><li><code>autograd.Function</code> - 函数-实现一个自研操作的向前和向后定义。 每个<code>Tensor</code>操作都至少创建一个函数节点，该节点连接到创建<code>Tensor</code>并编码其历史的函数。</li></ul><p><strong>At this point, we covered:</strong></p><ul><li>Defining a neural network</li><li>Processing inputs and calling backward</li></ul><p><strong>Still Left:</strong></p><ul><li>Computing the loss</li><li>Updating the weights of the network</li></ul><h5 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a><strong>Loss Function</strong></h5><p>loss函数接受(output, target)输入对，并计算一个值来估计输出与目标的距离。  </p><p> 在神经网络包中有几种不同的损失函数。 一个简单的损失是:<code>nn.MSELoss</code>，计算输入和目标之间的均方误差。  </p><p>For example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">output = net(<span class="hljs-built_in">input</span>)<br>target = torch.randn(<span class="hljs-number">10</span>)  <span class="hljs-comment"># a dummy target, for example</span><br>target = target.view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)  <span class="hljs-comment"># make it the same shape as output</span><br>criterion = nn.MSELoss()<br><br>loss = criterion(output, target)<br><span class="hljs-built_in">print</span>(loss)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">tensor</span><span class="hljs-params">(<span class="hljs-number">0.8715</span>, grad_fn=&lt;MseLossBackward0&gt;)</span></span><br></code></pre></td></tr></table></figure><p>现在，如果你使用它的<code>.grad_fn</code>属性向后跟踪loss，你会看到这样的计算图:  </p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs xl"><span class="hljs-function"><span class="hljs-title">input</span> -&gt;</span> <span class="hljs-function"><span class="hljs-title">conv2d</span> -&gt;</span> <span class="hljs-function"><span class="hljs-title">relu</span> -&gt;</span> <span class="hljs-function"><span class="hljs-title">maxpool2d</span> -&gt;</span> <span class="hljs-function"><span class="hljs-title">conv2d</span> -&gt;</span> <span class="hljs-function"><span class="hljs-title">relu</span> -&gt;</span> maxpool2d<br>      -&gt; <span class="hljs-function"><span class="hljs-title">flatten</span> -&gt;</span> <span class="hljs-function"><span class="hljs-title">linear</span> -&gt;</span> <span class="hljs-function"><span class="hljs-title">relu</span> -&gt;</span> <span class="hljs-function"><span class="hljs-title">linear</span> -&gt;</span> <span class="hljs-function"><span class="hljs-title">relu</span> -&gt;</span> linear<br>      -&gt; MSELoss<br>      -&gt; loss<br></code></pre></td></tr></table></figure><p>So, when we call <code>loss.backward()</code>, the whole graph is differentiated w.r.t. the neural net parameters, and all Tensors in the graph that have <code>requires_grad=True</code> will have their <code>.grad</code> Tensor accumulated with the gradient.</p><p>因此，当我们调用<code>loss.backward()</code>时，将整个图对神经网络参数w.r.t进行微分，图中所有具有requires_grad&#x3D;True的Tensors ，其<code>.grad</code>张量将随梯度累加。  </p><p>为了便于说明，让我们回溯以下几个步骤:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(loss.grad_fn)  <span class="hljs-comment"># MSELoss</span><br><span class="hljs-built_in">print</span>(loss.grad_fn.next_functions[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])  <span class="hljs-comment"># Linear</span><br><span class="hljs-built_in">print</span>(loss.grad_fn.next_functions[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].next_functions[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])  <span class="hljs-comment"># ReLU</span><br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs x86asm">&lt;MseLossBackward0 object <span class="hljs-meta">at</span> <span class="hljs-number">0x7fdd1a9f4c18</span>&gt;<br>&lt;AddmmBackward0 object <span class="hljs-meta">at</span> <span class="hljs-number">0x7fdd1a9f4940</span>&gt;<br>&lt;AccumulateGrad object <span class="hljs-meta">at</span> <span class="hljs-number">0x7fdd1a9f4940</span>&gt;<br></code></pre></td></tr></table></figure><h5 id="Backprop"><a href="#Backprop" class="headerlink" title="Backprop"></a><strong>Backprop</strong></h5><p>要反向传播错误，我们需要做的就是<code>lose .backward()</code>。 你需要清除现有的梯度，否则梯度将累积到现有的梯度。  </p><p>现在我们将调用<code>loss.backward()</code>，并查看conv1在向后移动之前和之后的偏移梯度。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">net.zero_grad()     <span class="hljs-comment"># zeroes the gradient buffers of all parameters</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;conv1.bias.grad before backward&#x27;</span>)<br><span class="hljs-built_in">print</span>(net.conv1.bias.grad)<br><br>loss.backward()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;conv1.bias.grad after backward&#x27;</span>)<br><span class="hljs-built_in">print</span>(net.conv1.bias.grad)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">conv1</span>.bias.grad before backward<br><span class="hljs-attribute">tensor</span>([<span class="hljs-number">0</span>., <span class="hljs-number">0</span>., <span class="hljs-number">0</span>., <span class="hljs-number">0</span>., <span class="hljs-number">0</span>., <span class="hljs-number">0</span>.])<br><span class="hljs-attribute">conv1</span>.bias.grad after backward<br><span class="hljs-attribute">tensor</span>([ <span class="hljs-number">0</span>.<span class="hljs-number">0044</span>,  <span class="hljs-number">0</span>.<span class="hljs-number">0015</span>, -<span class="hljs-number">0</span>.<span class="hljs-number">0037</span>, -<span class="hljs-number">0</span>.<span class="hljs-number">0018</span>, -<span class="hljs-number">0</span>.<span class="hljs-number">0075</span>,  <span class="hljs-number">0</span>.<span class="hljs-number">0060</span>])<br></code></pre></td></tr></table></figure><p>现在，我们已经知道了如何使用损失函数。</p><p><strong>Read Later:</strong></p><blockquote><p>神经网络包包含各种模块和损失函数，构成了深度神经网络的构建块。 这里有一个完整的列表和文档。  <a href="https://pytorch.org/docs/nn">here</a></p></blockquote><p><strong>The only thing left to learn is:</strong></p><blockquote><ul><li>Updating the weights of the network</li></ul></blockquote><h5 id="Update-the-weights"><a href="#Update-the-weights" class="headerlink" title="Update the weights"></a>Update the weights</h5><p>The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):</p><p><code>weight = weight - learning_rate * gradient</code></p><p>We can implement this using simple Python code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">learning_rate = <span class="hljs-number">0.01</span><br><span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> net.parameters():<br>    f.data.sub_(f.grad.data * learning_rate)<br></code></pre></td></tr></table></figure><p>However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: <code>torch.optim</code> that implements all these methods. Using it is very simple:</p><p>然而，当您使用神经网络时，您需要使用各种不同的更新规则，如SGD、Nesterov-SGD、Adam、RMSProp等。 为了实现这一点，我们制作了一个小包:<code>torch.optim</code>实现了所有这些方法。 使用它非常简单:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br><span class="hljs-comment"># create your optimizer</span><br>optimizer = optim.SGD(net.parameters(), lr=<span class="hljs-number">0.01</span>)<br><br><span class="hljs-comment"># in your training loop:</span><br>optimizer.zero_grad()   <span class="hljs-comment"># zero the gradient buffers</span><br>output = net(<span class="hljs-built_in">input</span>)<br>loss = criterion(output, target)<br>loss.backward()<br>optimizer.step()    <span class="hljs-comment"># Does the update</span><br></code></pre></td></tr></table></figure><p><strong>NOTE</strong></p><p>Observe how gradient buffers had to be manually set to zero using <code>optimizer.zero_grad()</code>. This is because gradients are accumulated as explained in the <a href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#backprop">Backprop</a> section.</p><p>观察如何使用<code>optimizer.zero_grad()</code>手动将梯度缓冲区设置为零。 这是因为像Backprop部分所解释的那样，坡度会累积。  </p><h4 id="TRAINING-A-CLASSIFIER"><a href="#TRAINING-A-CLASSIFIER" class="headerlink" title="TRAINING A CLASSIFIER"></a>TRAINING A CLASSIFIER</h4><p>This is it。 您已经了解了如何定义神经网络、计算损失和更新网络的权值。</p><p>现在你可能会想，  </p><h5 id="What-about-data"><a href="#What-about-data" class="headerlink" title="What about data?"></a>What about data?</h5><p>通常，当你需要处理图像、文本、音频或视频数据时，你可以使用标准的python包来将数据加载到numpy数组中。 然后你可以把这个数组转换成一个<code>torch.*Tensor</code>.。  </p><ul><li>For images, packages such as Pillow, OpenCV are useful</li><li>For audio, packages such as scipy and librosa</li><li>For text, either raw Python or Cython based loading, or NLTK and SpaCy are useful</li></ul><p>Specifically for vision, we have created a package called <code>torchvision</code>, that has data loaders for common datasets such as ImageNet, CIFAR10, MNIST, etc. and data transformers for images, viz., <code>torchvision.datasets</code> and <code>torch.utils.data.DataLoader</code>.</p><p>特别是对于视觉(vision)，我们创建了一个名为<code>torchvision</code>的包，它拥有用于常见数据集(如ImageNet, CIFAR10, MNIST等)的数据加载器，以及用于图像的数据转换器(如<code>torchvision.datasets</code>和<code>torch.utils.data.DataLoader</code>。  </p><p>这提供了极大的便利，并避免了编写样板代码。  </p><p>For this tutorial, we will use the CIFAR10 dataset. It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.</p><p>在本教程中，我们将使用CIFAR10数据集。 它有类:“飞机”，“汽车”，“鸟”，“猫”，“鹿”，“狗”，“青蛙”，“马”，“船”，“卡车”。 CIFAR-10的图像大小为3x32x32，即32x32像素的3通道彩色图像。  </p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202203121514751.png" alt="image-20220312151450419"></p><p>cifar10</p><h5 id="Training-an-image-classifier"><a href="#Training-an-image-classifier" class="headerlink" title="Training an image classifier"></a>Training an image classifier</h5><p>We will do the following steps in order:</p><ol><li><p>Load and normalize the CIFAR10 training and test datasets using <code>torchvision</code></p></li><li><p>Define a Convolutional Neural Network</p></li><li><p>Define a loss function</p></li><li><p>Train the network on the training data</p></li><li><p>Test the network on the test data</p></li><li><p><strong>Load and normalize CIFAR10</strong></p></li></ol><p>Using <code>torchvision</code>, it’s extremely easy to load CIFAR10.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br></code></pre></td></tr></table></figure><p>The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1].</p><p><strong>NOTE</strong></p><p>If running on Windows and you get a BrokenPipeError, try setting the num_worker of torch.utils.data.DataLoader() to 0.</p><p>如果在Windows上运行，你得到一个BrokenPipeError，尝试将<code>torch.utils.data.DataLoader()</code>的num_worker设置为0。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.Compose(<br>    [transforms.ToTensor(),<br>     transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])<br><br>batch_size = <span class="hljs-number">4</span><br><br>trainset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">True</span>,<br>                                        download=<span class="hljs-literal">True</span>, transform=transform)<br>trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,<br>                                          shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br><br>testset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">False</span>,<br>                                       download=<span class="hljs-literal">True</span>, transform=transform)<br>testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,<br>                                         shuffle=<span class="hljs-literal">False</span>, num_workers=<span class="hljs-number">2</span>)<br><br>classes = (<span class="hljs-string">&#x27;plane&#x27;</span>, <span class="hljs-string">&#x27;car&#x27;</span>, <span class="hljs-string">&#x27;bird&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>,<br>           <span class="hljs-string">&#x27;deer&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;frog&#x27;</span>, <span class="hljs-string">&#x27;horse&#x27;</span>, <span class="hljs-string">&#x27;ship&#x27;</span>, <span class="hljs-string">&#x27;truck&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">Downloading https:<span class="hljs-regexp">//</span>www.cs.toronto.edu<span class="hljs-regexp">/~kriz/</span>cifar-<span class="hljs-number">10</span>-python.tar.gz to .<span class="hljs-regexp">/data/</span>cifar-<span class="hljs-number">10</span>-python.tar.gz<br>Extracting .<span class="hljs-regexp">/data/</span>cifar-<span class="hljs-number">10</span>-python.tar.gz to ./data<br>Files already downloaded and verified<br></code></pre></td></tr></table></figure><p>Let us show some of the training images, for fun.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># functions to show an image</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">imshow</span>(<span class="hljs-params">img</span>):<br>    img = img / <span class="hljs-number">2</span> + <span class="hljs-number">0.5</span>     <span class="hljs-comment"># unnormalize</span><br>    npimg = img.numpy()<br>    plt.imshow(np.transpose(npimg, (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)))<br>    plt.show()<br><br><br><span class="hljs-comment"># get some random training images</span><br>dataiter = <span class="hljs-built_in">iter</span>(trainloader)<br>images, labels = dataiter.<span class="hljs-built_in">next</span>()<br><br><span class="hljs-comment"># show images</span><br>imshow(torchvision.utils.make_grid(images))<br><span class="hljs-comment"># print labels</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;classes[labels[j]]:5s&#125;</span>&#x27;</span> <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size)))<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202203121537042.png" alt="image-20220312153718966"></p><p>Out:</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">dog   horse truck ship</span><br></code></pre></td></tr></table></figure><ol start="2"><li><strong>Define a Convolutional Neural Network</strong></li></ol><p>从前面的神经网络部分复制神经网络，并修改它以获取3通道图像(而不是定义的1通道图像)。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        self.pool = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        self.fc1 = nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        self.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.pool(F.relu(self.conv1(x)))<br>        x = self.pool(F.relu(self.conv2(x)))<br>        x = torch.flatten(x, <span class="hljs-number">1</span>) <span class="hljs-comment"># flatten all dimensions except batch</span><br>        x = F.relu(self.fc1(x))<br>        x = F.relu(self.fc2(x))<br>        x = self.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br>net = Net()<br></code></pre></td></tr></table></figure><ol start="3"><li><strong>Define a Loss function and optimizer</strong></li></ol><p>Let’s use a Classification Cross-Entropy loss and SGD with momentum.(让我们使用一个分类交叉熵损失和SGD与动量。  )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.SGD(net.parameters(), lr=<span class="hljs-number">0.001</span>, momentum=<span class="hljs-number">0.9</span>)<br></code></pre></td></tr></table></figure><p><strong>4. Train the network</strong></p><p>这时候事情开始变得有趣了。 我们只需遍历数据迭代器，将输入输入到网络并进行优化。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):  <span class="hljs-comment"># loop over the dataset multiple times</span><br><br>    running_loss = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(trainloader, <span class="hljs-number">0</span>):<br>        <span class="hljs-comment"># get the inputs; data is a list of [inputs, labels]</span><br>        inputs, labels = data<br><br>        <span class="hljs-comment"># zero the parameter gradients</span><br>        optimizer.zero_grad()<br><br>        <span class="hljs-comment"># forward + backward + optimize</span><br>        outputs = net(inputs)<br>        loss = criterion(outputs, labels)<br>        loss.backward()<br>        optimizer.step()<br><br>        <span class="hljs-comment"># print statistics</span><br>        running_loss += loss.item()<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">2000</span> == <span class="hljs-number">1999</span>:    <span class="hljs-comment"># print every 2000 mini-batches</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;[<span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span>, <span class="hljs-subst">&#123;i + <span class="hljs-number">1</span>:5d&#125;</span>] loss: <span class="hljs-subst">&#123;running_loss / <span class="hljs-number">2000</span>:<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br>            running_loss = <span class="hljs-number">0.0</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished Training&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml">[<span class="hljs-number">1</span>,  <span class="hljs-number">2000</span>] <span class="hljs-attr">loss:</span> <span class="hljs-number">2.184</span><br>[<span class="hljs-number">1</span>,  <span class="hljs-number">4000</span>] <span class="hljs-attr">loss:</span> <span class="hljs-number">1.844</span><br>[<span class="hljs-number">1</span>,  <span class="hljs-number">6000</span>] <span class="hljs-attr">loss:</span> <span class="hljs-number">1.675</span><br>[<span class="hljs-number">1</span>,  <span class="hljs-number">8000</span>] <span class="hljs-attr">loss:</span> <span class="hljs-number">1.590</span><br>[<span class="hljs-number">1</span>, <span class="hljs-number">10000</span>] <span class="hljs-attr">loss:</span> <span class="hljs-number">1.520</span><br>[<span class="hljs-number">1</span>, <span class="hljs-number">12000</span>] <span class="hljs-attr">loss:</span> <span class="hljs-number">1.475</span><br>[<span class="hljs-number">2</span>,  <span class="hljs-number">2000</span>] <span class="hljs-attr">loss:</span> <span class="hljs-number">1.393</span><br>[<span class="hljs-number">2</span>,  <span class="hljs-number">4000</span>] <span class="hljs-attr">loss:</span> <span class="hljs-number">1.361</span><br>[<span class="hljs-number">2</span>,  <span class="hljs-number">6000</span>] <span class="hljs-attr">loss:</span> <span class="hljs-number">1.342</span><br>[<span class="hljs-number">2</span>,  <span class="hljs-number">8000</span>] <span class="hljs-attr">loss:</span> <span class="hljs-number">1.328</span><br>[<span class="hljs-number">2</span>, <span class="hljs-number">10000</span>] <span class="hljs-attr">loss:</span> <span class="hljs-number">1.313</span><br>[<span class="hljs-number">2</span>, <span class="hljs-number">12000</span>] <span class="hljs-attr">loss:</span> <span class="hljs-number">1.292</span><br><span class="hljs-string">Finished</span> <span class="hljs-string">Training</span><br></code></pre></td></tr></table></figure><p>Let’s quickly save our trained model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">PATH = <span class="hljs-string">&#x27;./cifar_net.pth&#x27;</span><br>torch.save(net.state_dict(), PATH)<br></code></pre></td></tr></table></figure><p><strong>5. Test the network on the test data</strong></p><p>我们对训练数据集进行了2次的训练。 但我们需要检查网络是否了解了什么。  </p><p>我们将通过预测神经网络输出的类标签来检查这一点，并根据基本事实来检查它。 如果预测是正确的，我们将样本添加到正确的预测列表中。  </p><p>好的,第一步。 让我们显示一个来自测试集的图像来熟悉一下。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">dataiter = <span class="hljs-built_in">iter</span>(testloader)<br>images, labels = dataiter.<span class="hljs-built_in">next</span>()<br><br><span class="hljs-comment"># print images</span><br>imshow(torchvision.utils.make_grid(images))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;GroundTruth: &#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;classes[labels[j]]:5s&#125;</span>&#x27;</span> <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)))<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202203121601674.png" alt="image-20220312160110591"></p><p>Out:</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-symbol">GroundTruth:</span>  cat   <span class="hljs-keyword">ship </span> <span class="hljs-keyword">ship </span> plane<br></code></pre></td></tr></table></figure><p>接下来，让我们重新加载已保存的模型(注意:这里并不需要保存和重新加载模型，我们这样做只是为了说明如何做到这一点):  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">net = Net()<br>net.load_state_dict(torch.load(PATH))<br></code></pre></td></tr></table></figure><p>好了，现在让我们看看神经网络对上面这些例子的看法:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">outputs = net(images)<br></code></pre></td></tr></table></figure><p>The outputs are energies for the 10 classes. The higher the energy for a class, the more the network thinks that the image is of the particular class. So, let’s get the index of the highest energy:</p><p>输出是10类的energies 。 一个类的energy 越高，网络就越认为这个图像是属于这个类的。 那么，让我们得到最高能量的指数:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">_, predicted = torch.<span class="hljs-built_in">max</span>(outputs, <span class="hljs-number">1</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Predicted: &#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;classes[predicted[j]]:5s&#125;</span>&#x27;</span><br>                              <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)))<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mel">Predicted:  cat   <span class="hljs-keyword">plane</span> ship  <span class="hljs-keyword">plane</span><br></code></pre></td></tr></table></figure><p>结果似乎很好。  </p><p>让我们看看网络在整个数据集上的表现。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">correct = <span class="hljs-number">0</span><br>total = <span class="hljs-number">0</span><br><span class="hljs-comment"># since we&#x27;re not training, we don&#x27;t need to calculate the gradients for our outputs</span><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> testloader:<br>        images, labels = data<br>        <span class="hljs-comment"># calculate outputs by running images through the network</span><br>        outputs = net(images)<br>        <span class="hljs-comment"># the class with the highest energy is what we choose as prediction</span><br>        _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>        total += labels.size(<span class="hljs-number">0</span>)<br>        correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Accuracy of the network on the 10000 test images: <span class="hljs-subst">&#123;<span class="hljs-number">100</span> * correct // total&#125;</span> %&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Accuracy</span> of the network <span class="hljs-literal">on</span> the <span class="hljs-number">10000</span> test images: <span class="hljs-number">53</span> %<br></code></pre></td></tr></table></figure><p>这看起来比概率要好得多，后者的准确率是10%(从10个类中随机选出一个类)。 看来网络学到了什么。  </p><p> 嗯，哪些类执行得好，哪些类执行得不好:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># prepare to count predictions for each class</span><br>correct_pred = &#123;classname: <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> classname <span class="hljs-keyword">in</span> classes&#125;<br>total_pred = &#123;classname: <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> classname <span class="hljs-keyword">in</span> classes&#125;<br><br><span class="hljs-comment"># again no gradients needed</span><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> testloader:<br>        images, labels = data<br>        outputs = net(images)<br>        _, predictions = torch.<span class="hljs-built_in">max</span>(outputs, <span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># collect the correct predictions for each class</span><br>        <span class="hljs-keyword">for</span> label, prediction <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(labels, predictions):<br>            <span class="hljs-keyword">if</span> label == prediction:<br>                correct_pred[classes[label]] += <span class="hljs-number">1</span><br>            total_pred[classes[label]] += <span class="hljs-number">1</span><br><br><br><span class="hljs-comment"># print accuracy for each class</span><br><span class="hljs-keyword">for</span> classname, correct_count <span class="hljs-keyword">in</span> correct_pred.items():<br>    accuracy = <span class="hljs-number">100</span> * <span class="hljs-built_in">float</span>(correct_count) / total_pred[classname]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Accuracy for class: <span class="hljs-subst">&#123;classname:5s&#125;</span> is <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.1</span>f&#125;</span> %&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Accuracy</span> for class: plane is <span class="hljs-number">73</span>.<span class="hljs-number">1</span> %<br><span class="hljs-attribute">Accuracy</span> for class: car   is <span class="hljs-number">61</span>.<span class="hljs-number">5</span> %<br><span class="hljs-attribute">Accuracy</span> for class: bird  is <span class="hljs-number">48</span>.<span class="hljs-number">2</span> %<br><span class="hljs-attribute">Accuracy</span> for class: cat   is <span class="hljs-number">34</span>.<span class="hljs-number">3</span> %<br><span class="hljs-attribute">Accuracy</span> for class: deer  is <span class="hljs-number">37</span>.<span class="hljs-number">2</span> %<br><span class="hljs-attribute">Accuracy</span> for class: dog   is <span class="hljs-number">39</span>.<span class="hljs-number">8</span> %<br><span class="hljs-attribute">Accuracy</span> for class: frog  is <span class="hljs-number">61</span>.<span class="hljs-number">0</span> %<br><span class="hljs-attribute">Accuracy</span> for class: horse is <span class="hljs-number">58</span>.<span class="hljs-number">1</span> %<br><span class="hljs-attribute">Accuracy</span> for class: ship  is <span class="hljs-number">76</span>.<span class="hljs-number">5</span> %<br><span class="hljs-attribute">Accuracy</span> for class: truck is <span class="hljs-number">43</span>.<span class="hljs-number">8</span> %<br></code></pre></td></tr></table></figure><p>好吧，接下来呢?  </p><p>我们如何在GPU上运行这些神经网络?  </p><h5 id="Training-on-GPU"><a href="#Training-on-GPU" class="headerlink" title="Training on GPU"></a>Training on GPU</h5><p>就像你把Tensor 转移到GPU上一样，你把神经网络转移到GPU上。  </p><p> 让我们首先定义我们的设备为第一个可见cuda设备，如果我们有cuda可用:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">device = torch.device(<span class="hljs-string">&#x27;cuda:0&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br><br><span class="hljs-comment"># Assuming that we are on a CUDA machine, this should print a CUDA device:</span><br><br><span class="hljs-built_in">print</span>(device)<br></code></pre></td></tr></table></figure><p>Out:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">cuda</span>:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>本节的其余部分假设该设备是CUDA设备。  </p><p>然后这些方法将递归地遍历所有模块，并将它们的参数和缓冲区转换为CUDA张量:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">net.to(device)<br></code></pre></td></tr></table></figure><p>记住，你必须在每一步向GPU发送输入和目标:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs, labels = data[<span class="hljs-number">0</span>].to(device), data[<span class="hljs-number">1</span>].to(device)<br></code></pre></td></tr></table></figure><p>为什么我没有注意到与CPU相比的巨大的加速? 因为你们的网络非常小。  </p><p>练习:尝试增加网络的宽度(第一个<code>nn.Conv2d</code>的参数2)。 和第二个<code>nn.Conv2d</code>的参数1。-它们需要是相同的数字)，看看你得到了什么样的加速。  </p><p>实现目标:</p><ul><li>在高水平上理解PyTorch的张量库和神经网络。</li><li>训练一个小的神经网络来分类图像</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Matlab/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Matlab/</url>
    
    <content type="html"><![CDATA[<h3 id="Matlab"><a href="#Matlab" class="headerlink" title="Matlab"></a>Matlab</h3><h4 id="常用的数学函数"><a href="#常用的数学函数" class="headerlink" title="常用的数学函数"></a>常用的数学函数</h4><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112191133410.png" alt="image-20211219113309256"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112191133571.png" alt="image-20211219113324497"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112191134793.png" alt="image-20211219113442756"></p><h4 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h4><p><strong>冒号表达式</strong></p><p>产生一个行向量</p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112191411356.png" alt="image-20211219141127273"></p><p>结构矩阵和单元矩阵</p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112191430408.png" alt="image-20211219143037339"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112191444294.png" alt="image-20211219144425177"></p><p><strong>算术运算</strong></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201105277.png" alt="image-20211220110522165"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201107561.png" alt="image-20211220110756491"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201109312.png" alt="image-20211220110945237"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201110874.png" alt="image-20211220111023786"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201120594.png" alt="image-20211220112014473"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201123999.png" alt="image-20211220112306900"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201123999.png"></p><p><strong>字符串处理</strong></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201130560.png" alt="image-20211220113026510"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201131366.png" alt="image-20211220113132303"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201131987.png" alt="image-20211220113143932"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201142039.png" alt="image-20211220114202937"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201143119.png" alt="image-20211220114354059"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201146875.png" alt="image-20211220114607753"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201150632.png" alt="image-20211220115048566"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201151425.png" alt="image-20211220115134354"></p><p><strong>特殊矩阵</strong><br><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112210910995.png" alt="image-20211221091052841"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112210914960.png" alt="image-20211221091416873"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112210923163.png" alt="image-20211221092341057"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112210940232.png" alt="image-20211221094043142"></p><p><strong>函数文件的定义与调用</strong></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112311913328.png" alt="image-20211231191346088"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112311915566.png" alt="image-20211231191522468"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112311924889.png" alt="image-20211231192420797"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112311919489.png" alt="image-20211231191908360"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112311920491.png" alt="image-20211231192012402"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112311921852.png" alt="image-20211231192107737"></p><p>匿名函数类似于引用 （起个别名）</p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112311948173.png" alt="image-20211231194850074"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112311950463.png" alt="image-20211231195017307"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112311951308.png" alt="image-20211231195152232"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112311954254.png" alt="image-20211231195422077"></p><h4 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h4><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201121611976.png" alt="image-20220112161101911"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201121612625.png" alt="image-20220112161214556"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201121619971.png" alt="image-20220112161959860"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201121703208.png" alt="image-20220112170342134"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202201121705110.png" alt="image-20220112170532030"></p><p>$sin(\frac{1}{x})$     :  fplot(@(x)sin(1.&#x2F;x),[0,0.2],’k’)</p><p>$g(x)&#x3D;\dfrac{1}{1+e^{-z}}$ :   fplot(@(x)1.&#x2F;(1+exp(-x)),[-10,10],’r’)</p><h4 id="Exemple"><a href="#Exemple" class="headerlink" title="Exemple"></a>Exemple</h4><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112191140818.png" alt="image-20211219114040725"></p><p>find 返回非零元素的下标</p><p>isprime(x) ：素数为1，非素数为0</p><p>x(k1)输出指定k1位置的数(k1可以为矩阵)</p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201127516.png" alt="image-20211220112652391"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112201140716.png" alt="image-20211220114019606"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112210919159.png" alt="image-20211221091942058"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112210919761.png" alt="image-20211221091950673"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112252131791.png" alt="image-20211225213116709"></p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112252133788.png" alt="image-20211225213354727" style="zoom:50%;" /><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112252140479.png" alt="image-20211225214028348"></p><p><img src="https://gitee.com/k_rookie/gitee-pages-imgs/raw/master/202112252140501.png" alt="image-20211225214037400"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Gym%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"/>
    <url>/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Gym%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="Gym入门使用教程"><a href="#Gym入门使用教程" class="headerlink" title="Gym入门使用教程"></a>Gym入门使用教程</h1><p><a href="https://www.gymlibrary.ml/">官方文档</a></p><h2 id="一，激活environment-查看环境基本信息"><a href="#一，激活environment-查看环境基本信息" class="headerlink" title="一，激活environment,查看环境基本信息"></a>一，激活environment,查看环境基本信息</h2><p>env.observation_space 得到state信息，是一个Box类，<br>env.observation_space.shape 得到state的shape<br>env.action_space 得到action的信息，是一个Discrete类<br>env.action_space.n 得到action的个数</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202208081541607.png" alt="image-20220808154138526"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python3">import gym,time<br>import random<br>import numpy as np<br>#初始化环境这里选择三个不同类别的环境<br>env1 = gym.make(&#x27;LunarLander-v2&#x27;)<br>env2 = gym.make(&#x27;Pong-v0&#x27;)<br>env3 = gym.make(&#x27;CartPole-v0&#x27;)<br>#查看环境状态<br>#可以看到观察环境空间状态信息，主要是环境相关矩阵，一般是一个box类<br>print(env1.observation_space, type(env1.observation_space))<br>print(env2.observation_space, type(env2.observation_space))<br>print(env3.observation_space, type(env3.observation_space))<br>#返回的是离散action空间的值<br>print(env1.action_space, type(env1.action_space))<br>print(env2.action_space, type(env2.action_space))<br>print(env3.action_space, type(env3.action_space))<br>#LunarLander-v2的state shape和action space大小<br>print(env1.observation_space.shape, env1.action_space.n)<br>#Pong-v0的state shape和action space大小<br>print(env2.observation_space.shape, env2.action_space.n)<br></code></pre></td></tr></table></figure><h2 id="二，使用reset初始化environment-查看state信息（换个游戏场景）"><a href="#二，使用reset初始化environment-查看state信息（换个游戏场景）" class="headerlink" title="二，使用reset初始化environment,查看state信息（换个游戏场景）"></a>二，使用reset初始化environment,查看state信息（换个游戏场景）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python3">state1 = env1.reset()<br>state2 = env2.reset()<br>state3 =env3.reset()<br></code></pre></td></tr></table></figure><h2 id="三，执行action并使用render可视化"><a href="#三，执行action并使用render可视化" class="headerlink" title="三，执行action并使用render可视化"></a>三，执行action并使用render可视化</h2><p>这里主要使用env.setp来执行，输入值为一个action的序号。返回值为new state,action reward,action terminal bool 和一个其他信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">state = env1.reset()<br>env1.render()<br>new_state, reward, done, info = env1.step(<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(reward, done, info)<br></code></pre></td></tr></table></figure><h2 id="四，如何执行完一个episodic"><a href="#四，如何执行完一个episodic" class="headerlink" title="四，如何执行完一个episodic"></a>四，如何执行完一个episodic</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">env = gym.make(<span class="hljs-string">&#x27;CartPole-v0&#x27;</span>)<br><span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):<br>    observation = env.reset() <span class="hljs-comment">#初始化环境每次迭代</span><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>        env.render() <span class="hljs-comment">#显示</span><br>        <span class="hljs-built_in">print</span>(observation)<br>        action = env.action_space.sample() <span class="hljs-comment">#随机选择action</span><br>        observation, reward, done, info = env.step(action)<br>        <span class="hljs-keyword">if</span> done:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Episode finished after &#123;&#125; timesteps&quot;</span>.<span class="hljs-built_in">format</span>(t+<span class="hljs-number">1</span>))<br>            <span class="hljs-keyword">break</span><br>env.close()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>利用Python进行数据分析·第2版</title>
    <link href="/2022/08/12/%E3%80%8A%E5%88%A9%E7%94%A8Python%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%C2%B7%E7%AC%AC2%E7%89%88%E3%80%8B/"/>
    <url>/2022/08/12/%E3%80%8A%E5%88%A9%E7%94%A8Python%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%C2%B7%E7%AC%AC2%E7%89%88%E3%80%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="前置-requirements-txt"><a href="#前置-requirements-txt" class="headerlink" title="前置 requirements.txt"></a>前置 requirements.txt</h2><p>python项目如何在另一个环境上重新构建项目所需要的运行环境依赖包？</p><p>使用的时候边记载是个很麻烦的事情，总会出现遗漏的包的问题，这个时候手动安装也很麻烦，不能确定代码报错的需要安装的包是什么版本。这些问题，requirements.txt都可以解决！</p><p>生成requirements.txt，有两种方式：</p><p>第一种 适用于 单虚拟环境的情况： ：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">pip <span class="hljs-keyword">freeze</span> &gt; requirements.txt<br></code></pre></td></tr></table></figure><p>为什么只适用于单虚拟环境？因为这种方式，会将环境中的依赖包全都加入，如果使用的全局环境，则下载的所有包都会在里面，不管是不时当前项目依赖的，如下图</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207242106058.png" alt="image-20220724210619993"></p><p>当然这种情况并不是我们想要的，当我们使用的是全局环境时，可以使用第二种方法。</p><p>第二种 (推荐) 使用 pipreqs ，github地址为： <a href="https://github.com/bndr/pipreqs">https://github.com/bndr/pipreqs</a></p><p># 安装</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> pipreqs<br></code></pre></td></tr></table></figure><p># 在当前目录生成</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs brainfuck"><span class="hljs-comment">pipreqs</span> <span class="hljs-string">.</span> <span class="hljs-literal">--</span><span class="hljs-comment">encoding=utf8</span> <span class="hljs-literal">--</span><span class="hljs-comment">force</span><br></code></pre></td></tr></table></figure><p>注意 –encoding&#x3D;utf8 为使用utf8编码，不然可能会报UnicodeDecodeError: ‘gbk’ codec can’t decode byte 0xae in position 406: illegal multibyte sequence 的错误。</p><p>–force 强制执行，当 生成目录下的requirements.txt存在时覆盖。</p><p>当当当，可以看见我依赖的只有这些啦</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207242107497.png" alt="image-20220724210714441"></p><p>使用requirements.txt安装依赖的方式：</p><p>pip install -r requirements.txt</p><h2 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h2><p> <strong>重要的Python库</strong></p><p>NumPy</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207141842308.png" alt="image-20220714184200197"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207141844506.png" alt="image-20220714184405458"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207141846643.png" alt="image-20220714184616600"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207141850242.png" alt="image-20220714185004185"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207141851874.png" alt="image-20220714185100829"></p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207141852340.png" alt="image-20220714185216285"></p><p>引入惯例</p><p>Python社区已经广泛采取了一些常用模块的命名惯例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">import</span> statsmodels <span class="hljs-keyword">as</span> sm<br></code></pre></td></tr></table></figure><h2 id="第九章-绘图和可视化"><a href="#第九章-绘图和可视化" class="headerlink" title="第九章 绘图和可视化"></a>第九章 绘图和可视化</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221559781.png" alt="image-20220722155938732"></p><p>学习本章代码案例的最简单方法是在Jupyter notebook进行交互式绘图。在Jupyter notebook中执行下面的语句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib notebook<br></code></pre></td></tr></table></figure><h3 id="9-1-matplotlib-API入门"><a href="#9-1-matplotlib-API入门" class="headerlink" title="9.1 matplotlib API入门"></a>9.1 matplotlib API入门</h3><p>matplotlib的通常引入约定是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">11</span>]: <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br></code></pre></td></tr></table></figure><p>在Jupyter中运行%matplotlib notebook（或在IPython中运行%matplotlib），就可以创建一个简单的图形。如果一切设置正确，会看到图9-1：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">12</span>]: <span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>In [<span class="hljs-number">13</span>]: data = np.arange(<span class="hljs-number">10</span>)<br><br>In [<span class="hljs-number">14</span>]: data<br>Out[<span class="hljs-number">14</span>]: array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>])<br><br>In [<span class="hljs-number">15</span>]: plt.plot(data)<br></code></pre></td></tr></table></figure><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221603162.png" alt="image-20220722160327118" style="zoom:50%;" /><p>图9-1 简单的线图</p><p>虽然seaborn这样的库和pandas的内置绘图函数能够处理许多普通的绘图任务，但如果需要自定义一些高级功能的话就必须学习matplotlib API。</p><blockquote><p>笔记：虽然本书没有详细地讨论matplotlib的各种功能，但足以将你引入门。matplotlib的示例库和文档是学习高级特性的最好资源。</p></blockquote><p><strong>Figure和Subplot</strong></p><p>matplotlib的图像都位于Figure对象中。你可以用plt.figure创建一个新的Figure：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">16</span>]: fig = plt.figure()<br></code></pre></td></tr></table></figure><p>如果用的是IPython，这时会弹出一个空窗口，但在Jupyter中，必须再输入更多命令才能看到。plt.figure有一些选项，特别是figsize，它用于确保当图片保存到磁盘时具有一定的大小和纵横比。</p><p>不能通过空Figure绘图。必须用add_subplot创建一个或多个subplot才行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">17</span>]: ax1 = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>这条代码的意思是：图像应该是2×2的（即最多4张图），且当前选中的是4个subplot中的第一个（编号从1开始）。如果再把后面两个subplot也创建出来，最终得到的图像如图9-2所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">18</span>]: ax2 = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><br>In [<span class="hljs-number">19</span>]: ax3 = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221611789.png" alt="image-20220722161101728"></p><blockquote><p>提示：使用Jupyter notebook有一点不同，即每个小窗重新执行后，图形会被重置。因此，对于复杂的图形，，你必须将所有的绘图命令存在一个小窗里。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">fig = plt.figure()<br>ax1 = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>ax2 = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>ax3 = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p>如果这时执行一条绘图命令（如plt.plot([1.5, 3.5, -2, 1.6])），matplotlib就会在最后一个用过的subplot（如果没有则创建一个）上进行绘制，隐藏创建figure和subplot的过程。因此，如果我们执行下列命令，你就会得到如图9-3所示的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">20</span>]: plt.plot(np.random.randn(<span class="hljs-number">50</span>).cumsum(), <span class="hljs-string">&#x27;k--&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221614012.png" alt="image-20220722161420954"></p><p>“k–”是一个线型选项，用于告诉matplotlib绘制黑色虚线图。上面那些由fig.add_subplot所返回的对象是AxesSubplot对象，直接调用它们的实例方法就可以在其它空着的格子里面画图了，如图9-4所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">21</span>]: ax1.hist(np.random.randn(<span class="hljs-number">100</span>), bins=<span class="hljs-number">20</span>, color=<span class="hljs-string">&#x27;k&#x27;</span>, alpha=<span class="hljs-number">0.3</span>)<br><br>In [<span class="hljs-number">22</span>]: ax2.scatter(np.arange(<span class="hljs-number">30</span>), np.arange(<span class="hljs-number">30</span>) + <span class="hljs-number">3</span> * np.random.randn(<span class="hljs-number">30</span>))<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221615217.png" alt="image-20220722161540156"></p><p>你可以在matplotlib的文档中找到各种图表类型。</p><p>创建包含subplot网格的figure是一个非常常见的任务，matplotlib有一个更为方便的方法plt.subplots，它可以创建一个新的Figure，并返回一个含有已创建的subplot对象的NumPy数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">24</span>]: fig, axes = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><br>In [<span class="hljs-number">25</span>]: axes<br>Out[<span class="hljs-number">25</span>]: <br>array([[&lt;matplotlib.axes._subplots.AxesSubplot <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7fb626374048</span>&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7fb62625db00</span>&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7fb6262f6c88</span>&gt;],<br>       [&lt;matplotlib.axes._subplots.AxesSubplot <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7fb6261a36a0</span>&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7fb626181860</span>&gt;,<br>        &lt;matplotlib.axes._subplots.AxesSubplot <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7fb6260fd4e0</span>&gt;]], dtype<br>=<span class="hljs-built_in">object</span>)<br></code></pre></td></tr></table></figure><p>这是非常实用的，因为可以轻松地对axes数组进行索引，就好像是一个二维数组一样，例如axes[0,1]。你还可以通过sharex和sharey指定subplot应该具有相同的X轴或Y轴。在比较相同范围的数据时，这也是非常实用的，否则，matplotlib会自动缩放各图表的界限。有关该方法的更多信息，请参见表9-1。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221638492.png" alt="image-20220722163849434"></p><p><strong>调整subplot周围的间距</strong></p><p>默认情况下，matplotlib会在subplot外围留下一定的边距，并在subplot之间留下一定的间距。间距跟图像的高度和宽度有关，因此，如果你调整了图像大小（不管是编程还是手工），间距也会自动调整。利用Figure的subplots_adjust方法可以轻而易举地修改间距，此外，它也是个顶级函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">subplots_adjust(left=<span class="hljs-literal">None</span>, bottom=<span class="hljs-literal">None</span>, right=<span class="hljs-literal">None</span>, top=<span class="hljs-literal">None</span>,<br>                wspace=<span class="hljs-literal">None</span>, hspace=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><p>wspace和hspace用于控制宽度和高度的百分比，可以用作subplot之间的间距。下面是一个简单的例子，其中我将间距收缩到了0（如图9-5所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">fig, axes = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, sharex=<span class="hljs-literal">True</span>, sharey=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>        axes[i, j].hist(np.random.randn(<span class="hljs-number">500</span>), bins=<span class="hljs-number">50</span>, color=<span class="hljs-string">&#x27;k&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br>plt.subplots_adjust(wspace=<span class="hljs-number">0</span>, hspace=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221640911.png" alt="image-20220722164012848"></p><p>不难看出，其中的轴标签重叠了。matplotlib不会检查标签是否重叠，所以对于这种情况，你只能自己设定刻度位置和刻度标签。后面几节将会详细介绍该内容。</p><p><strong>颜色、标记和线型</strong></p><p>matplotlib的plot函数接受一组X和Y坐标，还可以接受一个表示颜色和线型的字符串缩写。例如，要根据x和y绘制绿色虚线，你可以执行如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">ax.plot(x, y, <span class="hljs-string">&#x27;g--&#x27;</span>)<br></code></pre></td></tr></table></figure><p>这种在一个字符串中指定颜色和线型的方式非常方便。在实际中，如果你是用代码绘图，你可能不想通过处理字符串来获得想要的格式。通过下面这种更为明确的方式也能得到同样的效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">ax.plot(x, y, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, color=<span class="hljs-string">&#x27;g&#x27;</span>)<br></code></pre></td></tr></table></figure><p>常用的颜色可以使用颜色缩写，你也可以指定颜色码（例如，’#CECECE’）。你可以通过查看plot的文档字符串查看所有线型的合集（在IPython和Jupyter中使用plot?）。</p><p>线图可以使用标记强调数据点。因为matplotlib可以创建连续线图，在点之间进行插值，因此有时可能不太容易看出真实数据点的位置。标记也可以放到格式字符串中，但标记类型和线型必须放在颜色后面（见图9-6）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">30</span>]: <span class="hljs-keyword">from</span> numpy.random <span class="hljs-keyword">import</span> randn<br><br>In [<span class="hljs-number">31</span>]: plt.plot(randn(<span class="hljs-number">30</span>).cumsum(), <span class="hljs-string">&#x27;ko--&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221643184.png" alt="image-20220722164303120"></p><p>还可以将其写成更为明确的形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plot(randn(<span class="hljs-number">30</span>).cumsum(), color=<span class="hljs-string">&#x27;k&#x27;</span>, linestyle=<span class="hljs-string">&#x27;dashed&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br></code></pre></td></tr></table></figure><p>在线型图中，非实际数据点默认是按线性方式插值的。可以通过drawstyle选项修改（见图9-7）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">33</span>]: data = np.random.randn(<span class="hljs-number">30</span>).cumsum()<br><br>In [<span class="hljs-number">34</span>]: plt.plot(data, <span class="hljs-string">&#x27;k--&#x27;</span>, label=<span class="hljs-string">&#x27;Default&#x27;</span>)<br>Out[<span class="hljs-number">34</span>]: [&lt;matplotlib.lines.Line2D at <span class="hljs-number">0x7fb624d86160</span>&gt;]<br><br>In [<span class="hljs-number">35</span>]: plt.plot(data, <span class="hljs-string">&#x27;k-&#x27;</span>, drawstyle=<span class="hljs-string">&#x27;steps-post&#x27;</span>, label=<span class="hljs-string">&#x27;steps-post&#x27;</span>)<br>Out[<span class="hljs-number">35</span>]: [&lt;matplotlib.lines.Line2D at <span class="hljs-number">0x7fb624d869e8</span>&gt;]<br><br>In [<span class="hljs-number">36</span>]: plt.legend(loc=<span class="hljs-string">&#x27;best&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221644920.png" alt="image-20220722164452858"></p><p>你可能注意到运行上面代码时有输出&lt;matplotlib.lines.Line2D at …&gt;。matplotlib会返回引用了新添加的子组件的对象。大多数时候，你可以放心地忽略这些输出。这里，因为我们传递了label参数到plot，我们可以创建一个plot图例，指明每条使用plt.legend的线。</p><blockquote><p>笔记：你必须调用plt.legend（或使用ax.legend，如果引用了轴的话）来创建图例，无论你绘图时是否传递label标签选项。</p></blockquote><p><strong>刻度、标签和图例</strong></p><p>对于大多数的图表装饰项，其主要实现方式有二：使用过程型的pyplot接口（例如，matplotlib.pyplot）以及更为面向对象的原生matplotlib API。</p><p>pyplot接口的设计目的就是交互式使用，含有诸如xlim、xticks和xticklabels之类的方法。它们分别控制图表的范围、刻度位置、刻度标签等。其使用方式有以下两种：</p><ul><li>调用时不带参数，则返回当前的参数值（例如，plt.xlim()返回当前的X轴绘图范围）。</li><li>调用时带参数，则设置参数值（例如，plt.xlim([0,10])会将X轴的范围设置为0到10）。</li></ul><p>所有这些方法都是对当前或最近创建的AxesSubplot起作用的。它们各自对应subplot对象上的两个方法，以xlim为例，就是ax.get_xlim和ax.set_xlim。我更喜欢使用subplot的实例方法（因为我喜欢明确的事情，而且在处理多个subplot时这样也更清楚一些）。当然你完全可以选择自己觉得方便的那个。</p><p><strong>设置标题、轴标签、刻度以及刻度标签</strong></p><p>为了说明自定义轴，我将创建一个简单的图像并绘制一段随机漫步（如图9-8所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">37</span>]: fig = plt.figure()<br><br>In [<span class="hljs-number">38</span>]: ax = fig.add_subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>In [<span class="hljs-number">39</span>]: ax.plot(np.random.randn(<span class="hljs-number">1000</span>).cumsum())<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221647044.png" alt="image-20220722164714972"></p><p>要改变x轴刻度，最简单的办法是使用set_xticks和set_xticklabels。前者告诉matplotlib要将刻度放在数据范围中的哪些位置，默认情况下，这些位置也就是刻度标签。但我们可以通过set_xticklabels将任何其他的值用作标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">40</span>]: ticks = ax.set_xticks([<span class="hljs-number">0</span>, <span class="hljs-number">250</span>, <span class="hljs-number">500</span>, <span class="hljs-number">750</span>, <span class="hljs-number">1000</span>])<br><br>In [<span class="hljs-number">41</span>]: labels = ax.set_xticklabels([<span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>, <span class="hljs-string">&#x27;three&#x27;</span>, <span class="hljs-string">&#x27;four&#x27;</span>, <span class="hljs-string">&#x27;five&#x27;</span>],<br>   ....:                             rotation=<span class="hljs-number">30</span>, fontsize=<span class="hljs-string">&#x27;small&#x27;</span>)<br></code></pre></td></tr></table></figure><p>rotation选项设定x刻度标签倾斜30度。最后，再用set_xlabel为X轴设置一个名称，并用set_title设置一个标题（见图9-9的结果）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">42</span>]: ax.set_title(<span class="hljs-string">&#x27;My first matplotlib plot&#x27;</span>)<br>Out[<span class="hljs-number">42</span>]: &lt;matplotlib.text.Text at <span class="hljs-number">0x7fb624d055f8</span>&gt;<br><br>In [<span class="hljs-number">43</span>]: ax.set_xlabel(<span class="hljs-string">&#x27;Stages&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221653445.png" alt="image-20220722165330377"></p><p>Y轴的修改方式与此类似，只需将上述代码中的x替换为y即可。轴的类有集合方法，可以批量设定绘图选项。前面的例子，也可以写为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">props = &#123;<br>    <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;My first matplotlib plot&#x27;</span>,<br>    <span class="hljs-string">&#x27;xlabel&#x27;</span>: <span class="hljs-string">&#x27;Stages&#x27;</span><br>&#125;<br>ax.<span class="hljs-built_in">set</span>(**props)<br></code></pre></td></tr></table></figure><p><strong>添加图例</strong></p><p>图例（legend）是另一种用于标识图表元素的重要工具。添加图例的方式有多种。最简单的是在添加subplot的时候传入label参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">44</span>]: <span class="hljs-keyword">from</span> numpy.random <span class="hljs-keyword">import</span> randn<br><br>In [<span class="hljs-number">45</span>]: fig = plt.figure(); ax = fig.add_subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>In [<span class="hljs-number">46</span>]: ax.plot(randn(<span class="hljs-number">1000</span>).cumsum(), <span class="hljs-string">&#x27;k&#x27;</span>, label=<span class="hljs-string">&#x27;one&#x27;</span>)<br>Out[<span class="hljs-number">46</span>]: [&lt;matplotlib.lines.Line2D at <span class="hljs-number">0x7fb624bdf860</span>&gt;]<br><br>In [<span class="hljs-number">47</span>]: ax.plot(randn(<span class="hljs-number">1000</span>).cumsum(), <span class="hljs-string">&#x27;k--&#x27;</span>, label=<span class="hljs-string">&#x27;two&#x27;</span>)<br>Out[<span class="hljs-number">47</span>]: [&lt;matplotlib.lines.Line2D at <span class="hljs-number">0x7fb624be90f0</span>&gt;]<br><br>In [<span class="hljs-number">48</span>]: ax.plot(randn(<span class="hljs-number">1000</span>).cumsum(), <span class="hljs-string">&#x27;k.&#x27;</span>, label=<span class="hljs-string">&#x27;three&#x27;</span>)<br>Out[<span class="hljs-number">48</span>]: [&lt;matplotlib.lines.Line2D at <span class="hljs-number">0x7fb624be9160</span>&gt;]<br></code></pre></td></tr></table></figure><p>在此之后，你可以调用ax.legend()或plt.legend()来自动创建图例（结果见图9-10）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">49</span>]: ax.legend(loc=<span class="hljs-string">&#x27;best&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207221659843.png" alt="image-20220722165900772"></p><p>legend方法有几个其它的loc位置参数选项。请查看文档字符串（使用ax.legend?）。</p><p>loc告诉matplotlib要将图例放在哪。如果你不是吹毛求疵的话，”best”是不错的选择，因为它会选择最不碍事的位置。要从图例中去除一个或多个元素，不传入label或传入label&#x3D;’<em>nolegend</em>‘即可。（中文第一版这里把best错写成了beat）</p><p><strong>注解以及在Subplot上绘图</strong></p><p>除标准的绘图类型，你可能还希望绘制一些子集的注解，可能是文本、箭头或其他图形等。注解和文字可以通过text、arrow和annotate函数进行添加。text可以将文本绘制在图表的指定坐标(x,y)，还可以加上一些自定义格式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">ax.text(x, y, <span class="hljs-string">&#x27;Hello world!&#x27;</span>,<br>        family=<span class="hljs-string">&#x27;monospace&#x27;</span>, fontsize=<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p>注解中可以既含有文本也含有箭头。例如，我们根据最近的标准普尔500指数价格（来自Yahoo!Finance）绘制一张曲线图，并标出2008年到2009年金融危机期间的一些重要日期。你可以在Jupyter notebook的一个小窗中试验这段代码（图9-11是结果）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime<br><br>fig = plt.figure()<br>ax = fig.add_subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>data = pd.read_csv(<span class="hljs-string">&#x27;examples/spx.csv&#x27;</span>, index_col=<span class="hljs-number">0</span>, parse_dates=<span class="hljs-literal">True</span>)<br>spx = data[<span class="hljs-string">&#x27;SPX&#x27;</span>]<br><br>spx.plot(ax=ax, style=<span class="hljs-string">&#x27;k-&#x27;</span>)<br><br>crisis_data = [<br>    (datetime(<span class="hljs-number">2007</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>), <span class="hljs-string">&#x27;Peak of bull market&#x27;</span>),<br>    (datetime(<span class="hljs-number">2008</span>, <span class="hljs-number">3</span>, <span class="hljs-number">12</span>), <span class="hljs-string">&#x27;Bear Stearns Fails&#x27;</span>),<br>    (datetime(<span class="hljs-number">2008</span>, <span class="hljs-number">9</span>, <span class="hljs-number">15</span>), <span class="hljs-string">&#x27;Lehman Bankruptcy&#x27;</span>)<br>]<br><br><span class="hljs-keyword">for</span> date, label <span class="hljs-keyword">in</span> crisis_data:<br>    ax.annotate(label, xy=(date, spx.asof(date) + <span class="hljs-number">75</span>),<br>                xytext=(date, spx.asof(date) + <span class="hljs-number">225</span>),<br>                arrowprops=<span class="hljs-built_in">dict</span>(facecolor=<span class="hljs-string">&#x27;black&#x27;</span>, headwidth=<span class="hljs-number">4</span>, width=<span class="hljs-number">2</span>,<br>                                headlength=<span class="hljs-number">4</span>),<br>                horizontalalignment=<span class="hljs-string">&#x27;left&#x27;</span>, verticalalignment=<span class="hljs-string">&#x27;top&#x27;</span>)<br><br><span class="hljs-comment"># Zoom in on 2007-2010</span><br>ax.set_xlim([<span class="hljs-string">&#x27;1/1/2007&#x27;</span>, <span class="hljs-string">&#x27;1/1/2011&#x27;</span>])<br>ax.set_ylim([<span class="hljs-number">600</span>, <span class="hljs-number">1800</span>])<br><br>ax.set_title(<span class="hljs-string">&#x27;Important dates in the 2008-2009 financial crisis&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231014940.png" alt="image-20220723101428755"></p><p>这张图中有几个重要的点要强调：ax.annotate方法可以在指定的x和y坐标轴绘制标签。我们使用set_xlim和set_ylim人工设定起始和结束边界，而不使用matplotlib的默认方法。最后，用ax.set_title添加图标标题。</p><p>更多有关注解的示例，请访问matplotlib的在线示例库。</p><p>图形的绘制要麻烦一些。matplotlib有一些表示常见图形的对象。这些对象被称为块（patch）。其中有些（如Rectangle和Circle），可以在matplotlib.pyplot中找到，但完整集合位于matplotlib.patches。</p><p>要在图表中添加一个图形，你需要创建一个块对象shp，然后通过ax.add_patch(shp)将其添加到subplot中（如图9-12所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">fig = plt.figure()<br>ax = fig.add_subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>rect = plt.Rectangle((<span class="hljs-number">0.2</span>, <span class="hljs-number">0.75</span>), <span class="hljs-number">0.4</span>, <span class="hljs-number">0.15</span>, color=<span class="hljs-string">&#x27;k&#x27;</span>, alpha=<span class="hljs-number">0.3</span>)<br>circ = plt.Circle((<span class="hljs-number">0.7</span>, <span class="hljs-number">0.2</span>), <span class="hljs-number">0.15</span>, color=<span class="hljs-string">&#x27;b&#x27;</span>, alpha=<span class="hljs-number">0.3</span>)<br>pgon = plt.Polygon([[<span class="hljs-number">0.15</span>, <span class="hljs-number">0.15</span>], [<span class="hljs-number">0.35</span>, <span class="hljs-number">0.4</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.6</span>]],<br>                   color=<span class="hljs-string">&#x27;g&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br><br>ax.add_patch(rect)<br>ax.add_patch(circ)<br>ax.add_patch(pgon)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231019981.png" alt="image-20220723101900879"></p><p>如果查看许多常见图表对象的具体实现代码，你就会发现它们其实就是由块patch组装而成的。</p><p><strong>将图表保存到文件</strong></p><p>利用plt.savefig可以将当前图表保存到文件。该方法相当于Figure对象的实例方法savefig。例如，要将图表保存为SVG文件，你只需输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.savefig(<span class="hljs-string">&#x27;figpath.svg&#x27;</span>)<br></code></pre></td></tr></table></figure><p>文件类型是通过文件扩展名推断出来的。因此，如果你使用的是.pdf，就会得到一个PDF文件。我在发布图片时最常用到两个重要的选项是dpi（控制“每英寸点数”分辨率）和bbox_inches（可以剪除当前图表周围的空白部分）。要得到一张带有最小白边且分辨率为400DPI的PNG图片，你可以：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.savefig(<span class="hljs-string">&#x27;figpath.png&#x27;</span>, dpi=<span class="hljs-number">400</span>, bbox_inches=<span class="hljs-string">&#x27;tight&#x27;</span>)<br></code></pre></td></tr></table></figure><p>savefig并非一定要写入磁盘，也可以写入任何文件型的对象，比如BytesIO：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO<br>buffer = BytesIO()<br>plt.savefig(buffer)<br>plot_data = buffer.getvalue()<br></code></pre></td></tr></table></figure><p>表9-2列出了savefig的其它选项。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231025861.png" alt="image-20220723102514760"></p><p><strong>matplotlib配置</strong></p><p>matplotlib自带一些配色方案，以及为生成出版质量的图片而设定的默认配置信息。幸运的是，几乎所有默认行为都能通过一组全局参数进行自定义，它们可以管理图像大小、subplot边距、配色方案、字体大小、网格类型等。一种Python编程方式配置系统的方法是使用rc方法。例如，要将全局的图像默认大小设置为10×10，你可以执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.rc(<span class="hljs-string">&#x27;figure&#x27;</span>, figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><p>rc的第一个参数是希望自定义的对象，如’figure’、’axes’、’xtick’、’ytick’、’grid’、’legend’等。其后可以跟上一系列的关键字参数。一个简单的办法是将这些选项写成一个字典：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">font_options = &#123;<span class="hljs-string">&#x27;family&#x27;</span> : <span class="hljs-string">&#x27;monospace&#x27;</span>,<br>                <span class="hljs-string">&#x27;weight&#x27;</span> : <span class="hljs-string">&#x27;bold&#x27;</span>,<br>                <span class="hljs-string">&#x27;size&#x27;</span>   : <span class="hljs-string">&#x27;small&#x27;</span>&#125;<br>plt.rc(<span class="hljs-string">&#x27;font&#x27;</span>, **font_options)<br></code></pre></td></tr></table></figure><p>要了解全部的自定义选项，请查阅matplotlib的配置文件matplotlibrc（位于matplotlib&#x2F;mpl-data目录中）。如果对该文件进行了自定义，并将其放在你自己的.matplotlibrc目录中，则每次使用matplotlib时就会加载该文件。</p><p>下一节，我们会看到，seaborn包有若干内置的绘图主题或类型，它们使用了matplotlib的内部配置。</p><h3 id="9-2-使用pandas和seaborn绘图"><a href="#9-2-使用pandas和seaborn绘图" class="headerlink" title="9.2 使用pandas和seaborn绘图"></a>9.2 使用pandas和seaborn绘图</h3><p>matplotlib实际上是一种比较低级的工具。要绘制一张图表，你组装一些基本组件就行：数据展示（即图表类型：线型图、柱状图、盒形图、散布图、等值线图等）、图例、标题、刻度标签以及其他注解型信息。</p><p>在pandas中，我们有多列数据，还有行和列标签。pandas自身就有内置的方法，用于简化从DataFrame和Series绘制图形。另一个库seaborn（<a href="https://seaborn.pydata.org/%EF%BC%89%EF%BC%8C%E7%94%B1Michael">https://seaborn.pydata.org/），由Michael</a> Waskom创建的静态图形库。Seaborn简化了许多常见可视类型的创建。</p><blockquote><p>提示：引入seaborn会修改matplotlib默认的颜色方案和绘图类型，以提高可读性和美观度。即使你不使用seaborn API，你可能也会引入seaborn，作为提高美观度和绘制常见matplotlib图形的简化方法。</p></blockquote><p><strong>线型图</strong></p><p>Series和DataFrame都有一个用于生成各类图表的plot方法。默认情况下，它们所生成的是线型图（如图9-13所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">60</span>]: s = pd.Series(np.random.randn(<span class="hljs-number">10</span>).cumsum(), index=np.arange(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>))<br><br>In [<span class="hljs-number">61</span>]: s.plot()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231031311.png" alt="image-20220723103150223"></p><p>该Series对象的索引会被传给matplotlib，并用以绘制X轴。可以通过use_index&#x3D;False禁用该功能。X轴的刻度和界限可以通过xticks和xlim选项进行调节，Y轴就用yticks和ylim。plot参数的完整列表请参见表9-3。我只会讲解其中几个，剩下的就留给读者自己去研究了。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231038457.png" alt="image-20220723103804329"></p><p>pandas的大部分绘图方法都有一个可选的ax参数，它可以是一个matplotlib的subplot对象。这使你能够在网格布局中更为灵活地处理subplot的位置。</p><p>DataFrame的plot方法会在一个subplot中为各列绘制一条线，并自动创建图例（如图9-14所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">62</span>]: df = pd.DataFrame(np.random.randn(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>).cumsum(<span class="hljs-number">0</span>),<br>   ....:                   columns=[<span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;D&#x27;</span>],<br>   ....:                   index=np.arange(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>))<br><br>In [<span class="hljs-number">63</span>]: df.plot()<br></code></pre></td></tr></table></figure><p>plot属性包含一批不同绘图类型的方法。例如，df.plot()等价于df.plot.line()。后面会学习这些方法。</p><blockquote><p>笔记：plot的其他关键字参数会被传给相应的matplotlib绘图函数，所以要更深入地自定义图表，就必须学习更多有关matplotlib API的知识。</p></blockquote><p>DataFrame还有一些用于对列进行灵活处理的选项，例如，是要将所有列都绘制到一个subplot中还是创建各自的subplot。详细信息请参见表9-4。</p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231040165.png" alt="image-20220723104032051"></p><blockquote><p>注意： 有关时间序列的绘图，请见第11章。</p></blockquote><p><strong>柱状图</strong></p><p>plot.bar()和plot.barh()分别绘制水平和垂直的柱状图。这时，Series和DataFrame的索引将会被用作X（bar）或Y（barh）刻度（如图9-15所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">64</span>]: fig, axes = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br><br>In [<span class="hljs-number">65</span>]: data = pd.Series(np.random.rand(<span class="hljs-number">16</span>), index=<span class="hljs-built_in">list</span>(<span class="hljs-string">&#x27;abcdefghijklmnop&#x27;</span>))<br><br>In [<span class="hljs-number">66</span>]: data.plot.bar(ax=axes[<span class="hljs-number">0</span>], color=<span class="hljs-string">&#x27;k&#x27;</span>, alpha=<span class="hljs-number">0.7</span>)<br>Out[<span class="hljs-number">66</span>]: &lt;matplotlib.axes._subplots.AxesSubplot at <span class="hljs-number">0x7fb62493d470</span>&gt;<br><br>In [<span class="hljs-number">67</span>]: data.plot.barh(ax=axes[<span class="hljs-number">1</span>], color=<span class="hljs-string">&#x27;k&#x27;</span>, alpha=<span class="hljs-number">0.7</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231042592.png" alt="image-20220723104214531"></p><p>color&#x3D;’k’和alpha&#x3D;0.7设定了图形的颜色为黑色，并使用部分的填充透明度。对于DataFrame，柱状图会将每一行的值分为一组，并排显示，如图9-16所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">69</span>]: df = pd.DataFrame(np.random.rand(<span class="hljs-number">6</span>, <span class="hljs-number">4</span>),<br>   ....:                   index=[<span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>, <span class="hljs-string">&#x27;three&#x27;</span>, <span class="hljs-string">&#x27;four&#x27;</span>, <span class="hljs-string">&#x27;five&#x27;</span>, <span class="hljs-string">&#x27;six&#x27;</span>],<br>   ....:                   columns=pd.Index([<span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;D&#x27;</span>], name=<span class="hljs-string">&#x27;Genus&#x27;</span>))<br><br>In [<span class="hljs-number">70</span>]: df<br>Out[<span class="hljs-number">70</span>]: <br>Genus         A         B         C         D<br>one    <span class="hljs-number">0.370670</span>  <span class="hljs-number">0.602792</span>  <span class="hljs-number">0.229159</span>  <span class="hljs-number">0.486744</span><br>two    <span class="hljs-number">0.420082</span>  <span class="hljs-number">0.571653</span>  <span class="hljs-number">0.049024</span>  <span class="hljs-number">0.880592</span><br>three  <span class="hljs-number">0.814568</span>  <span class="hljs-number">0.277160</span>  <span class="hljs-number">0.880316</span>  <span class="hljs-number">0.431326</span><br>four   <span class="hljs-number">0.374020</span>  <span class="hljs-number">0.899420</span>  <span class="hljs-number">0.460304</span>  <span class="hljs-number">0.100843</span><br>five   <span class="hljs-number">0.433270</span>  <span class="hljs-number">0.125107</span>  <span class="hljs-number">0.494675</span>  <span class="hljs-number">0.961825</span><br>six    <span class="hljs-number">0.601648</span>  <span class="hljs-number">0.478576</span>  <span class="hljs-number">0.205690</span>  <span class="hljs-number">0.560547</span><br><br>In [<span class="hljs-number">71</span>]: df.plot.bar()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231042295.png" alt="image-20220723104238218"></p><p>注意，DataFrame各列的名称”Genus”被用作了图例的标题。</p><p>设置stacked&#x3D;True即可为DataFrame生成堆积柱状图，这样每行的值就会被堆积在一起（如图9-17所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">73</span>]: df.plot.barh(stacked=<span class="hljs-literal">True</span>, alpha=<span class="hljs-number">0.5</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231043935.png" alt="image-20220723104355868"></p><blockquote><p>笔记：柱状图有一个非常不错的用法：利用value_counts图形化显示Series中各值的出现频率，比如s.value_counts().plot.bar()。</p></blockquote><p>再以本书前面用过的那个有关小费的数据集为例，假设我们想要做一张堆积柱状图以展示每天各种聚会规模的数据点的百分比。我用read_csv将数据加载进来，然后根据日期和聚会规模创建一张交叉表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">75</span>]: tips = pd.read_csv(<span class="hljs-string">&#x27;examples/tips.csv&#x27;</span>)<br><br>In [<span class="hljs-number">76</span>]: party_counts = pd.crosstab(tips[<span class="hljs-string">&#x27;day&#x27;</span>], tips[<span class="hljs-string">&#x27;size&#x27;</span>])<br><br>In [<span class="hljs-number">77</span>]: party_counts<br>Out[<span class="hljs-number">77</span>]: <br>size  <span class="hljs-number">1</span>   <span class="hljs-number">2</span>   <span class="hljs-number">3</span>   <span class="hljs-number">4</span>  <span class="hljs-number">5</span>  <span class="hljs-number">6</span><br>day                      <br>Fri   <span class="hljs-number">1</span>  <span class="hljs-number">16</span>   <span class="hljs-number">1</span>   <span class="hljs-number">1</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span><br>Sat   <span class="hljs-number">2</span>  <span class="hljs-number">53</span>  <span class="hljs-number">18</span>  <span class="hljs-number">13</span>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span><br>Sun   <span class="hljs-number">0</span>  <span class="hljs-number">39</span>  <span class="hljs-number">15</span>  <span class="hljs-number">18</span>  <span class="hljs-number">3</span>  <span class="hljs-number">1</span><br>Thur  <span class="hljs-number">1</span>  <span class="hljs-number">48</span>   <span class="hljs-number">4</span>   <span class="hljs-number">5</span>  <span class="hljs-number">1</span>  <span class="hljs-number">3</span><br><br><span class="hljs-comment"># Not many 1- and 6-person parties</span><br>In [<span class="hljs-number">78</span>]: party_counts = party_counts.loc[:, <span class="hljs-number">2</span>:<span class="hljs-number">5</span>]<br></code></pre></td></tr></table></figure><p>然后进行规格化，使得各行的和为1，并生成图表（如图9-18所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Normalize to sum to 1</span><br>In [<span class="hljs-number">79</span>]: party_pcts = party_counts.div(party_counts.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>), axis=<span class="hljs-number">0</span>)<br><br>In [<span class="hljs-number">80</span>]: party_pcts<br>Out[<span class="hljs-number">80</span>]: <br>size         <span class="hljs-number">2</span>         <span class="hljs-number">3</span>         <span class="hljs-number">4</span>         <span class="hljs-number">5</span><br>day                                         <br>Fri   <span class="hljs-number">0.888889</span>  <span class="hljs-number">0.055556</span>  <span class="hljs-number">0.055556</span>  <span class="hljs-number">0.000000</span><br>Sat   <span class="hljs-number">0.623529</span>  <span class="hljs-number">0.211765</span>  <span class="hljs-number">0.152941</span>  <span class="hljs-number">0.011765</span><br>Sun   <span class="hljs-number">0.520000</span>  <span class="hljs-number">0.200000</span>  <span class="hljs-number">0.240000</span>  <span class="hljs-number">0.040000</span><br>Thur  <span class="hljs-number">0.827586</span>  <span class="hljs-number">0.068966</span>  <span class="hljs-number">0.086207</span>  <span class="hljs-number">0.017241</span><br><br>In [<span class="hljs-number">81</span>]: party_pcts.plot.bar()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231049952.png" alt="image-20220723104954886"></p><p>于是，通过该数据集就可以看出，聚会规模在周末会变大。</p><p>对于在绘制一个图形之前，需要进行合计的数据，使用seaborn可以减少工作量。用seaborn来看每天的小费比例（图9-19是结果）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">83</span>]: <span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br>In [<span class="hljs-number">84</span>]: tips[<span class="hljs-string">&#x27;tip_pct&#x27;</span>] = tips[<span class="hljs-string">&#x27;tip&#x27;</span>] / (tips[<span class="hljs-string">&#x27;total_bill&#x27;</span>] - tips[<span class="hljs-string">&#x27;tip&#x27;</span>])<br><br>In [<span class="hljs-number">85</span>]: tips.head()<br>Out[<span class="hljs-number">85</span>]: <br>   total_bill   tip smoker  day    time  size   tip_pct<br><span class="hljs-number">0</span>       <span class="hljs-number">16.99</span>  <span class="hljs-number">1.01</span>     No  Sun  Dinner     <span class="hljs-number">2</span>  <span class="hljs-number">0.063204</span><br><span class="hljs-number">1</span>       <span class="hljs-number">10.34</span>  <span class="hljs-number">1.66</span>     No  Sun  Dinner     <span class="hljs-number">3</span>  <span class="hljs-number">0.191244</span><br><span class="hljs-number">2</span>       <span class="hljs-number">21.01</span>  <span class="hljs-number">3.50</span>     No  Sun  Dinner     <span class="hljs-number">3</span>  <span class="hljs-number">0.199886</span><br><span class="hljs-number">3</span>       <span class="hljs-number">23.68</span>  <span class="hljs-number">3.31</span>     No  Sun  Dinner     <span class="hljs-number">2</span>  <span class="hljs-number">0.162494</span><br><span class="hljs-number">4</span>       <span class="hljs-number">24.59</span>  <span class="hljs-number">3.61</span>     No  Sun  Dinner     <span class="hljs-number">4</span>  <span class="hljs-number">0.172069</span><br><br>In [<span class="hljs-number">86</span>]: sns.barplot(x=<span class="hljs-string">&#x27;tip_pct&#x27;</span>, y=<span class="hljs-string">&#x27;day&#x27;</span>, data=tips, orient=<span class="hljs-string">&#x27;h&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231051930.png" alt="image-20220723105133868"></p><p>seaborn的绘制函数使用data参数，它可能是pandas的DataFrame。其它的参数是关于列的名字。因为一天的每个值有多次观察，柱状图的值是tip_pct的平均值。绘制在柱状图上的黑线代表95%置信区间（可以通过可选参数配置）。</p><p>seaborn.barplot有颜色选项，使我们能够通过一个额外的值设置（见图9-20）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">88</span>]: sns.barplot(x=<span class="hljs-string">&#x27;tip_pct&#x27;</span>, y=<span class="hljs-string">&#x27;day&#x27;</span>, hue=<span class="hljs-string">&#x27;time&#x27;</span>, data=tips, orient=<span class="hljs-string">&#x27;h&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207231054307.png" alt="image-20220723105400242"></p><p>注意，seaborn已经自动修改了图形的美观度：默认调色板，图形背景和网格线的颜色。你可以用seaborn.set在不同的图形外观之间切换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">90</span>]: sns.<span class="hljs-built_in">set</span>(style=<span class="hljs-string">&quot;whitegrid&quot;</span>)<br></code></pre></td></tr></table></figure><p><strong>直方图和密度图</strong></p><p>直方图（histogram）是一种可以对值频率进行离散化显示的柱状图。数据点被拆分到离散的、间隔均匀的面元中，绘制的是各面元中数据点的数量。再以前面那个小费数据为例，通过在Series使用plot.hist方法，我们可以生成一张“小费占消费总额百分比”的直方图（如图9-21所示）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">92</span>]: tips[<span class="hljs-string">&#x27;tip_pct&#x27;</span>].plot.hist(bins=<span class="hljs-number">50</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241238989.png" alt="image-20220724123753852"></p><p>与此相关的一种图表类型是密度图，它是通过计算“可能会产生观测数据的连续概率分布的估计”而产生的。一般的过程是将该分布近似为一组核（即诸如正态分布之类的较为简单的分布）。因此，密度图也被称作KDE（Kernel Density Estimate，核密度估计）图。使用plot.kde和标准混合正态分布估计即可生成一张密度图（见图9-22）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">94</span>]: tips[<span class="hljs-string">&#x27;tip_pct&#x27;</span>].plot.density()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241240396.png" alt="image-20220724124030315"></p><p>seaborn的distplot方法绘制直方图和密度图更加简单，还可以同时画出直方图和连续密度估计图。作为例子，考虑一个双峰分布，由两个不同的标准正态分布组成（见图9-23）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">96</span>]: comp1 = np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=<span class="hljs-number">200</span>)<br><br>In [<span class="hljs-number">97</span>]: comp2 = np.random.normal(<span class="hljs-number">10</span>, <span class="hljs-number">2</span>, size=<span class="hljs-number">200</span>)<br><br>In [<span class="hljs-number">98</span>]: values = pd.Series(np.concatenate([comp1, comp2]))<br><br>In [<span class="hljs-number">99</span>]: sns.distplot(values, bins=<span class="hljs-number">100</span>, color=<span class="hljs-string">&#x27;k&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241241980.png" alt="image-20220724124111907"></p><p><strong>散布图或点图</strong></p><p>点图或散布图是观察两个一维数据序列之间的关系的有效手段。在下面这个例子中，我加载了来自statsmodels项目的macrodata数据集，选择了几个变量，然后计算对数差：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">100</span>]: macro = pd.read_csv(<span class="hljs-string">&#x27;examples/macrodata.csv&#x27;</span>)<br><br>In [<span class="hljs-number">101</span>]: data = macro[[<span class="hljs-string">&#x27;cpi&#x27;</span>, <span class="hljs-string">&#x27;m1&#x27;</span>, <span class="hljs-string">&#x27;tbilrate&#x27;</span>, <span class="hljs-string">&#x27;unemp&#x27;</span>]]<br><br>In [<span class="hljs-number">102</span>]: trans_data = np.log(data).diff().dropna()<br><br>In [<span class="hljs-number">103</span>]: trans_data[-<span class="hljs-number">5</span>:]<br>Out[<span class="hljs-number">103</span>]: <br>          cpi        m1  tbilrate     unemp<br><span class="hljs-number">198</span> -<span class="hljs-number">0.007904</span>  <span class="hljs-number">0.045361</span> -<span class="hljs-number">0.396881</span>  <span class="hljs-number">0.105361</span><br><span class="hljs-number">199</span> -<span class="hljs-number">0.021979</span>  <span class="hljs-number">0.066753</span> -<span class="hljs-number">2.277267</span>  <span class="hljs-number">0.139762</span><br><span class="hljs-number">200</span>  <span class="hljs-number">0.002340</span>  <span class="hljs-number">0.010286</span>  <span class="hljs-number">0.606136</span>  <span class="hljs-number">0.160343</span><br><span class="hljs-number">201</span>  <span class="hljs-number">0.008419</span>  <span class="hljs-number">0.037461</span> -<span class="hljs-number">0.200671</span>  <span class="hljs-number">0.127339</span><br><span class="hljs-number">202</span>  <span class="hljs-number">0.008894</span>  <span class="hljs-number">0.012202</span> -<span class="hljs-number">0.405465</span>  <span class="hljs-number">0.042560</span><br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241241474.png" alt="image-20220724124144388"></p><p>在探索式数据分析工作中，同时观察一组变量的散布图是很有意义的，这也被称为散布图矩阵（scatter plot matrix）。纯手工创建这样的图表很费工夫，所以seaborn提供了一个便捷的pairplot函数，它支持在对角线上放置每个变量的直方图或密度估计（见图9-25）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">107</span>]: sns.pairplot(trans_data, diag_kind=<span class="hljs-string">&#x27;kde&#x27;</span>, plot_kws=&#123;<span class="hljs-string">&#x27;alpha&#x27;</span>: <span class="hljs-number">0.2</span>&#125;)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241242670.png" alt="image-20220724124210541"></p><p>你可能注意到了plot_kws参数。它可以让我们传递配置选项到非对角线元素上的图形使用。对于更详细的配置选项，可以查阅seaborn.pairplot文档字符串。</p><p><strong>分面网格（facet grid）和类型数据</strong></p><p>要是数据集有额外的分组维度呢？有多个分类变量的数据可视化的一种方法是使用小面网格。seaborn有一个有用的内置函数factorplot，可以简化制作多种分面图（见图9-26）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">108</span>]: sns.factorplot(x=<span class="hljs-string">&#x27;day&#x27;</span>, y=<span class="hljs-string">&#x27;tip_pct&#x27;</span>, hue=<span class="hljs-string">&#x27;time&#x27;</span>, col=<span class="hljs-string">&#x27;smoker&#x27;</span>,<br>  .....:                kind=<span class="hljs-string">&#x27;bar&#x27;</span>, data=tips[tips.tip_pct &lt; <span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241242631.png" alt="image-20220724124253548"></p><p>除了在分面中用不同的颜色按时间分组，我们还可以通过给每个时间值添加一行来扩展分面网格：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">109</span>]: sns.factorplot(x=<span class="hljs-string">&#x27;day&#x27;</span>, y=<span class="hljs-string">&#x27;tip_pct&#x27;</span>, row=<span class="hljs-string">&#x27;time&#x27;</span>,<br>   .....:                col=<span class="hljs-string">&#x27;smoker&#x27;</span>,<br>   .....:                kind=<span class="hljs-string">&#x27;bar&#x27;</span>, data=tips[tips.tip_pct &lt; <span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241243830.png" alt="image-20220724124318731"></p><p>factorplot支持其它的绘图类型，你可能会用到。例如，盒图（它可以显示中位数，四分位数，和异常值）就是一个有用的可视化类型（见图9-28）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">In [<span class="hljs-number">110</span>]: sns.factorplot(x=<span class="hljs-string">&#x27;tip_pct&#x27;</span>, y=<span class="hljs-string">&#x27;day&#x27;</span>, kind=<span class="hljs-string">&#x27;box&#x27;</span>,<br>   .....:                data=tips[tips.tip_pct &lt; <span class="hljs-number">0.5</span>])<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202207241243112.png" alt="image-20220724124359020"></p><p>使用更通用的seaborn.FacetGrid类，你可以创建自己的分面网格。请查阅seaborn的文档（<a href="https://seaborn.pydata.org/%EF%BC%89%E3%80%82">https://seaborn.pydata.org/）。</a></p><h3 id="9-3-其它的Python可视化工具"><a href="#9-3-其它的Python可视化工具" class="headerlink" title="9.3 其它的Python可视化工具"></a>9.3 其它的Python可视化工具</h3><p>与其它开源库类似，Python创建图形的方式非常多（根本罗列不完）。自从2010年，许多开发工作都集中在创建交互式图形以便在Web上发布。利用工具如Boken（<a href="https://bokeh.pydata.org/en/latest/%EF%BC%89%E5%92%8CPlotly%EF%BC%88https://github.com/plotly/plotly.py%EF%BC%89%EF%BC%8C%E7%8E%B0%E5%9C%A8%E5%8F%AF%E4%BB%A5%E5%88%9B%E5%BB%BA%E5%8A%A8%E6%80%81%E4%BA%A4%E4%BA%92%E5%9B%BE%E5%BD%A2%EF%BC%8C%E7%94%A8%E4%BA%8E%E7%BD%91%E9%A1%B5%E6%B5%8F%E8%A7%88%E5%99%A8%E3%80%82">https://bokeh.pydata.org/en/latest/）和Plotly（https://github.com/plotly/plotly.py），现在可以创建动态交互图形，用于网页浏览器。</a></p><p>对于创建用于打印或网页的静态图形，我建议默认使用matplotlib和附加的库，比如pandas和seaborn。对于其它数据可视化要求，学习其它的可用工具可能是有用的。我鼓励你探索绘图的生态系统，因为它将持续发展。</p><h3 id="9-4-总结"><a href="#9-4-总结" class="headerlink" title="9.4 总结"></a>9.4 总结</h3><p>本章的目的是熟悉一些基本的数据可视化操作，使用pandas，matplotlib，和seaborn。如果视觉显示数据分析的结果对你的工作很重要，我鼓励你寻求更多的资源来了解更高效的数据可视化。这是一个活跃的研究领域，你可以通过在线和纸质的形式学习许多优秀的资源。</p><p>下一章，我们将重点放在pandas的数据聚合和分组操作上。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/12/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2022/08/12/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<p>title: 测试文章<br>date: 2022-08-12 20:30:51<br>tags:原创</p><p>这是一篇测试文章 </p><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202208122044625.png" alt="image-20220812204459558"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/08/12/hello-world/"/>
    <url>/2022/08/12/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
