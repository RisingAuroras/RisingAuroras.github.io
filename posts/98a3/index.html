<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="动手学深度学习, 马克图布">
    <meta name="description" content="记录我的遗忘">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>动手学深度学习 | 马克图布</title>
    <link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/favicon.jfif">

    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/css/matery.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/css/my.css">

    <script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/jquery/jquery.min.js"></script>
<meta name="generator" content="Hexo 6.2.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="马克图布" type="application/atom+xml">
</head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/medias/log3.jfif" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">马克图布</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/medias/log3.jfif" class="logo-img circle responsive-img">
        
        <div class="logo-name">马克图布</div>
        <div class="logo-desc">
            
            记录我的遗忘
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
    </ul>
</div>


        </div>

        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/medias/featureimages/11.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">动手学深度学习</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E7%BB%8F%E5%85%B8/">
                                <span class="chip bg-color">经典</span>
                            </a>
                        
                            <a href="/tags/%E6%9D%8E%E6%B2%90/">
                                <span class="chip bg-color">李沐</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">
                                机器学习
                            </a>
                        
                            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                深度学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2023-01-29
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="写在之前"><a href="#写在之前" class="headerlink" title="写在之前"></a>写在之前</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301721989.png" alt="image-20230330172144907"></p>
<h2 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301291657863.png" alt="image-20230129165706791"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301291700571.png" alt="image-20230129170034502"></p>
<p>计算图可以显示的去构造</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301291703810.png" alt="image-20230129170304746"></p>
<p>也可以隐式构造</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301291705600.png" alt="image-20230129170505529"></p>
<p>自动求导的两种方式：正向or反向</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301291707264.png" alt="image-20230129170721187"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301291709281.png" alt="image-20230129170959203"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301291714175.png" alt="image-20230129171434113"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301291715506.png" alt="image-20230129171544432"></p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>训练数据集</p>
<p>验证数据集</p>
<p>测试数据集</p>
<blockquote>
<p>NOTE:</p>
<p>不要把验证数据集合测试数据集弄混</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302021957036.png" alt="image-20230202195726933"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302022001052.png" alt="image-20230202200152001"></p>
<h2 id="模型的选择以及过拟合和欠拟合"><a href="#模型的选择以及过拟合和欠拟合" class="headerlink" title="模型的选择以及过拟合和欠拟合"></a>模型的选择以及过拟合和欠拟合</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302022005139.png" alt="image-20230202200548074"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302022011089.png" alt="image-20230202201140008"> <img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302022011490.png" alt="image-20230202201120419"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302022015329.png" alt="image-20230202201554242"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302022017752.png" alt="image-20230202201745674"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302022024356.png" alt="image-20230202202416282"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302022027545.png" alt="image-20230202202726488"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302022034066.png" alt="image-20230202203438984"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302022037701.png" alt="image-20230202203703633"></p>
<h2 id="处理过拟合的方法"><a href="#处理过拟合的方法" class="headerlink" title="处理过拟合的方法"></a>处理过拟合的方法</h2><h3 id="weight-decay"><a href="#weight-decay" class="headerlink" title="weight decay"></a>weight decay</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302031953390.png" alt="image-20230203195306276"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302031957683.png" alt="image-20230203195747613"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302031959075.png" alt="image-20230203195936993"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302032006834.png" alt="image-20230203200638760"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302032008626.png" alt="image-20230203200837562"></p>
<p>对$w_t$的权重衰减值往往设为1e-2/1e-3/1e-4(ps: 1-$\eta\lambda$)</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302032012019.png" alt="image-20230203201226954"></p>
<h3 id="dropout（主流）"><a href="#dropout（主流）" class="headerlink" title="dropout（主流）"></a>dropout（主流）</h3><p>dropout用在全连接层，BN用在卷积层，两者不冲突</p>
<p>Dropout是一种常见的正则化方法，用来减少模型的复杂度</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302032126516.png" alt="image-20230203212659444"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302032130598.png" alt="image-20230203213009516"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302032132709.png" alt="image-20230203213229619"></p>
<p>只在训练过程中使用正则方法，在测试过程(推理)中是不需要正则的</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302032134722.png" alt="image-20230203213407665"></p>
<p>多用在多层感知机的隐藏层上，很少用在CNN之类的模型上</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302032139071.png" alt="image-20230203213934978"></p>
<h2 id="数值稳定性"><a href="#数值稳定性" class="headerlink" title="数值稳定性"></a>数值稳定性</h2><h3 id="梯度爆炸-amp-梯度消失"><a href="#梯度爆炸-amp-梯度消失" class="headerlink" title="梯度爆炸&amp;梯度消失"></a>梯度爆炸&amp;梯度消失</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041108287.png" alt="image-20230204110817173"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041110951.png" alt="image-20230204111004817"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041115525.png" alt="image-20230204111557460"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041116370.png" alt="image-20230204111643277"></p>
<p>用<code>gpu</code>训练时，我们往往会采用16位浮点数，因为训练速度上16位浮点数比32位浮点数快大约两倍</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041118752.png" alt="image-20230204111810686"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041122168.png" alt="image-20230204112238101"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041131910.png" alt="image-20230204113107843"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041131995.png" alt="image-20230204113127937"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041132402.png" alt="image-20230204113230349"></p>
<h3 id="让训练更加稳定—模型初始化和激活函数"><a href="#让训练更加稳定—模型初始化和激活函数" class="headerlink" title="让训练更加稳定—模型初始化和激活函数"></a>让训练更加稳定—模型初始化和激活函数</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041134276.png" alt="image-20230204113423212"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041140874.png" alt="image-20230204114047803"></p>
<p>初始化是为了让训练不一开始就炸掉</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041147957.png" alt="image-20230204114708895"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041153853.png" alt="image-20230204115357761"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041157137.png" alt="image-20230204115724051"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041200601.png" alt="image-20230204120001528"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041203914.png" alt="image-20230204120311834"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041206263.png" alt="image-20230204120657176"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041212374.png" alt="image-20230204121258315"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041218904.png" alt="image-20230204121838824"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302041219124.png" alt="image-20230204121900068"></p>
<p>也就是说用xavier初始化权重，激活函数可以选择relu,tanh或者调整后的sigmoid</p>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271037248.png" alt="image-20230327103653119"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271037137.png" alt="image-20230327103735060"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271037712.png" alt="image-20230327103751626"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271037708.png" alt="image-20230327103759646"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271038698.png" alt="image-20230327103810583"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271038848.png" alt="image-20230327103825765"></p>
<p>一般$p_h=k_h-1,p_w =k_w-1$</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271038640.png" alt="image-20230327103834567"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271039644.png" alt="image-20230327103950571"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271040618.png" alt="image-20230327104018564"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271040427.png" alt="image-20230327104032359"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271041995.png" alt="image-20230327104039015"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271041749.png" alt="image-20230327104130676"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303250927730.png" alt="image-20230325092716665"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271111776.png" alt="image-20230327111143703"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303250935150.png" alt="image-20230325093546087"></p>
<h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271044909.png" alt="image-20230327104405839"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303251550315.png" alt="image-20230325155036195"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303251553648.png" alt="image-20230325155315574"></p>
<p>池化输出的shape公式和卷积的类似</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303251554968.png" alt="image-20230325155424908"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303251556624.png" alt="image-20230325155632562"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303251557200.png" alt="image-20230325155752151"></p>
<p>最大池化和平均 池化都是常用的操作</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303251558043.png" alt="image-20230325155826996"></p>
<h2 id="卷积神经网络模型"><a href="#卷积神经网络模型" class="headerlink" title="卷积神经网络模型"></a>卷积神经网络模型</h2><p>pytorch 中很多都已经集成了</p>
<blockquote>
<p><code>torchvision</code> is a package in the PyTorch ecosystem that provides various tools and utilities for working with computer vision tasks. One of the key features of <code>torchvision</code> is its pre-trained models, which are neural networks trained on large datasets such as ImageNet.</p>
<p>Here are some of the pre-trained models available in <code>torchvision</code>:</p>
<ol>
<li>AlexNet: a deep convolutional neural network (CNN) introduced in 2012, which was one of the first models to achieve high accuracy on the ImageNet dataset.</li>
<li>VGG: a series of CNNs with varying depths, developed by the Visual Geometry Group at the University of Oxford.</li>
<li>ResNet: a family of CNNs introduced in 2015 that includes skip connections, allowing the model to learn residual functions.</li>
<li>Inception: a family of CNNs that use a combination of different convolutional filters at different scales.</li>
<li>DenseNet: a CNN architecture that connects each layer to every other layer in a feed-forward fashion.</li>
<li>MobileNet: a lightweight CNN designed for mobile and embedded applications.</li>
<li>EfficientNet: a family of CNNs that use a compound scaling method to improve both accuracy and efficiency.</li>
</ol>
<p>These models are often used as a starting point for transfer learning, where a pre-trained model is fine-tuned on a new dataset for a specific task. This can greatly reduce the amount of training data and time needed to achieve high performance on a new task.</p>
</blockquote>
<h3 id="经典卷积神经网络-LeNet"><a href="#经典卷积神经网络-LeNet" class="headerlink" title="经典卷积神经网络 LeNet"></a>经典卷积神经网络 LeNet</h3><p>虽然现在看来有一些东西已经过时了，但是它的出现证实了卷积神经网络在图片处理中确实大有益处，且启发了后来了AlexNet</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271120195.png" alt="image-20230327112043139"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271122389.png" alt="image-20230327112201293"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303271122420.png" alt="image-20230327112222332"></p>
<p>网络架构</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l

net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>检查模型</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
<span class="token keyword">for</span> layer <span class="token keyword">in</span> net<span class="token punctuation">:</span>
    X <span class="token operator">=</span> layer<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span><span class="token string">'output shape: \t'</span><span class="token punctuation">,</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">Conv2d output shape: 	 torch.Size([1, 6, 28, 28])
Sigmoid output shape: 	 torch.Size([1, 6, 28, 28])
AvgPool2d output shape: 	 torch.Size([1, 6, 14, 14])
Conv2d output shape: 	 torch.Size([1, 16, 10, 10])
Sigmoid output shape: 	 torch.Size([1, 16, 10, 10])
AvgPool2d output shape: 	 torch.Size([1, 16, 5, 5])
Flatten output shape: 	 torch.Size([1, 400])
Linear output shape: 	 torch.Size([1, 120])
Sigmoid output shape: 	 torch.Size([1, 120])
Linear output shape: 	 torch.Size([1, 84])
Sigmoid output shape: 	 torch.Size([1, 84])
Linear output shape: 	 torch.Size([1, 10])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>调参经验：</p>
<p>LeNet 调参记录</p>
<p>epochs : 100	 batch_size : 256  	激活函数 : Sigmoid  	  lr : 0.9</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303280914067.png"></p>
<p>epochs : 100	 batch_size : 32  	激活函数 : Sigmoid  	  lr : 0.9</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303280936719.png" alt="image-20230328093654668"></p>
<p>epochs : 100	 batch_size : 256  	激活函数 : ReLU  	  lr : 0.1</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303280945319.png" alt="image-20230328094548268"></p>
<p>epochs : 100	 batch_size : 32 	激活函数 : ReLU  	  lr : 0.1</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281005499.png" alt="image-20230328100501449"></p>
<p>epochs : 100	 batch_size : 32 	激活函数 : ReLU  	  lr : 0.1    改变通道：如下</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281156717.png" alt="image-20230328115615648"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281155032.png" alt="image-20230328115534900"></p>
<h3 id="深度卷积神经网络-AlexNet"><a href="#深度卷积神经网络-AlexNet" class="headerlink" title="深度卷积神经网络 AlexNet"></a>深度卷积神经网络 AlexNet</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281718792.png" alt="image-20230328171857703"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281719658.png" alt="image-20230328171909537"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281719143.png" alt="image-20230328171921004"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281719098.png" alt="image-20230328171942981"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281720307.png" alt="image-20230328172006027"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281720411.png" alt="image-20230328172046323"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281720846.png" alt="image-20230328172055626"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281721672.png" alt="image-20230328172115570"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281715753.png" alt="image-20230328171521619"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281733079.png" alt="image-20230328173300993"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281733999.png" alt="image-20230328173338914"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">'''
根据AlexNet论文: https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf ，实现如下：
'''</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
      nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">96</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># </span>
      nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">192</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">192</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">192</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">192</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># 6*6*256</span>
      nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">6</span><span class="token operator">*</span><span class="token number">6</span><span class="token operator">*</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">2048</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2048</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2048</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2048</span><span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>


X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> layer <span class="token keyword">in</span> net<span class="token punctuation">:</span>
    X<span class="token operator">=</span>layer<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span><span class="token string">'output shape:\t'</span><span class="token punctuation">,</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    
<span class="token triple-quoted-string string">'''
Conv2d output shape:	 torch.Size([1, 96, 55, 55])
ReLU output shape:	 torch.Size([1, 96, 55, 55])
MaxPool2d output shape:	 torch.Size([1, 96, 27, 27])
Conv2d output shape:	 torch.Size([1, 256, 27, 27])
ReLU output shape:	 torch.Size([1, 256, 27, 27])
MaxPool2d output shape:	 torch.Size([1, 256, 13, 13])
Conv2d output shape:	 torch.Size([1, 384, 13, 13])
ReLU output shape:	 torch.Size([1, 384, 13, 13])
Conv2d output shape:	 torch.Size([1, 384, 13, 13])
ReLU output shape:	 torch.Size([1, 384, 13, 13])
Conv2d output shape:	 torch.Size([1, 256, 13, 13])
ReLU output shape:	 torch.Size([1, 256, 13, 13])
MaxPool2d output shape:	 torch.Size([1, 256, 6, 6])
Flatten output shape:	 torch.Size([1, 9216])
Linear output shape:	 torch.Size([1, 4096])
ReLU output shape:	 torch.Size([1, 4096])
Dropout output shape:	 torch.Size([1, 4096])
Linear output shape:	 torch.Size([1, 4096])
ReLU output shape:	 torch.Size([1, 4096])
Dropout output shape:	 torch.Size([1, 4096])
Linear output shape:	 torch.Size([1, 1000])
ReLU output shape:	 torch.Size([1, 1000])
'''</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303281744222.png" alt="image-20230328174406291"></p>
<h3 id="使用块的网络-VGG"><a href="#使用块的网络-VGG" class="headerlink" title="使用块的网络 VGG"></a>使用块的网络 VGG</h3><p>VGGNet 使用VGG块，采用3x3的卷积，堆的更深</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291538465.png" alt="image-20230329153837376"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291539345.png" alt="image-20230329153906663"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291540829.png" alt="image-20230329154015692"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291541279.png" alt="image-20230329154128141"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291542093.png" alt="image-20230329154143562"></p>
<p>网址：<a target="_blank" rel="noopener" href="https://cv.gluon.ai/model_zoo/classification.html">https://cv.gluon.ai/model_zoo/classification.html</a></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291543100.png" alt="image-20230329154326000"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291538424.png" alt="image-20230329153819273"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291551165.png" alt="image-20230329155132081"></p>
<p>VGG11代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># vgg_net       </span>
<span class="token keyword">def</span> <span class="token function">vgg_block</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span>
                                kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        in_channels <span class="token operator">=</span> out_channels
    layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">VGGNet11</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>conv_arch<span class="token punctuation">:</span><span class="token builtin">tuple</span><span class="token punctuation">,</span>in_channels<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>VGGNet11<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        layers<span class="token punctuation">:</span> List<span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> num_conv<span class="token punctuation">,</span>out_channels <span class="token keyword">in</span> conv_arch<span class="token punctuation">:</span>
            layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>vgg_block<span class="token punctuation">(</span>num_conv<span class="token punctuation">,</span>in_channels<span class="token punctuation">,</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
            in_channels <span class="token operator">=</span> out_channels

        self<span class="token punctuation">.</span>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">,</span>
                            nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_channels <span class="token operator">*</span> <span class="token number">7</span> <span class="token operator">*</span> <span class="token number">7</span><span class="token punctuation">,</span><span class="token number">4096</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span><span class="token number">4096</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span><span class="token number">176</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                        <span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">checkout_channel</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>X<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> blk <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">:</span>
            X <span class="token operator">=</span> blk<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>blk<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span><span class="token string">'output shape:\t'</span><span class="token punctuation">,</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3 id="网络中的网络-NiN"><a href="#网络中的网络-NiN" class="headerlink" title="网络中的网络 NiN"></a>网络中的网络 NiN</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291728276.png" alt="image-20230329172840171"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291728161.png" alt="image-20230329172856076"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291729135.png" alt="image-20230329172913037"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291730751.png" alt="image-20230329173003645"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291830440.png" alt="image-20230329183033295"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291830800.png" alt="image-20230329183050720"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291831798.png" alt="image-20230329183144698"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291834092.png" alt="image-20230329183426994"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291834524.png" alt="image-20230329183446428"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291835738.png" alt="image-20230329183523649"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303291837900.png" alt="image-20230329183701828"></p>
<p>文章中说：与全连接层相比，global average pooling (GAP) 的两个优点：</p>
<p>① One advantage of global average pooling over the fully connected layers is that it is more native to the convolution structure by enforcing correspondences between feature maps and categories. （GAP 通过强制使特征图和类别之间相对应，对于卷积结构更来说这个转换更自然。）</p>
<p>② Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. （GAP 层没有参数用于优化，避免了过拟合。）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">nin_block</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> strides<span class="token punctuation">,</span> padding<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> strides<span class="token punctuation">,</span> padding<span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nin_block<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nin_block<span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nin_block<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">384</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nin_block<span class="token punctuation">(</span><span class="token number">384</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> layer <span class="token keyword">in</span> net<span class="token punctuation">:</span>
    X <span class="token operator">=</span> layer<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span><span class="token string">'output shape:\t'</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    
    
<span class="token triple-quoted-string string">'''
Sequential output shape:	 torch.Size([1, 96, 54, 54])
MaxPool2d output shape:	 torch.Size([1, 96, 26, 26])
Sequential output shape:	 torch.Size([1, 256, 26, 26])
MaxPool2d output shape:	 torch.Size([1, 256, 12, 12])
Sequential output shape:	 torch.Size([1, 384, 12, 12])
MaxPool2d output shape:	 torch.Size([1, 384, 5, 5])
Dropout output shape:	 torch.Size([1, 384, 5, 5])
Sequential output shape:	 torch.Size([1, 10, 5, 5])
AdaptiveAvgPool2d output shape:	 torch.Size([1, 10, 1, 1])
Flatten output shape:	 torch.Size([1, 10])
'''</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3 id="含并行连结的网络-GoogLeNet-x2F-Inception-V3"><a href="#含并行连结的网络-GoogLeNet-x2F-Inception-V3" class="headerlink" title="含并行连结的网络 GoogLeNet / Inception V3"></a>含并行连结的网络 GoogLeNet / Inception V3</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301530837.png" alt="image-20230330153035711"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301530342.png" alt="image-20230330153057186"></p>
<p>干嘛选呢？都用就是了。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301531894.png" alt="image-20230330153156789"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301532979.png" alt="image-20230330153258890"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301533696.png" alt="image-20230330153310596"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301533606.png" alt="image-20230330153334507"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301534838.png" alt="image-20230330153407744"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301534021.png" alt="image-20230330153415920"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301534587.png" alt="image-20230330153426470"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301534812.png" alt="image-20230330153459740"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301535188.png" alt="image-20230330153509106"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301535550.png" alt="image-20230330153517031"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301536931.png" alt="image-20230330153604840"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301536131.png" alt="image-20230330153620645"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Inception</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> c1<span class="token punctuation">,</span> c2<span class="token punctuation">,</span> c3<span class="token punctuation">,</span> c4<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Inception<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p1_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> c1<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p2_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> c2<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p2_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>c2<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c2<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p3_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> c3<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p3_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>c3<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c3<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p4_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p4_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> c4<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        p1 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p1_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        p2 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p2_2<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p2_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        p3 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p3_2<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p3_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        p4 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p4_2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p4_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>p1<span class="token punctuation">,</span> p2<span class="token punctuation">,</span> p3<span class="token punctuation">,</span> p4<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    
    
    
<span class="token comment">#GoogleNet 模型实现</span>
b1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

b2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

b3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Inception<span class="token punctuation">(</span><span class="token number">192</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   Inception<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

b4 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Inception<span class="token punctuation">(</span><span class="token number">480</span><span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">208</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">48</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   Inception<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">160</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">112</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   Inception<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   Inception<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">112</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">144</span><span class="token punctuation">,</span> <span class="token number">288</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   Inception<span class="token punctuation">(</span><span class="token number">528</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">160</span><span class="token punctuation">,</span> <span class="token number">320</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

b5 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Inception<span class="token punctuation">(</span><span class="token number">832</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">160</span><span class="token punctuation">,</span> <span class="token number">320</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   Inception<span class="token punctuation">(</span><span class="token number">832</span><span class="token punctuation">,</span> <span class="token number">384</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">192</span><span class="token punctuation">,</span> <span class="token number">384</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">48</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3<span class="token punctuation">,</span> b4<span class="token punctuation">,</span> b5<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>



X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> layer <span class="token keyword">in</span> net<span class="token punctuation">:</span>
    X <span class="token operator">=</span> layer<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span><span class="token string">'output shape:\t'</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    
<span class="token triple-quoted-string string">'''
Sequential output shape:	 torch.Size([1, 64, 24, 24])
Sequential output shape:	 torch.Size([1, 192, 12, 12])
Sequential output shape:	 torch.Size([1, 480, 6, 6])
Sequential output shape:	 torch.Size([1, 832, 3, 3])
Sequential output shape:	 torch.Size([1, 1024])
Linear output shape:	 torch.Size([1, 10])
'''</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h3><p>保证层间输出和梯度符合某一特定分布，以提高数据和损失的稳定性</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301722215.png" alt="image-20230330172204107"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301723708.png" alt="image-20230330172228876"></p>
<p>有的论文说BN并不会减少协变量偏移，只是可能加入了噪音</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301730235.png" alt="image-20230330173022144"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303301730860.png" alt="image-20230330173047786"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># start from scratch</span>
<span class="token keyword">def</span> <span class="token function">batch_norm</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> beta<span class="token punctuation">,</span> moving_mean<span class="token punctuation">,</span> moving_var<span class="token punctuation">,</span> eps<span class="token punctuation">,</span> momentum<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>is_grad_enabled<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        X_hat <span class="token operator">=</span> <span class="token punctuation">(</span>X <span class="token operator">-</span> moving_mean<span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>moving_var <span class="token operator">+</span> eps<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
            mean <span class="token operator">=</span> X<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            var <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>X <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            mean <span class="token operator">=</span> X<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            var <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>X <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        X_hat <span class="token operator">=</span> <span class="token punctuation">(</span>X <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>var <span class="token operator">+</span> eps<span class="token punctuation">)</span>
        moving_mean <span class="token operator">=</span> momentum <span class="token operator">*</span> moving_mean <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> momentum<span class="token punctuation">)</span> <span class="token operator">*</span> mean
        moving_var <span class="token operator">=</span> momentum <span class="token operator">*</span> moving_var <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> momentum<span class="token punctuation">)</span> <span class="token operator">*</span> var
    Y <span class="token operator">=</span> gamma <span class="token operator">*</span> X_hat <span class="token operator">+</span> beta
    <span class="token keyword">return</span> Y<span class="token punctuation">,</span> moving_mean<span class="token punctuation">.</span>data<span class="token punctuation">,</span> moving_var<span class="token punctuation">.</span>data
<span class="token keyword">class</span> <span class="token class-name">BatchNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_features<span class="token punctuation">,</span> num_dims<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> num_dims <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
            shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_features<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_features<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>beta <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>moving_mean <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>moving_var <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>moving_mean<span class="token punctuation">.</span>device <span class="token operator">!=</span> X<span class="token punctuation">.</span>device<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>moving_mean <span class="token operator">=</span> self<span class="token punctuation">.</span>moving_mean<span class="token punctuation">.</span>to<span class="token punctuation">(</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>moving_var <span class="token operator">=</span> self<span class="token punctuation">.</span>moving_var<span class="token punctuation">.</span>to<span class="token punctuation">(</span>X<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        Y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>moving_mean<span class="token punctuation">,</span> self<span class="token punctuation">.</span>moving_var <span class="token operator">=</span> batch_norm<span class="token punctuation">(</span>
            X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gamma<span class="token punctuation">,</span> self<span class="token punctuation">.</span>beta<span class="token punctuation">,</span> self<span class="token punctuation">.</span>moving_mean<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>moving_var<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-5</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> Y
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> BatchNorm<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> num_dims<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> BatchNorm<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> num_dims<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> BatchNorm<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> num_dims<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> BatchNorm<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> num_dims<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># concise</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="残差网络-ResNet"><a href="#残差网络-ResNet" class="headerlink" title="残差网络 ResNet"></a>残差网络 ResNet</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311551925.png" alt="image-20230331155140788"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311551194.png" alt="image-20230331155158080"></p>
<p>残差块具有两种结构，左面那个是输入输出通道不变的时候；右边那个是改变通道数的时候</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311554874.png" alt="image-20230331155400771"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311556713.png" alt="image-20230331155607605"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311555992.png" alt="image-20230331155558876"></p>
<p>很多学者对残差结构进行了调整。何凯明等在获得竞赛大奖后，也对残差结构进行了优化：预激活 pre-activation，BN和ReLU都提前了，即上面图片最右边的结构和下面的图片所示结构。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311615176.png" alt="image-20230331161547081"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311556904.png" alt="image-20230331155643795"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311557634.png" alt="image-20230331155720520"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311557534.png" alt="image-20230331155743424"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311622067.png" alt="image-20230331162247959"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311623966.png" alt="image-20230331162341835"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311610044.png" alt="image-20230331161003961"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311609609.png" alt="image-20230331160925487"></p>
<p>从梯度上将ResNet为什么work</p>
<p>加入g(x)为一个全连接层，对数据拟合效果比较好，那么它的梯度相应的会比较小，所以越往后，它的梯度越小。</p>
<p>但是用ResNet后，引入加法，使得后面层的梯度不至于过小。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303311530952.png" alt="image-20230331152953828"></p>
<p>ResNet18代码</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Residual</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span>
                 use_1x1conv<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span>
                               kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>strides<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>num_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span>
                               kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> use_1x1conv<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span>
                                   kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>strides<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_channels<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_channels<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        Y <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        Y <span class="token operator">=</span> self<span class="token punctuation">.</span>bn2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>Y<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">:</span>
            X <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        Y <span class="token operator">+=</span> X
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>Y<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">ResNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>in_channels<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        b1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        b2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>ResNet<span class="token punctuation">.</span>resnet_block<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> first_block<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        b3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>ResNet<span class="token punctuation">.</span>resnet_block<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        b4 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>ResNet<span class="token punctuation">.</span>resnet_block<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        b5 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>ResNet<span class="token punctuation">.</span>resnet_block<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3<span class="token punctuation">,</span> b4<span class="token punctuation">,</span> b5<span class="token punctuation">,</span>
                            nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                            nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">resnet_block</span><span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span> num_residuals<span class="token punctuation">,</span>
                    first_block<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        blk <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_residuals<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> <span class="token keyword">not</span> first_block<span class="token punctuation">:</span>
                blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Residual<span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span>
                                    use_1x1conv<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Residual<span class="token punctuation">(</span>num_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> blk
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">checkout_channel</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>X<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> blk <span class="token keyword">in</span> self<span class="token punctuation">.</span>model<span class="token punctuation">:</span>
            X <span class="token operator">=</span> blk<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>blk<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span><span class="token string">'output shape:\t'</span><span class="token punctuation">,</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304011613344.png" alt="image-20230401161329223"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304011606782.png" alt="image-20230401160625637"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304011607270.png" alt="image-20230401160703174"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304011607059.png" alt="image-20230401160715971"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304011607048.png" alt="image-20230401160753991"></p>
<p>⼀个稠密块由多个卷积块组成，每个卷积块使⽤相同数量的输出通道。然⽽，在前向传播中，我们将每个卷<br>积块的输⼊和输出在通道维上连结。  </p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l


<span class="token keyword">def</span> <span class="token function">conv_block</span><span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>input_channels<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">DenseBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_convs<span class="token punctuation">,</span> input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DenseBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        layer <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>
            layer<span class="token punctuation">.</span>append<span class="token punctuation">(</span>conv_block<span class="token punctuation">(</span>
                num_channels <span class="token operator">*</span> i <span class="token operator">+</span> input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layer<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> blk <span class="token keyword">in</span> self<span class="token punctuation">.</span>net<span class="token punctuation">:</span>
            Y <span class="token operator">=</span> blk<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            <span class="token comment"># # 连接通道维度上每个块的输⼊和输出</span>
            X <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> X
    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>在下⾯的例⼦中，我们定义⼀个有2个输出通道数为10的DenseBlock。使⽤通道数为3的输⼊时，我们会得到<br>通道数为3 + 2 × 10 = 23的输出。卷积块的通道数控制了输出通道数相对于输⼊通道数的增⻓，因此也被称<br>为增⻓率（growth rate）。  </p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">blk <span class="token operator">=</span> DenseBlock<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> blk<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
Y<span class="token punctuation">.</span>shape
<span class="token comment"># torch.Size([4, 23, 8, 8])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304011610101.png" alt="image-20230401161003045"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">transition_block</span><span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>input_channels<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">blk <span class="token operator">=</span> transition_block<span class="token punctuation">(</span><span class="token number">23</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
blk<span class="token punctuation">(</span>Y<span class="token punctuation">)</span><span class="token punctuation">.</span>shape
<span class="token comment">#torch.Size([4, 10, 4, 4])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>



<p>DenseNet模型</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">b1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

num_channels<span class="token punctuation">,</span> growth_rate <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span>
num_convs_in_dense_blocks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>
blks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> num_convs <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>num_convs_in_dense_blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>
    blks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>DenseBlock<span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span> growth_rate<span class="token punctuation">)</span><span class="token punctuation">)</span>
    num_channels <span class="token operator">+=</span> num_convs <span class="token operator">*</span> growth_rate
    <span class="token keyword">if</span> i <span class="token operator">!=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>num_convs_in_dense_blocks<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
        blks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>transition_block<span class="token punctuation">(</span>num_channels<span class="token punctuation">,</span> num_channels <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        num_channels <span class="token operator">=</span> num_channels <span class="token operator">//</span> <span class="token number">2</span>

net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    b1<span class="token punctuation">,</span> <span class="token operator">*</span>blks<span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_channels<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_channels<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h2 id="Squeeze-Excite-Net-SENet"><a href="#Squeeze-Excite-Net-SENet" class="headerlink" title="Squeeze-Excite Net(SENet)"></a>Squeeze-Excite Net(SENet)</h2><h2 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a>ShuffleNet</h2><h2 id="CBAM"><a href="#CBAM" class="headerlink" title="CBAM"></a>CBAM</h2><h2 id="深度学习硬件：CPU-和-GPU"><a href="#深度学习硬件：CPU-和-GPU" class="headerlink" title="深度学习硬件：CPU 和 GPU"></a>深度学习硬件：CPU 和 GPU</h2><h2 id="单机多卡并行"><a href="#单机多卡并行" class="headerlink" title="单机多卡并行"></a>单机多卡并行</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304021700238.png" alt="image-20230402170011074"></p>
<h2 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h2><h2 id="数据增广"><a href="#数据增广" class="headerlink" title="数据增广"></a>数据增广</h2><p>确定需要什么样的数据增强，可以从后往前推，即分析测试集中的数据可能会是什么样的，然后选择合适的数据增强。可以近似的看做一个正则操作，降低过拟合，甚至有可能测试精度会高于训练精度</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304071719791.png" alt="image-20230407171905676"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304071719157.png" alt="image-20230407171914054"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304071719589.png" alt="image-20230407171922460"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304071719336.png" alt="image-20230407171932260"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304071720235.png" alt="image-20230407172022141"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304071720451.png" alt="image-20230407172029345"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304071720605.png" alt="image-20230407172035497"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304071720858.png" alt="image-20230407172042558"></p>
<h2 id="微调-很重要"><a href="#微调-很重要" class="headerlink" title="微调[很重要]"></a>微调[很重要]</h2><p>pretrained</p>
<p>迁移学习</p>
<p>源数据集和目标数据集要类似</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304091459520.png" alt="image-20230409145945281"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304091500583.png" alt="image-20230409150005469"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304091500761.png" alt="image-20230409150035625"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304091500207.png" alt="image-20230409150052077"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304091501624.png" alt="image-20230409150112535"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304091501805.png" alt="image-20230409150134668"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304091502679.png" alt="image-20230409150208564"></p>
<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><h3 id="物体检测和数据集"><a href="#物体检测和数据集" class="headerlink" title="物体检测和数据集"></a>物体检测和数据集</h3><h2 id="调参总结"><a href="#调参总结" class="headerlink" title="调参总结"></a>调参总结</h2><ol>
<li>batch size对训练集精度的影响比较大，batch size较小，可以更好的拟合训练集，训练精度越大；但是对测试集的精度基本没有影响。</li>
<li>ReLU()效果确实比Sigmoid要好一些</li>
</ol>
<h2 id="沐言沐语"><a href="#沐言沐语" class="headerlink" title="沐言沐语"></a>沐言沐语</h2><ol>
<li>二维卷积指的输入的图片只考虑高和宽，不考虑channel。如果是深度图，就是额外多一个深度，可以用3D卷积</li>
<li>卷积操作，对于位置是非常敏感的，用池化操作消除这种影响</li>
<li>池化层一般放在卷积层后（消除卷积对于位置的敏感性），池化层目前用的越来越少了。池化可以1 消除卷积对于位置的敏感性 2 减少计算量 ， 对于2 我们可以把stride放入卷积层中来减少计算量，对于1 我们现在常常做数据增强，来淡化卷积对于位置的敏感性</li>
<li>通过LeNet可以看出，卷积把高和宽不断减小，但是我们在这个过程中，让通道数不断增多，每一个通道可以看做空间的一个pattern(模式)，然后通过MLP训练得到输出。（通常来说，高宽减半的时候，通道数会变为原来的两倍，但是也不能把输出通道调的太大，不然容易overfitting）</li>
<li>图片作为网络的输出时，图片的大小可能是随机的，不一定是输入要求的大小，一般不会直接resize，而是保持高宽比，从里面抠出一个方的，可以多抠几次，一般效果不会差很多。</li>
<li>尽量用简单的模型，不要过度设计</li>
<li>就是pytorch训练好后，模型怎么进行部署应用，c++的话可以试试libtorch(边缘计算的话推荐TensorRT)</li>
<li>在实际深度学习中，尽量不要微调经典网络中的结构，但是可以改变一下通道数，或者输入输出的大小</li>
<li>随着网络深度的增加，可以识别的pattern也增多，即通道数增多，但是，通道数太大容易overfitting，目前来看1024就已经差不多了</li>
<li>BN会加速收敛（可以将lr调的比较大），但是对最后的精度基本没有提升。而且，BN一般用于网络比较深的时候，浅层MLP+BN效果可能没有那么好，用在激活函数之前</li>
<li>关于batch size大小，利用率gpu-util 到90%就行了。或者是增加batch size，来看每秒钟处理的样本数，如果增加到某一个大小，处理样本数增加不了很多的话就可以不用增加了。</li>
<li>cos函数的学习率</li>
<li>训练acc不是一定大于测试acc，比如我们后面可能会在训练集中加入大量噪音或者做data argumentation，测试acc就有可能会大于训练acc</li>
<li>增加数据确实是提高泛发性最简单，最有效的方法。</li>
<li>不要过度调参，过度调参可能导致只fit到当前数据，泛化性不能保证，所以调参到一个相对还ok的结果就行了</li>
<li>resnet18大概18M的样子，一个卷积层大概1M的样子</li>
<li><code>w -= lr*w.grad 和 w = w-lr*w.grad的区别</code></li>
<li>能用矩阵就别写for-loop</li>
<li>python 用多进程避免全局锁</li>
<li>想清楚你要干嘛，不然要学的东西那么多，怎么可能学的过来</li>
<li>卷积的运算浮点数要远远大于全连接</li>
<li>硬件 和 目前深度学习的发展，讨论鸡生蛋和蛋生鸡的问题，目前，确实是硬件shape深度学习。目前大模型和大数据离开不了硬件的发展。</li>
<li>多gpu训练，时间没变快是一件很常见的事，原因可能有1. data的读取花费大量的时间（有可能出现通讯开销大于计算开销） 2.虽然gpu增多了，但是你的数据批量大小没变，导致性能没有充分利用，要至少保证每个gpu的批量大小和之前单gpu时的批量大小一样，注意这时候测试精度可能会变低，要适当地调整学习率（增大）【batich size 和 lr两个因素控制了测试精度】，数据集多样性不够大的时候，不能用特别大的batch size，3.框架对并行的处理不够好（pytorch）4. 神经网络太小，导致数据并行没有明显优势 5.看看机器有没有别人在用</li>
<li>模型性能 看计算量（flops）和内存访问的比率，这个比率越高越好。往往参数比较小，算力比较高的模型性能高，比如卷积性能还不错，矩阵乘法也还可以</li>
<li>关于batch size大小的设置，和数据多样性、算法模型等都是有关的，一般分类的话有$n$个类，那么batch size大小在$10\cdot n$这个量级上</li>
<li>有时候发现其他的优化算法好像还没有SGD效果好，因为SGD做了很强的正则，有很多噪音在里面。</li>
<li>工业进行目标检测时，如果数据集小，可以进行迁移学习，把一个在很大数据集上训练好的模型拿过来</li>
<li>在图像分类任务而言，不用那么的追求高精度，95%已经够用了。工业实际与打比赛的要求确实不一样，工业更多专注数据质量（数据每天都在变化），打比赛是调模型（因为是死数据），工业是85%精度可以部署测试，然后不断增强数据质量，不断喂大量数据，基本3个月-半年后，模型基本可以达到95%以上是没问题的，然后部署生产环境，闭环落地！</li>
<li></li>
</ol>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="叶子竞赛"><a href="#叶子竞赛" class="headerlink" title="叶子竞赛"></a>叶子竞赛</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304112205208.png" alt="image-20230411220551065"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304112200778.png" alt="image-20230411220021667"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304112204613.png" alt="image-20230411220400525"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304112217327.png" alt="image-20230411221139960"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304112216312.png" alt="image-20230411221627070"></p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">马克图布</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://risingauroras.github.io/posts/98a3/">https://risingauroras.github.io/posts/98a3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">马克图布</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E7%BB%8F%E5%85%B8/">
                                    <span class="chip bg-color">经典</span>
                                </a>
                            
                                <a href="/tags/%E6%9D%8E%E6%B2%90/">
                                    <span class="chip bg-color">李沐</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/share/css/share.min.css">
<div id="article-share">

    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments textarea {
        box-sizing: border-box;
        background: url("/medias/comment_bg.png") 100% 100% no-repeat;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #4cbf30;
        font-weight: 500;
        text-decoration: none;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="vcomments" class="card-content" style="display: grid">
    </div>
</div>

<script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/valine/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.iohttps://unpkg.com/valine/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'rq8NIIMaU9s4hlhozChVSkCo-9Nh9j0Va',
        appKey: 'J9SyUPeeBACIilhmoO49Jt20',
        notify: 'true' === 'true',
        verify: 'true' === 'true',
        visitor: 'true' === 'true',
        avatar: 'monsterid',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: 'just go go',
        enableQQ: true,
        emojiCDN: '//i0.hdslb.com/bfs/emote/', 
        // 表情title和图片映射
        emojiMaps: {
        "tv_doge": "6ea59c827c414b4a2955fe79e0f6fd3dcd515e24.png",
        "tv_亲亲": "a8111ad55953ef5e3be3327ef94eb4a39d535d06.png",
        "tv_偷笑": "bb690d4107620f1c15cff29509db529a73aee261.png",
        "tv_再见": "180129b8ea851044ce71caf55cc8ce44bd4a4fc8.png",
        "tv_冷漠": "b9cbc755c2b3ee43be07ca13de84e5b699a3f101.png",
        "tv_发怒": "34ba3cd204d5b05fec70ce08fa9fa0dd612409ff.png",
        "tv_发财": "34db290afd2963723c6eb3c4560667db7253a21a.png",
        "tv_可爱": "9e55fd9b500ac4b96613539f1ce2f9499e314ed9.png",
        "tv_吐血": "09dd16a7aa59b77baa1155d47484409624470c77.png",
        "tv_呆": "fe1179ebaa191569b0d31cecafe7a2cd1c951c9d.png",
        "tv_呕吐": "9f996894a39e282ccf5e66856af49483f81870f3.png",
        "tv_困": "241ee304e44c0af029adceb294399391e4737ef2.png",
        "tv_坏笑": "1f0b87f731a671079842116e0991c91c2c88645a.png",
        "tv_大佬": "093c1e2c490161aca397afc45573c877cdead616.png",
        "tv_大哭": "23269aeb35f99daee28dda129676f6e9ea87934f.png",
        "tv_委屈": "d04dba7b5465779e9755d2ab6f0a897b9b33bb77.png",
        "tv_害羞": "a37683fb5642fa3ddfc7f4e5525fd13e42a2bdb1.png",
        "tv_尴尬": "7cfa62dafc59798a3d3fb262d421eeeff166cfa4.png",
        "tv_微笑": "70dc5c7b56f93eb61bddba11e28fb1d18fddcd4c.png",
        "tv_思考": "90cf159733e558137ed20aa04d09964436f618a1.png",
        "tv_惊吓": "0d15c7e2ee58e935adc6a7193ee042388adc22af.png",
        "tv_打脸": "56ab10b624063e966bfcb76ea5dc4794d87dfd47.png",
        "tv_抓狂": "fe31c08edad661d63762b04e17b8d5ae3c71a757.png",
        "tv_抠鼻": "c666f55e88d471e51bbd9fab9bb308110824a6eb.png",
        "tv_斜眼笑": "911f987aa8bc1bee12d52aafe62bc41ef4474e6c.png",
        "tv_无奈": "ea8ed89ee9878f2fece2dda0ea8a5dbfe21b5751.png",
        "tv_晕": "5443c22b4d07fb1907ccc610c8e6db254f2461b7.png",
        "tv_流汗": "cead1c351ab8d79e9f369605beb90148db0fbed3.png",
        "tv_流泪": "7e71cde7858f0cd50d74b0264aa26db612a8a167.png",
        "tv_流鼻血": "c32d39db2737f89b904ca32700d140a9241b0767.png",
        "tv_点赞": "f85c354995bd99e28fc76c869bfe42ba6438eff4.png",
        "tv_生气": "26702dcafdab5e8225b43ffd23c94ac1ff932654.png",
        "tv_生病": "8b0ec90e6b86771092a498c54f09fc94621c1900.png",
        "tv_疑问": "0793d949b18d7be716078349c202c15ff166f314.png",
        "tv_白眼": "c1d59f439e379ee50eef488bcb5e5378e5044ea4.png",
        "tv_皱眉": "72ccad6679fea0d14cce648b4d818e09b8ffea2d.png",
        "tv_目瞪口呆": "0b8cb81a68de5d5365212c99375e7ace3e7891b7.png",
        "tv_睡着": "8b196675b53af58264f383c50ad0945048290b33.png",
        "tv_笑哭": "1abc628f6d4f4caf9d0e7800878f4697abbc8273.png",
        "tv_腼腆": "89712c0d4af73e67f89e35cbc518420380a7f6f4.png",
        "tv_色": "61822c7e9aae5da76475e7892534545336b23a6f.png",
        "tv_调侃": "4bc022533ef31544ca0d72c12c808cf4a1cce3e3.png",
        "tv_调皮": "b9c41de8e82dd7a8515ae5e3cb63e898bf245186.png",
        "tv_鄙视": "6e72339f346a692a495b123174b49e4e8e781303.png",
        "tv_闭嘴": "c9e990da7f6e93975e25fd8b70e2e290aa4086ef.png",
        "tv_难过": "87f46748d3f142ebc6586ff58860d0e2fc8263ba.png",
        "tv_馋": "fc7e829b845c43c623c8b490ee3602b7f0e76a31.png",
        "tv_鬼脸": "0ffbbddf8a94d124ca2f54b360bbc04feb6bbfea.png",
        "tv_黑人问号": "45821a01f51bc867da9edbaa2e070410819a95b2.png",
        "tv_鼓掌": "1d21793f96ef4e6f48b23e53e3b9e42da833a0f6.png"
            // ... 更多表情
        },
        requiredFields: ['nick','mail'], //设置必填项
    });
</script>

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/posts/4485/">
                    <div class="card-image">
                        
                        
                        <img src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/medias/featureimages/14.jpg" class="responsive-img" alt="特征工程">
                        
                        <span class="card-title">特征工程</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2023-02-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    机器学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">
                        <span class="chip bg-color">特征工程</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/posts/c2fd/">
                    <div class="card-image">
                        
                        
                        <img src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/medias/featureimages/19.jpg" class="responsive-img" alt="强化学习的数学原理">
                        
                        <span class="card-title">强化学习的数学原理</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2023-01-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    强化学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2023</span>
            
            <span id="year">2019</span>
            <a href="/about" target="_blank">马克图布</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/RisingAuroras" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:auroras.k6@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>













</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/materialize/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/aos/aos.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/js/matery.js"></script>
	<script type="text/javascript">
        var OriginTitile=document.title,st;
        document.addEventListener("visibilitychange",function(){
            document.hidden?(document.title="ヽ(●-`Д´-)ノ你要玩捉迷藏嘛",clearTimeout(st)):(document.title="(Ő∀Ő3)ノ好哦！",st=setTimeout(function(){document.title=OriginTitile},3e3))
        })
    </script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

	
    

    

    

    
    <script src="https://cdn.jsdelivr.net/gh/RisingAuroras/RisingAuroras.github.io/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
